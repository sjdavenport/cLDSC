Automatically generated by Mendeley Desktop 1.17.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Chen2020,
	title={Robust inference via multiplier bootstrap},
	author={Chen, Xi and Zhou, Wen-Xin and others},
	journal={Annals of Statistics},
	volume={48},
	number={3},
	pages={1665--1691},
	year={2020},
	publisher={Institute of Mathematical Statistics}
}

@article{Nocedal2006,
	title={Quadratic programming},
	author={Nocedal, Jorge and Wright, Stephen J},
	journal={Numerical optimization},
	pages={448--492},
	year={2006},
	publisher={Springer}
}

@article{Eck2018,
	title={Bootstrapping for multivariate linear regression models},
	author={Eck, Daniel J},
	journal={Statistics \& Probability Letters},
	volume={134},
	pages={141--149},
	year={2018},
	publisher={Elsevier}
}

@article{Wright2006,
	title={Numerical {O}ptimization},
	author={Wright, Stephen and Nocedal, Jorge and others},
	journal={Springer Science},
	year={2006}
}

@book{Vander2000,
  title={Asymptotic statistics},
  author={Van der Vaart, Aad W},
  volume={3},
  year={2000},
  publisher={Cambridge university press}
}

Thesis:
Peak Confidence Regions:
EXTREMUM ESTIMATORS
@book{Amemiya1985,
	title={Advanced Econometrics},
	author={Amemiya, Takeshi},
	year={1985},
	publisher={Harvard University Press}
}

@book{Hayashi2000,
	title={Econometrics},
	author={Hayashi, Fumio },
	year={2000},
	publisher={Princeton University Press}
}

MLE
@inproceedings{Fisher1925,
	title={Theory of statistical estimation},
	author={Fisher, Ronald Aylmer},
	booktitle={Mathematical Proceedings of the Cambridge Philosophical Society},
	volume={22},
	number={5},
	pages={700--725},
	year={1925},
	organization={Cambridge University Press}
}

@article{Efron1978,
	title={Assessing the accuracy of the maximum likelihood estimator: Observed versus expected Fisher information},
	author={Efron, Bradley and Hinkley, David V},
	journal={Biometrika},
	volume={65},
	number={3},
	pages={457--483},
	year={1978},
	publisher={Oxford University Press}
}

@article{Efron2006,
	title={Minimum volume confidence regions for a multivariate normal mean vector},
	author={Efron, Bradley},
	journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	volume={68},
	number={4},
	pages={655--670},
	year={2006},
	publisher={Wiley Online Library}
}

@article{Mammen1993,
	title={Bootstrap and wild bootstrap for high dimensional linear models},
	author={Mammen, Enno},
	journal={The annals of statistics},
	volume={21},
	number={1},
	pages={255--285},
	year={1993},
	publisher={Institute of Mathematical Statistics}
}

@article{Kosorok2003,
	title={Bootstraps of sums of independent but not identically distributed stochastic processes},
	author={Kosorok, Michael R},
	journal={Journal of Multivariate Analysis},
	volume={84},
	number={2},
	pages={299--318},
	year={2003},
	publisher={Elsevier}
}

@article{Braunstein1992,
	title={How large a sample is needed for the maximum likelihood estimator to be approximately Gaussian?},
	author={Braunstein, Samuel L},
	journal={Journal of Physics A: Mathematical and General},
	volume={25},
	number={13},
	pages={3813},
	year={1992},
	publisher={IOP Publishing}
}


MLE Bootstrap
@article{Cheng2010bootstrap,
	title={Bootstrap consistency for general semiparametric M-estimation},
	author={Cheng, Guang and Huang, Jianhua Z and others},
	journal={The Annals of Statistics},
	volume={38},
	number={5},
	pages={2884--2915},
	year={2010},
	publisher={Institute of Mathematical Statistics}
}

@book{Bibby1979,
	title={Multivariate analysis},
	author={Bibby, John M and Mardia, Kantilal Varichand and Kent, John T},
	year={1979},
	publisher={Academic Press}
}

@article{Wellner1996,
	title={Bootstrapping Z-estimators},
	author={Wellner, Jon A and Zhan, Yihui},
	journal={University of Washington Department of Statistics Technical Report},
	volume={308},
	pages={5},
	year={1996}
}

@article{Lahiri1992,
	title={On bootstrapping M-estimators},
	author={Lahiri, SN},
	journal={Sankhy{\=a}: The Indian Journal of Statistics, Series A},
	pages={157--170},
	year={1992},
	publisher={JSTOR}
}

@article{Shorack1982,
	title={Bootstrapping robust regression},
	author={Shorack, Galen R},
	journal={Communications in statistics-theory and methods},
	volume={11},
	number={9},
	pages={961--972},
	year={1982},
	publisher={Taylor \& Francis}
}

@article{Abrevaya2005,
	title={On the bootstrap of the maximum score estimator},
	author={Abrevaya, Jason and Huang, Jian},
	journal={Econometrica},
	volume={73},
	number={4},
	pages={1175--1204},
	year={2005},
	publisher={Wiley Online Library}
}
@article{Heijmans1986,
	title={Asymptotic normality of maximum likelihood estimators obtained from normally distributed but dependent observations},
	author={Heijmans, Risto DH and Magnus, Jan R},
	journal={Econometric Theory},
	pages={374--412},
	year={1986},
	publisher={JSTOR}
}
@article{Heijmans1986b,
	title={On the first--order efficiency and asymptotic normality of maximum likelihood estimators obtained from dependent observations},
	author={Heijmans, Risto Donald Henri and Magnus, Jan Rudolf},
	journal={Statistica Neerlandica},
	volume={40},
	number={3},
	pages={169--188},
	year={1986},
	publisher={Wiley Online Library}
}

@book{Dugue1937,
	author = {Dugué, Daniel},
	keywords = {Probability theory, mathematical statistics},
	language = {fre},
	title = {Application des propriétés de la limite au sens du calcul des probabilités à l'étude de diverses questions d'estimation},
	url = {http://eudml.org/doc/192873},
	year = {1937},
}

@article{Bahadur1967,
	title={Rates of convergence of estimates and test statistics},
	author={Bahadur, Raghu Raj},
	journal={The Annals of Mathematical Statistics},
	volume={38},
	number={2},
	pages={303--324},
	year={1967},
	publisher={JSTOR}
}

@article{Bradley1962,
	title={The asymptotic properties of ML estimators when sampling from associated populations},
	author={Bradley, Ralph A and Gart, John J},
	journal={Biometrika},
	volume={49},
	number={1/2},
	pages={205--214},
	year={1962},
	publisher={JSTOR}
}

@article{Nordberg1980,
	title={Asymptotic normality of maximum likelihood estimators based on independent, unequally distributed observations in exponential family models},
	author={Nordberg, Lennart},
	journal={Scandinavian Journal of Statistics},
	pages={27--32},
	year={1980},
	publisher={JSTOR}
}

@article{Sweeting1980,
	title={Uniform asymptotic normality of the maximum likelihood estimator},
	author={Sweeting, Trevor J},
	journal={The Annals of Statistics},
	pages={1375--1381},
	year={1980},
	publisher={JSTOR}
}

@article{Wald1948,
	title={Asymptotic properties of the maximum likelihood estimate of an unknown parameter of a discrete stochastic process},
	author={Wald, Abraham},
	journal={The Annals of Mathematical Statistics},
	pages={40--46},
	year={1948},
	publisher={JSTOR}
}

@book{Cramer1946,
	title={Mathematical Methods of Statistics},
	author={Cramer, Harold},
	year={1946},
	publisher={Asia Publishing House}
}

INID case
@article{Hoadley1971,
	title={Asymptotic properties of maximum likelihood estimators for the independent not identically distributed case},
	author={Hoadley, Bruce},
	journal={The Annals of mathematical statistics},
	pages={1977--1991},
	year={1971},
	publisher={JSTOR}
}

@article{Philippou1975,
	title={Asymptotic normality of the maximum likelihood estimate in the independent not identically distributed case},
	author={Philippou, Andreas N and Roussas, George G},
	journal={Annals of the Institute of Statistical Mathematics},
	volume={27},
	number={1},
	pages={45--55},
	year={1975},
	publisher={Springer}
}


More General stats papers

@article{Efron1986,
	title={Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy},
	author={Efron, Bradley and Tibshirani, Robert},
	journal={Statistical science},
	pages={54--75},
	year={1986},
	publisher={JSTOR}
}

@book{Efron1994,
	title={An introduction to the bootstrap},
	author={Efron, Bradley and Tibshirani, Robert J},
	year={1994},
	publisher={CRC press}
}

@book{McKenna2018,
abstract = {Despite the increasing use of computers, the basic need for mathematical tables continues. Tables serve a vital role in preliminary surveys of problems before programming for machine operation, and they are indispensable to thousands of engineers and scientists without access to machines. Because of automatic computers, however, and because of recent scientific advances, a greater variety of functions and a higher accuracy of tabulation than have been available until now are required. In 1954, a conference on mathematical tables, sponsored by M.I.T. and the National Science Foundation, met to discuss a modernization and extension of Jahnke and Emde's classical tables of functions. This volume, published 10 years later by the U.S. Department of Commerce, is the result. Designed to include a maximum of information and to meet the needs of scientists in all fields, it is a monumental piece of work, a comprehensive and self-contained summary of the mathematical functions that arise in physical and engineering problems. The book contains 29 sets of tables, some to as high as 20 places: mathematical constants; physical constants and conversion factors (6 tables); exponential integral and related functions (7); error function and Fresnel integrals (12); Bessel functions of integer (12) and fractional (13) order; integrals of Bessel functions (2); Struve and related functions (2); confluent hypergeometric functions (2); Coulomb wave functions (2); hypergeometric functions; Jacobian elliptic and theta functions (2); elliptic integrals {\{}9); Weierstrass elliptic and related functions; parabolic cylinder functions {\{}3); Mathieu functions (2); spheroidal wave functions (5); orthogonal polynomials (13); combinatorial analysis (9); numerical interpolation, differentiation and integration (11); probability functions (ll); scales of notation {\{}6); miscellaneous functions {\{}9); Laplace transforms (2); and others. Each of these sections is prefaced by a list of related formulas and graphs: differential equations, series expansions, special functions, and other basic relations. These constitute an unusually valuable reference work in themselves. The prefatory material also includes an explanation of the numerical methods involved in using the tables that follow and a bibliography. Numerical examples illustrate the use of each table and explain the computation of function values which lie outside its range, while the editors' introduction describes higher-order interpolation procedures. Well over 100 figures illustrate the text. In all, this is one of the most ambitious and useful books of its type ever published, an essential aid in all scientific and engineering research, problem solving, experimentation and field work. This low-cost edition contains every page of the original government publication. Preface by A. V. Astin. Foreword by Advisory Committee, Conference on Mathematical Tables. Editors' Introduction. Indices to Subjects, Notations.}}}}},
author = {McKenna, Sean},
doi = {10.1007/978-3-319-78999-6},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/Random{\_}Field{\_}Theory/Applications/Geoscience/Statistical Parametric Mapping for Geoscience Applications.pdf:pdf},
isbn = {0486612724},
publisher = {Springer International Publishing},
title = {{Statistical Parametric Mapping for Geoscience Applications}},
year = {2018}
}
@article{Xia2008,
abstract = {Classical canonical correlation analysis is one of the fundamental tools in statistics to investigate the linear association between two sets of variables. We propose a method, called semiparametric canonical analysis, to generalize canonical correlation analysis to incorporate the important non-linear association. Semiparametric canonical analysis is easy to implement and interpret. Statistical properties are proved. A consistent estimation method is developed. Selection of significant semiparametric canonical analysis components is discussed. Simulations suggest that the methods proposed have satisfactory performance in finite samples. One environmental data set and one data set in social science are investigated, in which non-linear canonical associations are observed and interpreted.},
author = {Xia, Yingcun},
doi = {10.1111/j.1467-9868.2007.00647.x},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xia - 2008 - A Semiparametric Approach to Canonical Analysis(2).pdf:pdf},
isbn = {13697412, 14679868},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {CCA,Canonical correlation analysis,Consistency,Cross-validation,Kernel smoothing,Non-linear time series,Semiparametrics,Single-index model},
mendeley-tags = {CCA,Semiparametrics},
number = {3},
pages = {519--543},
title = {{A Semiparametric Approach to Canonical Analysis}},
volume = {70},
year = {2008}
}
@article{Hausman2016,
author = {Hausman, Jerry and Luo, Y E and Palmer, Christopher},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/FunctionalDataAnlaysis/ERRORS IN THE DEPENDENT VARIABLE.pdf:pdf},
keywords = {functional analysis,measurement error,quantile regression},
number = {April 2016},
title = {{ERRORS IN THE DEPENDENT VARIABLE OF QUANTILE REGRESSION MODELS}},
year = {2016}
}
@article{Neal1998,
author = {Neal, Radford},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neal - 1998 - Markov Chain Sampling Methods for Dirichlet Mixture Models(2).pdf:pdf},
title = {{Markov Chain Sampling Methods for Dirichlet Mixture Models}},
year = {1998}
}
@article{Keeler2016,
author = {Keeler, Paul},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Stochastic Processes/Notes on stochastic processes.pdf:pdf},
pages = {1--34},
title = {{Notes on stochastic processes}},
year = {2016}
}
@article{Muller2006,
abstract = {We discuss Bayesian approaches to multiple comparison problems, using a decision theoretic perspective to critically compare competing approaches. We set up decision problems that lead to the use of FDR-based rules and generalizations. Alternative definitions of the probability model and the utility function lead to different rules and problem-specific adjustments. Using a loss function that controls realized FDR we derive an optimal Bayes rule that is a variation of the Benjamini and Hochberg (1995) procedure. The cutoff is based on increments in ordered posterior probabilities instead of ordered p- values. Throughout the discussion we take a Bayesian perspective. In particular, we focus on conditional expected FDR, conditional on the data. Variations of the probability model include explicit modeling for dependence. Variations of the utility function include weighting by the extent of a true negative and accounting for the impact in the final decision.},
author = {Muller, P and Parmigiani, Giovanni and Rice, Kenneth},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Multiple Testing/Bayesian/FDR and Bayesian Multiple Comparisons Rules.pdf:pdf},
keywords = {and phrases,decision problems,false,multiplicities},
title = {{FDR and Bayesian multiple comparisons rules}},
year = {2006}
}
@article{Robertson2016,
abstract = {The problem of selection bias has long been recognized in the analysis of two-stage trials, where promising candidates are selected in stage 1 for confirmatory analysis in stage 2. To efficiently correct for bias, uniformly minimum variance conditionally unbiased estimators (UMVCUEs) have been proposed for a wide variety of trial settings, but where the population parameter estimates are assumed to be independent. We relax this assumption and derive the UMVCUE in the multivariate normal setting with an arbitrary known covariance structure. One area of application is the estimation of odds ratios (ORs) when combining a genome-wide scan with a replication study. Our framework explicitly accounts for correlated single nucleotide polymorphisms, as might occur due to linkage disequilibrium. We illustrate our approach on the measurement of the association between 11 genetic variants and the risk of Crohn's disease, as reported in Parkes and others (2007. Sequence variants in the autophagy gene IRGM and multiple other replicating loci contribute to Crohn's disease susceptibility. Nat. Gen. 39: (7), 830-832.), and show that the estimated ORs can vary substantially if both selection and correlation are taken into account.},
author = {Robertson, David S. and Prevost, A. Toby and Bowden, Jack},
doi = {10.1093/biostatistics/kxw012},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Robertson, Prevost, Bowden - 2016 - Accounting for selection and correlation in the analysis of two-stage genome-wide association studie.pdf:pdf},
issn = {14684357},
journal = {Biostatistics},
keywords = {Correlated outcomes,Genome-wide scan,Selection bias,Two-stage sample,Uniformly minimum variance conditionally unbiased},
number = {4},
pages = {634--649},
pmid = {26993061},
title = {{Accounting for selection and correlation in the analysis of two-stage genome-wide association studies}},
volume = {17},
year = {2016}
}
@misc{Nosko1969,
author = {Nosko, V. P.},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nosko - 1969 - Local Structure of Gaussian Random Fields in the Vicinity of High-Level Light Sources.pdf:pdf},
title = {{Local Structure of Gaussian Random Fields in the Vicinity of High-Level Light Sources}},
year = {1969}
}
@article{Yongcheng2008,
abstract = {One of the major interests in extreme-value statistics is to infer the tail properties of the distribution functions in the domain of attraction of an extreme-value distribution and to predict rare events. In recent years, much effort in developing new methodologies has been made by many researchers in this area so as to diminish the impact of the bias in the estimation and achieve some asymptotic optimality in inference problems such as estimating the optimal sample fractions and constructing confidence intervals of various quantities. In particular, bootstrap and empirical likelihood methods, which have been widely used in many areas of statistics, have drawn attention. This paper reviews some novel applications of the bootstrap and the empirical likelihood techniques in extreme-value statistics.},
author = {Qi, Yongcheng},
doi = {10.1007/s10687-007-0049-8},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/EVT/Bootstrap and empirical likelihood methods in extremes.pdf:pdf},
issn = {13861999},
journal = {Extremes},
keywords = {Bootstrap,Confidence interval,Empirical likelihood,Extremes,High quantile,Sample fraction,Tail index},
number = {1},
pages = {81--97},
title = {{Bootstrap and empirical likelihood methods in extremes}},
volume = {11},
year = {2008}
}
@article{Cribben2015,
abstract = {Spectral clustering is a computationally feasible and model-free method widely used in the identification of communities in networks. In this work, we introduce a data-driven method, namely Network Change Points Detection (NCPD), which detects change points in the network structure of a multivariate time series, with each component of the time series represented by a node in the network. NCPD consists of three parts: spectral clustering allows us to consider high dimensional time series where the dimension of the time series is greater than the number of time points (N {\textgreater} T); the principal angles allows for estimation of the change in terms of network/graph structures across time without prior knowledge of the number or location of the change points; permutation and bootstrapping methods are used to perform inference on the change points. NCPD is applied to various simulated data sets as well as to a resting state functional Magnetic Resonance Imaging (fMRI) data set. The results illustrate the ability of NCPD to observe how the network structure changes over the time course. The new methodology also allows us to identify common functional states across subjects. Finally, the method promises to offer a deep insight into the large-scale characterisations and dynamics of the brain},
archivePrefix = {arXiv},
arxivId = {1509.03730},
author = {Cribben, Ivor and Yu, Yi},
eprint = {1509.03730},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cribben, Yu - 2015 - Estimating whole brain dynamics using spectral clustering(2).pdf:pdf},
keywords = {change point analysis,fmri,network change points,resting state data,spectral clustering,sta-,tionary bootstrap},
pages = {1--28},
title = {{Estimating whole brain dynamics using spectral clustering}},
year = {2015}
}
@article{Worsley2000,
abstract = {Images from positron emission tomography (PET) and functional magnetic resonance imaging (fMRI) are often modelled as stationary Gaussian random fields, and a general linear model is used to test for the effect of explanatory varaibles on a set of such images (Friston et al., 1994; Worsley and Friston, 1995). Thompson et al. (1996) have modelled displacements of brain surfaces as a multivariate Gaussian random field. In order to test for significant local maxima in such fields using the theory of Adler (1981) and its recent refinements (Worsley, 1994, 1995a, 1995b; Siegmund andWorsley, 1995), we need to estimate the roughness of such fields. This is defined as the variance matrix of the derivative of the random field in each dimension. Some methods have been given by Worlsey et al. (1992) for the special case of stationary variance of the random field, and where the random field is sampled on a uniform lattice. In this note we generalise to the case of multivariate Gaussian data with unknown non-stationary variance matrix, and non-lattice sampling. This latter is particularly important for studying the displacement of brain surfaces, which will be the subject of a future publication.},
author = {Worsley, K. J.},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Worsley - 1996 - An unbiased estimator for the roughness of a multivariate Gaussian random eld 1 Model.pdf:pdf},
isbn = {1514398389},
pages = {1--5},
title = {{An unbiased estimator for the roughness of a multivariate Gaussian random eld 1 Model}},
year = {1996}
}
@article{Lee2016,
	title={Exact post-selection inference, with application to the lasso},
	author={Lee, Jason D and Sun, Dennis L and Sun, Yuekai and Taylor, Jonathan E and others},
	journal={The Annals of Statistics},
	volume={44},
	number={3},
	pages={907--927},
	year={2016},
	publisher={Institute of Mathematical Statistics}
}
@@article{lee2016exact,
	title={Exact post-selection inference, with application to the lasso},
	author={Lee, Jason D and Sun, Dennis L and Sun, Yuekai and Taylor, Jonathan E and others},
	journal={The Annals of Statistics},
	volume={44},
	number={3},
	pages={907--927},
	year={2016},
	publisher={Institute of Mathematical Statistics}
}article{Filippi2015,
a@article{lee2016exact,
title={Exact post-selection inference, with application to the lasso},
author={Lee, Jason D and Sun, Dennis L and Sun, Yuekai and Taylor, Jonathan E and others},
journal={The Annals of Statistics},
volume={44},
number={3},
pages={907--927},
year={2016},
publisher={Institute of Mathematical Statistics}
}rchivePrefix = {arXiv},
a@article{lee2016exact,
title={Exact post-selection inference, with application to the lasso},
author={Lee, Jason D and Sun, Dennis L and Sun, Yuekai and Taylor, Jonathan E and others},
journal={The Annals of Statistics},
volume={44},
number={3},
pages={907--927},
year={2016},
publisher={Institute of Mathematical Statistics}
}rxivId = {arXiv:1506.00829v2},
author = {Filippi, Sarah and Holmes, Chris C.},
doi = {10.1214/16-BA1027},
eprint = {arXiv:1506.00829v2},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Filippi, Holmes - 2015 - A Bayesian nonparametric approach to testing for dependence between random variables(2).pdf:pdf},
issn = {1936-0975},
keywords = {bayesian nonparametrics,dependence measure,hypothesis,olya tree,p},
title = {{A Bayesian nonparametric approach to testing for dependence between random variables}},
year = {2015}
}
@article{Lauritzen2017,
abstract = {We analyze the problem of maximum likelihood estimation for Gaussian distributions that are multivariate totally positive of order two (MTP2). By exploiting connections to phylogenetics and single-linkage clustering, we give a simple proof that the maximum likelihood estimator (MLE) for such distributions exists based on at least 2 observations, irrespective of the underlying dimension. Slawski and Hein, who first proved this result, also provided empirical evidence showing that the MTP2 constraint serves as an implicit regularizer and leads to sparsity in the estimated inverse covariance matrix, determining what we name the ML graph. We show that the maximum weight spanning forest (MWSF) of the empirical correlation matrix is a spanning forest of the ML graph. In addition, we show that we can find an upper bound for the ML graph by adding edges to the MSWF corresponding to correlations in excess of those explained by the forest. This also gives new theoretical results in the study of inverse M-matrices. We provide globally convergent coordinate descent algorithms for calculating the MLE under the MTP2 constraint which are structurally similar to iterative proportional scaling. We conclude the paper with a discussion of signed MTP2 distributions.},
archivePrefix = {arXiv},
arxivId = {1702.04031},
author = {Lauritzen, Steffen and Uhler, Caroline and Zwiernik, Piotr},
eprint = {1702.04031},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Graphical Models/Maximum likelihood estimation in Gaussian models under total positivity.pdf:pdf},
keywords = {and phrases,attractive gaussian markov random,field,gaussian graphical model,gmrf,inverse m-matrix,mtp 2 distributions,non-frustrated grmf,ultrametric},
title = {{Maximum likelihood estimation in Gaussian models under total positivity}},
year = {2017}
}
@article{AlgStats,
author = {Sullivant, Seth},
doi = {10.1201/9781420035766},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Algebraic Stats/Algebraic Statistics.pdf:pdf},
isbn = {978-1-58488-204-6},
keywords = {Algebraic Statistics},
mendeley-tags = {Algebraic Statistics},
title = {{Algebraic Statistics}},
url = {http://www.crcnetbase.com/doi/book/10.1201/9781420035766},
year = {2000}
}
@article{Lawrence2004,
author = {Lawrence, N.D.},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lawrence - 2004 - Gaussian process latent variable models for visualisation of high dimensional data(2).pdf:pdf},
isbn = {9780262201520},
issn = {10495258},
journal = {Computer},
number = {5},
pages = {329--336},
title = {{Gaussian process latent variable models for visualisation of high dimensional data}},
volume = {16},
year = {2004}
}
@article{Nadarajah2008,
abstract = {Maximum and minimum of correlated Gaussian random variables arise naturally with respect to statistical static time analysis. It appears, however, that only approximations have been used in the literature to study the distribution of the max/min of correlated Gaussian random variables. In this paper, we would like to point out that the statistics literature has long established simple expressions for the exact distribution of the max/min. We provide some of the known expressions for the following: the probability density function, moment generating function, and the moments. We also provide two simple programs for computing the probability density functions of the max/min and an illustration of the results to statistical static time analysis.},
author = {Nadarajah, Saralees and Kotz, Samuel},
doi = {10.1109/TVLSI.2007.912191},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/EVT/Exact Distribution of the Max$\backslash$:Min of Two Gaussian Random Variables.pdf:pdf},
issn = {10638210},
journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
keywords = {Maximum,Minimum,Moment generating function (MGF),Moments,Probability density function (pdf),Statistical static time analysis (SSTA)},
number = {2},
pages = {210--212},
title = {{Exact distribution of the max/min of two Gaussian random variables}},
volume = {16},
year = {2008}
}
@article{Goeman2011,
abstract = {Motivated by the practice of exploratory research, we formulate an approach to multiple testing that reverses the conventional roles of the user and the multiple testing procedure. Traditionally, the user chooses the error criterion, and the procedure the resulting rejected set. Instead, we propose to let the user choose the rejected set freely, and to let the multiple testing procedure return a confidence statement on the number of false rejections incurred. In our approach, such confidence statements are simultaneous for all choices of the rejected set, so that post hoc selection of the rejected set does not compromise their validity. The proposed reversal of roles requires nothing more than a review of the familiar closed testing procedure, but with a focus on the non-consonant rejections that this procedure makes. We suggest several shortcuts to avoid the computational problems associated with closed testing.},
archivePrefix = {arXiv},
arxivId = {arXiv:1208.2841v2},
author = {Goeman, Jelle J. and Solari, Aldo},
doi = {10.1214/11-STS356},
eprint = {arXiv:1208.2841v2},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Multiple Testing/Sequences and Trees/1208.2841.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Closed testing,and phrases,confidence set,false discovery pr},
number = {4},
pages = {584--597},
title = {{Multiple Testing for Exploratory Research}},
volume = {26},
year = {2011}
}
@article{Posch2005,
abstract = {Integrating selection and confirmation phases into a single trial can expedite the development of new treatments and allows to use all accumulated data in the decision process. In this paper we review adaptive treatment selection based on combination tests and propose overall adjusted p-values and simultaneous confidence intervals. Also point estimation in adaptive trials is considered. The methodology is illustrated in a detailed example based on an actual planned study.},
author = {Posch, Martin and Koenig, Franz and Branson, Michael and Brannath, Werner and Dunger-Baldauf, Cornelia and Bauer, Peter},
doi = {10.1002/sim.2389},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Posch et al. - 2005 - Testing and estimation in flexible group sequential designs with adaptive treatment selection.pdf:pdf},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {Clinical trials,Flexible designs,Simultaneous confidence intervals,Treatment selection},
number = {24},
pages = {3697--3714},
pmid = {16320264},
title = {{Testing and estimation in flexible group sequential designs with adaptive treatment selection}},
volume = {24},
year = {2005}
}
@book{Brillinger,
author = {Brillinger, D and Gani, J and Hartigan, J and Krickeberg, K},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brillinger et al. - Unknown - Springer Series in Statistics Springer Series in Statistics(2).pdf:pdf},
isbn = {9781475719062},
keywords = {Semiparametrics},
mendeley-tags = {Semiparametrics},
title = {{Springer Series in Statistics Springer Series in Statistics}}
}
@article{Diedrichsen2016,
abstract = {We present analytical expressions for the means and covariances of the sample distribution of the cross-validated Mahalanobis distance. This measure has proven to be especially useful in the context of representational similarity analysis (RSA) of neural activity patterns as measured by means of functional magnetic resonance imaging (fMRI). These expressions allow us to construct a normal approximation to the estimated distances, which in turn enables powerful inference on the measured statistics. Using the results, the difference between two distances can be statistically assessed, and the measured structure of the distances can be efficiently compared to predictions from computational models.},
archivePrefix = {arXiv},
arxivId = {1607.01371},
author = {Diedrichsen, J{\"{o}}rn and Provost, Serge and Zareamoghaddam, Hossein},
eprint = {1607.01371},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Non-Parametric Bayes/On the distribution of cross-validated.pdf:pdf},
pages = {1--24},
title = {{On the distribution of cross-validated Mahalanobis distances}},
year = {2016}
}
@article{Reid2017,
	title={Post-selection point and interval estimation of signal sizes in Gaussian samples},
	author={Reid, Stephen and Taylor, Jonathan and Tibshirani, Robert},
	journal={Canadian Journal of Statistics},
	volume={45},
	number={2},
	pages={128--148},
	year={2017},
	publisher={Wiley Online Library}
}
@article{Robinson1991,
abstract = {The history, empirical evidence and classical explanations of the significant-digit (or Benford's) law are reviewed, followed by a summary of recent invariant-measure characterizations. Then a new statistical derivation of the law in the form of a CLT-like theorem for significant digits is presented. If distributions are selected at random (in any "unbiased" way) and random samples are then taken from each of these distributions, the significant digits of the combined sample will converge to the logarithmic (Benford) distribution. This helps explain and predict the appearance of the significant-digit phenomenon in many different emprical contexts and helps justify its recent application to computer design, mathematical modelling and detection of fraud in accounting data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Robinson, G},
doi = {10.2307/2246134},
eprint = {arXiv:1011.1669v3},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Robinson - 1991 - That BLUP Is a Good Thing The Estimation of Random Effects.pdf:pdf},
isbn = {0883-4237},
issn = {2168-8745},
journal = {Statistical Science},
number = {1},
pages = {15--51},
pmid = {20948974},
title = {{That BLUP Is a Good Thing: The Estimation of Random Effects}},
volume = {6},
year = {1991}
}
@article{Gangbo2004,
author = {Gangbo, Wilfrid},
doi = {10.1002/cta.281},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Functional Time Series Analysis/An Introduction to the Mass Transportation Theory and its Applications.pdf:pdf},
journal = {Analysis},
keywords = {Wasserstein Distance},
mendeley-tags = {Wasserstein Distance},
number = {56},
pages = {1--49},
title = {{An Introduction to the Mass Transportation Theory and its Applications}},
year = {2004}
}
@article{Columbi2011,
abstract = {We consider the Gumbel or extreme value statistics describing the distribution function pG($\nu$max) of the maximum values of a random field $\nu$ within patches of fixed size. We present, for smooth Gaussian random fields in two and three dimensions, an analytical estimate of pG which is expected to hold in a regime where local maxima of the field are moderately high and weakly clustered.$\backslash$n$\backslash$nWhen the patch size becomes sufficiently large, the negative of the logarithm of the cumulative extreme value distribution is simply equal to the average of the Euler characteristic of the field in the excursion $\nu$≥$\nu$max inside the patches. The Gumbel statistics therefore represents an interesting alternative probe of the genus as a test of non-Gaussianity, e.g. in cosmic microwave background temperature maps or in 3D galaxy catalogues. It can be approximated, except in the remote positive tail, by a negative Weibull-type form, converging slowly to the expected Gumbel-type form for infinitely large patch size. Convergence is facilitated when large-scale correlations are weaker.$\backslash$n$\backslash$nWe compare the analytic predictions to numerical experiments for the case of a scale-free Gaussian field in two dimensions, achieving impressive agreement between approximate theory and measurements. We also discuss the generalization of our formalism to non-Gaussian fields.},
archivePrefix = {arXiv},
arxivId = {astro-ph.CO/1102.5707},
author = {Colombi, St{\'{e}}phane and Davis, Olaf and Devriendt, Julien and Prunet, Simon and Silk, Joe},
doi = {10.1111/j.1365-2966.2011.18563.x},
eprint = {1102.5707},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Colombi et al. - 2011 - Extreme value statistics of smooth Gaussian random fields.pdf:pdf},
issn = {00358711},
journal = {Monthly Notices of the Royal Astronomical Society},
keywords = {Large-scale structure of Universe,Methods: analytical,Methods: statistical},
number = {3},
pages = {2436--2445},
primaryClass = {astro-ph.CO},
title = {{Extreme value statistics of smooth Gaussian random fields}},
volume = {414},
year = {2011}
}
@article{Bai2018,
abstract = {We consider the testing and estimation of change-points, locations where the distribution abruptly changes, in a sequence of observations. Motivated by this problem, in this contribution we first investigate the extremes of Gaussian fields with trend which then help us give asymptotic p-value approximations of the likelihood ratio statistics from change-point models.},
archivePrefix = {arXiv},
arxivId = {1805.00239},
author = {Bai, Long},
eprint = {1805.00239},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/Random{\_}Field{\_}Theory/Applications/Changepoints/Estimation of Change-point Models.pdf:pdf},
keywords = {change-point,gaussian fields,pickands constant,piterbarg constant},
pages = {1--23},
title = {{Estimation of Change-point Models}},
url = {http://arxiv.org/abs/1805.00239},
year = {2018}
}
@article{Gretton2012,
abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD). We present two distribution-free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
author = {Gretton, Arthur},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gretton - 2012 - A Kernel Two-Sample Test(2).pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
pages = {723--773},
title = {{A Kernel Two-Sample Test}},
volume = {13},
year = {2012}
}
@article{ThomasSFerguson1973,
author = {{Thomas S Ferguson}},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thomas S Ferguson - 1973 - A Beyesian Analysis of Some non-parametric problems(2).pdf:pdf},
journal = {The Annals of Statistics},
number = {2},
pages = {209--230},
title = {{A Beyesian Analysis of Some non-parametric problems}},
volume = {1},
year = {1973}
}
@article{Groetsch1995,
author = {Groetsch, C. W.},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Groetsch - 1995 - Inclusions and Identities for the Moore-Penrose Inverse of a Closed Linear Operator(2).pdf:pdf},
journal = {Math. Nachr},
pages = {157--164},
title = {{Inclusions and Identities for the Moore-Penrose Inverse of a Closed Linear Operator}},
volume = {171},
year = {1995}
}
@article{Algeri2017,
abstract = {The identification of new rare signals in data, the detection of a sudden change in a trend, and the selection of competing models, are among the most challenging problems in statistical practice. These challenges can be tackled using a test of hypothesis where a nuisance parameter is present only under the alternative, and a computationally efficient solution can be obtained by the "Testing One Hypothesis Multiple times" (TOHM) method. In the one-dimensional setting, a fine discretization of the space of the non-identifiable parameter is specified, and a global p-value is obtained by approximating the distribution of the supremum of the resulting stochastic process. In this paper, we propose a computationally efficient inferential tool to perform TOHM in the multidimensional setting. Here, the approximations of interest typically involve the expected Euler Characteristics (EC) of the excursion set of the underlying random field. We introduce a simple algorithm to compute the EC in multiple dimensions and for arbitrary large significance levels. This leads to an highly generalizable computational tool to perform inference under non-standard regularity conditions.},
archivePrefix = {arXiv},
arxivId = {1803.03858},
author = {Algeri, Sara and van Dyk, David A.},
eprint = {1803.03858},
file = {:data/fireback/davenpor/davenpor/Referreeing/Testing One Hypothesis Multiple Times- The Multidimensional Case/Supplementary Material/StatSinica{\_}submitted.pdf:pdf},
keywords = {and phrases,bump hunting,in hypothesis testing,multiple hypothesis testing,non-identifiabily,non-nested models comparison},
title = {{Testing One Hypothesis Multiple Times: The Multidimensional Case}},
year = {2017}
}
@article{Zhong2011,
author = {Zhong, Hua and Prentice, Ross L},
doi = {10.1002/gepi.20437.Correcting},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhong, Prentice - 2011 - Correcting winner's curse in odds ratios genomewide association findings for major complex human diseases(2).pdf:pdf},
journal = {Genetic Epidemiology},
keywords = {Winner's Curse,bias correction,complex human,confidence interval,genomewide association study,odds ratio},
mendeley-tags = {Winner's Curse},
number = {1},
pages = {78--91},
title = {{Correcting "winner's curse" in odds ratios genomewide association findings for major complex human diseases}},
volume = {34},
year = {2011}
}
@article{Griffiths2011,
abstract = {The Indian buffet process is a stochastic process defining a probability distribution over equiva- lence classes of sparse binary matrices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilisticmodels that represent objects using a potentially infinite array of features, or that involve bipartite graphs in which the size of at least one class of nodes is unknown. We give a detailed derivation of this distribution, and illustrate its use as a prior in an infinite latent featuremodel. We then review recent applications of the Indian buffet process in machine learning, discuss its extensions, and summarize its connections to other stochastic processes.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Griffiths, Thomas L. and Ghahramani, Zoubin},
doi = {10.1016/j.biotechadv.2011.08.021.Secreted},
eprint = {NIHMS150003},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Griffiths, Ghahramani - 2011 - The Indian Buffet Process An Introduction and Review(2).pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal ofMachine Learning Research},
keywords = {beta process,chinese,exchangeable distributions,latent variable models,markov chain monte carlo,nonparametric bayes,restaurant processes,sparse binary matrices},
pages = {1185--1224},
pmid = {290096100001},
title = {{The Indian Buffet Process: An Introduction and Review}},
volume = {12},
year = {2011}
}
@article{MacEachern1994,
abstract = {In this article, the Dirichlet process prior is used to provide a nonparametric Bayesian estimate of a vector of normal means. In the past there have been computational difficulties with this model. This article solves the computational difficulties by developing a "Gibbs sampler" algorithm. The estimator developed in this article is then compared to parametric empirical Bayes estimators (PEB) and nonparametric empirical Bayes estimators (NPEB) in a Monte Carlo study. The Monte Carlo study demonstrates that in some conditions the PEB is better than the NPEB and in other conditions the NPEB is better than the PEB. The Monte Carlo study also shows that the estimator developed in this article produces estimates that are about as good as the PEB when the PEB is better and produces estimates that are as good as the NPEB estimator when that method is better},
author = {MacEachern, Steven N.},
doi = {10.1080/03610919408813196},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/MacEachern - 1994 - Estimating Normal Means with a Conjugate Style Dirichlet Process Prior(2).pdf:pdf},
isbn = {0361091940881},
issn = {0361-0918},
journal = {Communications in Statistics - Simulation and Computation},
number = {3},
pages = {727--741},
title = {{Estimating Normal Means with a Conjugate Style Dirichlet Process Prior}},
volume = {23},
year = {1994}
}
@article{Freedman2018,
author = {Freedman, David and Lane, David and Freedman, David and Lane, David},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/Resampling/Permutation Test/A Nonstochastic Interpretation of Reported Significance Levels.pdf:pdf},
journal = {Journal of Business {\&} Economic Statistics},
keywords = {hypothesis testing,multiple regression,p values,significance levels},
number = {4},
pages = {292--298},
title = {{A Nonstochastic Interpretation of Reported Significance Levels}},
volume = {1},
year = {2018}
}
@article{Hesterberg2015,
abstract = {Bootstrapping has enormous potential in statistics education and practice, but there are subtle issues and ways to go wrong. For example, the common combination of nonparametric boot- strapping and bootstrap percentile confidence intervals is less accurate than using t-intervals for small samples, though more accurate for larger samples.Mygoals in this article are to provide a deeper understanding of bootstrap methods—how they work, when theywork or not, and whichmethodswork better—and to highlight pedagogical issues. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {1411.5279},
author = {Hesterberg, Tim},
doi = {10.1080/00031305.2015.1089789},
eprint = {1411.5279},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/What Teachers Should Know About the Bootstrap$\backslash$: Resampling in the.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {bootstrap,permutation test,randomization test,teaching},
number = {4},
pages = {371----386},
pmid = {27019512},
title = {{What Teachers Should Know about the Bootstrap: Resampling in the Undergraduate Statistics Curriculum}},
volume = {69},
year = {2015}
}
@article{Efron1998,
author = {Efron, B Y Bradley and Tibshirani, Robert},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Parametric Bootstrap/The Problem of Regions.pdf:pdf},
number = {5},
pages = {1687--1718},
title = {{The Problem of Regions}},
volume = {26},
year = {1998}
}
@article{Dempster1977,
abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
archivePrefix = {arXiv},
arxivId = {0710.5696v2},
author = {Dempster, A.P. and Laird, N.M. and Rubin, Donald B},
doi = {http://dx.doi.org/10.2307/2984875},
eprint = {0710.5696v2},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dempster, Laird, Rubin - 1977 - Maximum likelihood from incomplete data via the EM algorithm(2).pdf:pdf},
isbn = {0000000779},
issn = {00359246},
journal = {Journal of the Royal Statistical Society Series B Methodological},
number = {1},
pages = {1--38},
pmid = {9501024},
title = {{Maximum likelihood from incomplete data via the EM algorithm}},
volume = {39},
year = {1977}
}
@article{Davidson1999,
author = {Davidson, Russell and MacKinnon, James G.},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davidson, MacKinnon - 1999 - Bootstrap Testing in Nonlinear Models.pdf:pdf},
journal = {Interational Economic REview},
number = {2},
pages = {487--508},
title = {{Bootstrap Testing in Nonlinear Models}},
volume = {40},
year = {1999}
}
@article{Scott2014,
author = {Scott, Lecturer Clayton},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Scott - 2014 - The Bayes Classifier Properties of the Bayes Risk(2).pdf:pdf},
pages = {1--4},
title = {{The Bayes Classifier Properties of the Bayes Risk}},
year = {2014}
}
@article{Azais2015,
author = {Azais, Jean-Marc and Castro, Yohann and Mourareau, Stephane},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Azais, Castro, Mourareau - 2015 - Power of the spacing test for least-angle regression(2).pdf:pdf},
keywords = {1 -minimization,and phrases,hypothesis testing,lasso,power,spacing test},
pages = {1--22},
title = {{Power of the spacing test for least-angle regression}},
year = {2015}
}
@article{Sun2015,
abstract = {The paper develops a unified theoretical and computational framework for false discovery control in multiple testing of spatial signals. We consider both pointwise and clusterwise spatial analyses, and derive oracle procedures which optimally control the false discovery rate, false discovery exceedance and false cluster rate. A data-driven finite approximation strategy is developed to mimic the oracle procedures on a continuous spatial domain. Our multiple-testing procedures are asymptotically valid and can be effectively implemented using Bayesian computational algorithms for analysis of large spatial data sets. Numerical results show that the procedures proposed lead to more accurate error control and better power performance than conventional methods. We demonstrate our methods for analysing the time trends in tropospheric ozone in eastern USA.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Sun, Wenguang and Reich, Brian J. and {Tony Cai}, T. and Guindani, Michele and Schwartzman, Armin},
doi = {10.1111/rssb.12064},
eprint = {NIHMS150003},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Spatial Stats/False discovery control in large-scale spatialmultiple testing.pdf:pdf},
isbn = {0000000000000},
issn = {14679868},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Compound decision theory,False cluster rate,False discovery exceedance,False discovery rate,Large-scale multiple testing,Spatial dependence},
number = {1},
pages = {59--83},
pmid = {25642138},
title = {{False discovery control in large-scale spatial multiple testing}},
volume = {77},
year = {2015}
}
@article{Whitehead1986,
author = {Whitehead, John},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bias/On the Bias of Maximum Likelihood Estimation Following a Sequential Test.pdf:pdf},
number = {3},
pages = {573--581},
title = {{On the Bias of Maximum Likelihood Estimation Following a Sequential Test}},
volume = {73},
year = {2017}
}
@article{Rubin1981,
abstract = {The Bayesian bootstrap is the Bayesian analogue of the bootstrap. Instead of simulating the sampling distribution of a statistic estimating a parameter, the Bayesian bootstrap simulates the posterior distribution of the parameter; operationally and inferentially the methods are quite similar. Because both methods of drawing inferences are based on somewhat peculiar model assumptions and the resulting inferences are generally sensitive to these assumptions, neither method should be applied without some consideration of the reasonableness of these model assumptions. In this sense, neither method is a true bootstrap procedure yielding inferences unaided by external assumptions.},
author = {Rubin, Donald B.},
doi = {10.1214/aos/1176345338},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Bayesian Bootstrap/The Bayesian Bootstrap.pdf:pdf},
isbn = {1862391912},
issn = {0090-5364},
journal = {Annals of Statistics},
number = {1},
pages = {130--134},
title = {{The Bayesian Bootstrap}},
volume = {9},
year = {1981}
}
@article{Cheng2013,
author = {Cheng, Dan},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng - 2013 - The Excursion Probability of Gaussian and Asymptotically Gaussian Random Fields.pdf:pdf},
title = {{The Excursion Probability of Gaussian and Asymptotically Gaussian Random Fields}},
year = {2013}
}
@article{Solari2010,
abstract = {Closed testing and partitioning are recognized as fundamental principles of familywise error control. In this paper, we argue that sequential rejection can be considered equally fundamental as a general principle of multiple testing. We present a general sequentially rejective multiple testing procedure and show that many well-known familywise error controlling methods can be constructed as special cases of this procedure, among which are the procedures of Holm, Shaffer and Hochberg, parallel and serial gatekeeping procedures, modern procedures for multiple testing in graphs, resampling-based multiple testing procedures and even the closed testing and partitioning procedures themselves. We also give a general proof that sequentially rejective multiple testing procedures strongly control the familywise error if they fulfill simple criteria of monotonicity of the critical values and a limited form of weak familywise error control in each single step. The sequential rejection principle gives a novel theoretical perspective on many well-known multiple testing procedures, emphasizing the sequential aspect. Its main practical usefulness is for the development of multiple testing procedures for null hypotheses, possibly logically related, that are structured in a graph. We illustrate this by presenting a uniform improvement of a recently published procedure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.3313v1},
author = {Goeman, Jelle J. and Solari, Aldo},
doi = {10.1214/10-AOS829},
eprint = {arXiv:1211.3313v1},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Multiple Testing/Sequences and Trees/THE SEQUENTIAL REJECTION PRINCIPLE OF FAMILYWISE ERROR CONTROL.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Familywise error rate,Graph,Multiple testing},
number = {6},
pages = {3782--3810},
title = {{The sequential rejection principle of familywise error control}},
volume = {38},
year = {2010}
}
@article{Efron2005,
author = {Efron, Bradley},
file = {:homes/davenpor/global/TomsMiniProject/Project Papers/Correlated z-values and the accuracy of large-scale statistical estimates.pdf:pdf},
keywords = {acceleration,correlation penalty,empirical process,mehler,non-null z -values,rms correlation,s identity},
number = {2005},
pages = {1--28},
title = {{Correlated z -values and the accuracy of large-scale statistical estimates}}
}
@article{Ghosh1984,
author = {Ghosh, Jayanta Kumar and Sen, Pranab Kumar},
file = {:data/fireback/davenpor/davenpor/Referreeing/Testing One Hypothesis Multiple Times- The Multidimensional Case/MArk2/References/1985 - Ghosh and Sen.pdf:pdf},
keywords = {phrases},
number = {1467},
title = {{On the asymptotic performance of the log likelihood ratio statistic for the mixture model and related results}},
year = {1984}
}
@article{Lopes2018,
archivePrefix = {arXiv},
arxivId = {arXiv:1809.08539v1},
author = {Lopes, Miles E},
eprint = {arXiv:1809.08539v1},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/Random{\_}Field{\_}Theory/Other/Tail Probabilities/ON THE MAXIMUM OF DEPENDENT GAUSSIAN RANDOM.pdf:pdf},
keywords = {and phrases,comparison inequalities,gaussian processes,grant dms 1613218,in part by nsf,small deviations,the author was supported},
pages = {1--12},
title = {{On the maximum of dependent gaussian random variables: a sharp bound for the lower tail}},
year = {2018}
}
@article{Neyman1959,
author = {Neyman},
title = {{Optimal asymptotic tests of composite statistical hypotheses}},
year = {1959}
}
@article{Harris2016,
abstract = {We consider the problem of selective inference after solving a (randomized) convex statistical learning program in the form of a penalized or constrained loss function. Our first main result is a change-of-measure formula that describes many conditional sampling problems of interest in selective inference. Our approach is model-agnostic in the sense that users may provide their own statistical model for inference, we simply provide the modification of each distribution in the model after the selection. Our second main result describes the geometric structure in the Jacobian appearing in the change of measure, drawing connections to curvature measures appearing in Weyl-Steiner volume-of-tubes formulae. This Jacobian is necessary for problems in which the convex penalty is not polyhedral, with the prototypical example being group LASSO or the nuclear norm. We derive explicit formulae for the Jacobian of the group LASSO. To illustrate the generality of our method, we consider many examples throughout, varying both the penalty or constraint in the statistical learning problem as well as the loss function, also considering selective inference after solving multiple statistical learning programs. Penalties considered include LASSO, forward stepwise, stagewise algorithms, marginal screening and generalized LASSO. Loss functions considered include squared-error, logistic, and log-det for covariance matrix estimation. Having described the appropriate distribution we wish to sample from through our first two results, we outline a framework for sampling using a projected Langevin sampler in the (commonly occuring) case that the distribution is log-concave.},
archivePrefix = {arXiv},
arxivId = {1609.05609},
author = {Harris, Xiaoying Tian and Panigrahi, Snigdha and Markovic, Jelena and Bi, Nan and Taylor, Jonathan},
eprint = {1609.05609},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Harris et al. - 2016 - Selective sampling after solving a convex problem(2).pdf:pdf},
pages = {1--69},
title = {{Selective sampling after solving a convex problem}},
year = {2016}
}
@article{Bowden2008,
abstract = {Straightforward estimation of a treatment's effect in an adaptive clinical trial can be severely hindered when it has been chosen from a larger group of potential candidates. This is because selection mechanisms that condition on the rank order of treatment statistics introduce bias. Nevertheless, designs of this sort are seen as a practical and efficient way to fast track the most promising compounds in drug development. In this paper we extend the method of Cohen and Sackrowitz (1989) who proposed a two-stage unbiased estimate for the best performing treatment at interim. This enables their estimate to work for unequal stage one and two sample sizes, and also when the quantity of interest is the best, second best, or j -th best treatment out of k. The implications of this new flexibility are explored via simulation.},
author = {Bowden, Jack and Glimm, Ekkehard},
doi = {10.1002/bimj.200810442},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bowden, Glimm - 2008 - Unbiased estimation of selected treatment means in two-stage trials.pdf:pdf},
issn = {03233847},
journal = {Biometrical Journal},
keywords = {Adaptive trial,Point estimation,Selection bias,UMVCUE},
number = {4},
pages = {515--527},
pmid = {18663760},
title = {{Unbiased estimation of selected treatment means in two-stage trials}},
volume = {50},
year = {2008}
}
@article{Nosko1969b,
author = {Nosko, V. P.},
journal = {Proceedings of the USSR-Japan Symposium on Probability},
pages = {216--222},
title = {{The Characteristics of Excursions of Gaussian Homogeneous Random Fields above a high level}},
year = {1969}
}
@article{Xu2013,
abstract = {In recent years, a great many methods of learning from multi-view data by considering the diversity of different views have been proposed. These views may be obtained from multiple sources or different feature subsets. In trying to organize and highlight similarities and differences between the variety of multi-view learning approaches, we review a number of representative multi-view learning algorithms in different areas and classify them into three groups: 1) co-training, 2) multiple kernel learning, and 3) subspace learning. Notably, co-training style algorithms train alternately to maximize the mutual agreement on two distinct views of the data; multiple kernel learning algorithms exploit kernels that naturally correspond to different views and combine kernels either linearly or non-linearly to improve learning performance; and subspace learning algorithms aim to obtain a latent subspace shared by multiple views by assuming that the input views are generated from this latent subspace. Though there is significant variance in the approaches to integrating multiple views to improve learning performance, they mainly exploit either the consensus principle or the complementary principle to ensure the success of multi-view learning. Since accessing multiple views is the fundament of multi-view learning, with the exception of study on learning a model from multiple views, it is also valuable to study how to construct multiple views and how to evaluate these views. Overall, by exploring the consistency and complementary properties of different views, multi-view learning is rendered more effective, more promising, and has better generalization ability than single-view learning.},
archivePrefix = {arXiv},
arxivId = {1304.5634},
author = {Xu, Chang and Tao, Dacheng and Xu, Chao},
doi = {10.1145/1553374.1553391},
eprint = {1304.5634},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Multiviewlearning/A Survey on Multi-view Learning.pdf:pdf},
isbn = {9781605585161},
issn = {1605585165},
pages = {1--59},
pmid = {8762716},
title = {{A Survey on Multi-view Learning}},
year = {2013}
}
@article{Celeux1995,
author = {Celeux, Gilles and Govaert, G{\'{e}}rard},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Celeux, Govaert - 1995 - Gaussian parsimonious mixture models(2).pdf:pdf},
journal = {Pattern Recognition},
number = {5},
pages = {781--793},
title = {{Gaussian parsimonious mixture models}},
volume = {28},
year = {1995}
}
@article{Teh2010a,
author = {Teh, Yee Whye},
file = {:data/fireback/davenpor/davenpor/Reading Groups/Adams Reading Group/Graphical Models Lecture Notes.pdf:pdf},
title = {{Graphical Models}},
year = {2010}
}
@article{Meijer2015,
author = {Meijer, Rosa J. and Krebs, Thijmen J P and Goeman, Jelle J.},
doi = {10.1515/sagmb-2013-0075},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Multiple Testing/A region-based multiple testing method for hypotheses ordered in space or time.pdf:pdf},
issn = {15446115},
journal = {Statistical Applications in Genetics and Molecular Biology},
keywords = {familywise error rate,multiple testing,region hypotheses},
number = {1},
pages = {1--19},
title = {{A region-based multiple testing method for hypotheses ordered in space or time}},
volume = {14},
year = {2015}
}
@book{Nickl2015,
author = {Gin{\'{e}}, Evarist and Nickl, Richard},
doi = {10.1017/CBO9781107337862},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gin{\'{e}}, Nickl - 2015 - Mathematical Foundations of Infinite-Dimensional Statistical Models.pdf:pdf},
isbn = {9781107043169},
title = {{Mathematical Foundations of Infinite-Dimensional Statistical Models}},
year = {2015}
}
@article{Reid2014,
abstract = {Recent developments by Lee et al (2013) in post selection inference for the Lasso are adapted to the orthogonal setting whereby sample elements have different underlying signal sizes. We show that other selection procedures, like selecting the {\$}K{\$} largest (absolute) sample elements and the Benjamini-Hochberg procedure, can be cast into their framework, allowing us to leverage their results. Point and interval estimates for signal sizes are proposed. These seem to perform quite well against competitors, both recent and more tenured.},
archivePrefix = {arXiv},
arxivId = {arXiv:1405.3340v3},
author = {Reid, Stephen and Taylor, Jonathan and Tibshirani, Robert J},
doi = {10.1002/cjs.11320},
eprint = {arXiv:1405.3340v3},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Reid, Taylor, Tibshirani - 2014 - Post selection point and interval estimation of signal sizes in Gaussian samples(2).pdf:pdf},
issn = {1708945X},
journal = {arXiv preprint arXiv:1405.3340},
number = {1},
pages = {1--22},
title = {{Post selection point and interval estimation of signal sizes in Gaussian samples}},
year = {2014}
}
@book{Leadbetter1967,
author = {Leadbetter, M.R and Rootzen, H},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leadbetter, Rootzen - Unknown - On Extreme Values in Stationary Random Fields.pdf:pdf},
title = {{On Extreme Values in Stationary Random Fields}}
}
@article{Winkler2016,
author = {Berger, R},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Berger - 1982 - Multiparameter hypethesis testing and acceptance sampling.pdf:pdf},
journal = {Technometrics},
keywords = {consumer,multiple inference,producer,s risk},
number = {4},
pages = {295--300},
title = {{Multiparameter hypethesis testing and acceptance sampling}},
volume = {24},
year = {1982}
}
@article{Hadwiger1972,
abstract = {We discuss the Euler Characteristic and some of the consequences of its topo- logical invariance. We start by following the path of history that motivated the study of this characteristic. Several equivalent definitions are given, along with some advan- tages of each. We then discuss how the Euler Characteristic is related to graph coloring in the Heawood coloring theorem, continuous tangent vector fields on a manifold in the Poincar{\'{e}}-Hopf index theorem, and differential geometry in the Gauss-Bonnet theorem.},
author = {Hadwiger, H and Mani, P},
doi = {10.1090/S0002-9939-1962-0137109-2},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hadwiger, Mani - 1972 - On the Euler characteristic of polyhedra.pdf:pdf},
issn = {0002-9939},
journal = {Proceedings of the American Mathematical Society},
number = {38},
pages = {139--143},
publisher = {Oxford Union Society},
title = {{On the Euler characteristic of polyhedra}},
volume = {19},
year = {1972}
}
@article{Benjamini2005a,
abstract = {Often in applied research, confidence intervals (CIs) are constructed or reported only for parameters selected after viewing the data. We show that such selected intervals fail to provide the assumed coverage probability. By generalizing the false discover), rate (FDR) approach from multiple testing to selected multiple CIs, we suggest the false coverage-statement rate (FCR) as a measure of interval coverage following selection. A general procedure is then introduced, offering FCR control at level q under any selection rule. The procedure constructs a marginal CI for each selected parameter, but instead of the confidence level 1 - q being used marginally, q is divided by the number of parameters considered and multiplied by the number selected. If we further use the FDR controlling testing procedure of Benjamini and Hochberg for selecting the parameters, the newly suggested procedure offers CIs that are dual to the testing procedure and are shown to be optimal in the independent case. Under the positive regression dependency condition of Benjamini and Yekutieli, the FCR is controlled for one-sided tests and CIs, as well as for a modification for two-sided testing. Results for general dependency are also given. Finally, using the equivalence of the CIs to testing, we prove that the procedure of Benjamini and Hochberg offers directional FDR control as conjectured.},
author = {Benjamini, Yoav and Yekutieli, Daniel},
doi = {10.1198/016214504000001907},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Benjamini, Yekutieli - 2005 - False Discovery Rate–Adjusted Multiple Confidence Intervals for Selected Parameters(2).pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {confidence interval,directional decision,false discovery rate,multiple comparison procedure,positive regression dependency,simultaneous,type iii error},
number = {469},
pages = {71--81},
title = {{False Discovery Rate–Adjusted Multiple Confidence Intervals for Selected Parameters}},
url = {http://www.tandfonline.com/doi/abs/10.1198/016214504000001907},
volume = {100},
year = {2005}
}
@article{Davies2016,
author = {Davies, Robert B},
file = {:data/fireback/davenpor/davenpor/Referreeing/Testing One Hypothesis Multiple Times- The Multidimensional Case/1987 Hypothesis Testing when a Nuisance Parameter is Present Only Under the Alternatives.pdf:pdf},
number = {1},
pages = {33--43},
title = {{Biometrika Trust Hypothesis Testing when a Nuisance Parameter is Present Only Under the Alternatives Published by : Oxford University Press on behalf of Biometrika Trust Stable URL : http://www.jstor.org/stable/2336019 Your use of the JSTOR archive indica}},
volume = {74},
year = {2016}
}
@article{Pan2002,
abstract = {The generalized estimating equation (GEE) approach is widely used in regression analyses with correlated response data. Under mild conditions, the resulting regression coefficient estimator is consistent and asymptotically normal with its variance being consistently estimated by the so-called sandwich estimator. Statistical inference is thus accomplished by using the asymptotic Wald chi-squared test. However, it has been noted in the literature that for small samples the sandwich estimator may not perform well and may lead to much inflated type I errors for the Wald chi-squared test. Here we propose using an approximate t- or F-test that takes account of the variability of the sandwich estimator. The level of type I error of the proposed t- or F-test is guaranteed to be no larger than that of the Wald chi-squared test. The satisfactory performance of the proposed new tests is confirmed in a simulation study. Our proposal also has some advantages when compared with other new approaches based on direct modifications of the sandwich estimator, including the one that corrects the downward bias of the sandwich estimator. In addition to hypothesis testing, our result has a clear implication on constructing Wald-type confidence intervals or regions.},
author = {Pan, Wei and Wall, Melanie M.},
doi = {10.1002/sim.1142},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pan, Wall - 2002 - Small-sample adjustments in using the sandwich variance estimator in generalized estimating equations.pdf:pdf},
isbn = {1097-0258},
issn = {02776715},
journal = {Statistics in Medicine},
keywords = {Bias correction,F-test,GEE,Robust variance estimator,Wald chi-squared test,t-test,z-test},
number = {10},
pages = {1429--1441},
pmid = {12185894},
title = {{Small-sample adjustments in using the sandwich variance estimator in generalized estimating equations}},
volume = {21},
year = {2002}
}
@article{Liang1986,
abstract = {This paper proposes an extension of generalized linear models to the analysis of longitudinal data. We introduce a class of estimating equations that give consistent estimates of the regression parameters and of their variance under mild assumptions about the time dependence. The estimating equations are derived without specifying the joint distribution of a subject's observations yet they reduce to the score equations for multivariate Gaussian outcomes. Asymptotic theory is presented for the general class of estimators. Specific cases in which we assume independence, m-dependence and exchangeable correlation structures from each subject are discussed. Efficiency of the proposed estimators in two simple situations is considered. The approach is closely related to quasi-likelihood.},
author = {Liang, Kung-Yee and Zeger, Scott L},
doi = {10.1093/biomet/73.1.13},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Random Effects/Longitudinal Data Analysis Using Generalized Linear Models.pdf:pdf},
isbn = {0006-3444},
issn = {00063444},
journal = {Biometrika Trust},
keywords = {Estimating equation,Generalized linear model,Longitudinal data,Quasi-likelihood,Repeated measures.},
number = {1},
pages = {13--22},
pmid = {136},
title = {{Longitudinal data analysis using generalized linear models}},
volume = {73},
year = {1986}
}
@article{Hager1989,
author = {Hager, W},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/fMRI/Big Data/UPDATING THE INVERSE OF A MATRIX*.pdf:pdf},
journal = {SIAM Review},
keywords = {1,65-02,65f05,ams,concerning various applications of,expo-,history,matrix perturbations,matrix updates,mos,response to gene golub,s suggestion that an,sherman-morrison,sitory paper be prepared,subject classifications,the sherman-morrison,this paper is in,woodbury},
number = {2},
pages = {221--239},
title = {{Updating the Inverse of a Matrix}},
volume = {31},
year = {1989}
}
@article{Fan2017,
abstract = {Over the last two decades, many exciting variable selection methods have been developed for finding a small group of covariates that are associated with the response from a large pool. Can the discoveries by such data mining approaches be spurious due to high-dimensionality and limited sample size? Can our fundamental assumptions on exogeneity of covariates needed for such variable selection be validated with the data? To answer these questions, we need to derive the distributions of the maximum spurious correlations given certain number of predictors, namely, the distribution of the correlation of a response variable {\$}Y{\$} with the best {\$}s{\$} linear combinations of {\$}p{\$} covariates {\$}\backslashmathbf{\{}X{\}}{\$}, even when {\$}\backslashmathbf{\{}X{\}}{\$} and {\$}Y{\$} are independent. When the covariance matrix of {\$}\backslashmathbf{\{}X{\}}{\$} possesses the restricted eigenvalue property, we derive such distributions for both finite {\$}s{\$} and diverging {\$}s{\$}, using Gaussian approximation and empirical process techniques. However, such a distribution depends on the unknown covariance matrix of {\$}\backslashmathbf{\{}X{\}}{\$}. Hence, we propose a multiplier bootstrap method to approximate the unknown distributions and establish the consistency of such a simple bootstrap approach. The results are further extended to the situation where residuals are from regularized fits. Our approach is then applied to construct the upper confidence limit for the maximum spurious correlation and testing exogeneity of covariates. The former provides a baseline for guiding false discoveries due to data mining and the latter tests whether our fundamental assumptions for high-dimensional model selection are statistically valid. Our techniques and results are illustrated by both numerical examples.},
archivePrefix = {arXiv},
arxivId = {1502.04237},
author = {Fan, Jianqing and Shao, Qi-Man and Zhou, Wen-Xin},
eprint = {1502.04237},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Asymptotic Statistics/ARE DISCOVERIES SPURIOUS? DISTRIBUTIONS OF MAXIMUM SPURIOUS CORRELATIONS AND THEIR APPLICATIONS.pdf:pdf},
journal = {Annals of Statistics},
pages = {48},
title = {{Are Discoveries Spurious? Distributions of Maximum Spurious Correlations and Their Applications}},
year = {2015}
}
@article{Drton2007,
abstract = {Graphical models provide a framework for exploration of multivariate dependence patterns. The connection between graph and statistical model is made by identifying the vertices of the graph with the observed variables and translating the pattern of edges in the graph into a pattern of conditional independences that is imposed on the variables' joint distribution. Focusing on Gaussian models, we review classical graphical models. For these models the defining conditional independences are equivalent to vanishing of certain (partial) correlation coefficients associated with individual edges that are absent from the graph. Hence, Gaussian graphical model selection can be performed by multiple testing of hypotheses about vanishing (partial) correlation coefficients. We show and exemplify how this approach allows one to perform model selection while controlling error rates for incorrect edge inclusion.},
archivePrefix = {arXiv},
arxivId = {arXiv:math/0508267v3},
author = {Drton, M and Perlman, M D},
doi = {10.1214/088342307000000113},
eprint = {0508267v3},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Drton, Perlman - 2007 - Multiple testing and error control in Gaussian graphical model selection(2).pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Acyclic directed graph, Bayesian network, bidirect,acyclic directed graph,and phrases,bayesian network,bidi-,chain graph,concentration graph,covariance graph,dag,graphical model,multiple testing,rected graph,undirected graph},
number = {3},
pages = {430--449 ST  -- Multiple testing and error control i},
primaryClass = {arXiv:math},
title = {{Multiple testing and error control in Gaussian graphical model selection}},
volume = {22},
year = {2007}
}
@article{Efron2004,
abstract = {The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a C p estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.},
archivePrefix = {arXiv},
arxivId = {arXiv:math/0406456v2},
author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert J and Ishwaran, Hemant and Knight, Keith and Loubes, Jean Michel and Massart, Pascal and Madigan, David and Ridgeway, Greg and Rosset, Saharon and Zhu, J. I. and Stine, Robert A. and Turlach, Berwin A. and Weisberg, Sanford},
doi = {10.1214/009053604000000067},
eprint = {0406456v2},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Efron et al. - 2004 - Least angle regression(2).pdf:pdf},
isbn = {00905364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Boosting,Coefficient paths,Lasso,Linear regression,Variable selection},
number = {2},
pages = {407--499},
pmid = {1000198917},
primaryClass = {arXiv:math},
title = {{Least angle regression}},
volume = {32},
year = {2004}
}
@article{Azzimonti2016,
abstract = {We focus on the problem of estimating and quantifying uncertainties on the excursion set of a function under a limited evaluation budget. We adopt a Bayesian approach where the objective function is assumed to be a realization of a random field, typically assumed Gaussian. In this setting, the posterior distribution on the objective function gives rise to a poste-rior distribution of excursion sets. Several approaches exist to summarize the distribution of the excursion sets based on random closed set the-ory. While the recently proposed Vorob'ev approach leads to analytically tractable expectations, further notions of variability require Monte Carlo estimators relying on Gaussian random field conditional simulations. In the present work we propose a method to choose simulation points and obtain realizations of the conditional field at fine designs through affine predictors. The points are chosen optimally in the sense that they mini-mize the expected distance in measure between the posterior excursion set and its reconstruction. The proposed method reduces the computational costs due to simulations and enables the prediction of realizations on fine designs even in large dimensions. We apply this reconstruction approach to obtain realizations of an excursion set on a fine grid which allow us to give a new measure of uncertainty based on the distance transform of the excursion set. Finally we present an application of the method where the distribution of the volume of excursion is estimated in a six-dimensional example.},
archivePrefix = {arXiv},
arxivId = {1501.03659},
author = {Azzimonti, Dario and Bect, Julien and Chevalier, Cl{\'{e}}ment and Ginsbourger, David},
doi = {10.1137/141000749},
eprint = {1501.03659},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Random Fields/Quantifying Uncertainties on Excursion Sets Under a Gaussian Random Field.pdf:pdf},
isbn = {1501.03659},
issn = {2166-2525},
journal = {SIAM/ASA Journal on Uncertainty Quantification},
keywords = {1,10,1137,141000749,60g15,62m30,ams subject classifications,application fields where mathematical,conditional simulations,distance transform,doi,gaussian processes,in a number of,introduction,models are used,set estimation},
number = {1},
pages = {850--874},
title = {{Quantifying Uncertainties on Excursion Sets Under a Gaussian Random Field Prior}},
volume = {4},
year = {2016}
}
@book{Efron1982,
abstract = {The jackknife and the bootstrap are nonparametric methods for assessing the errors in a statistical estimation problem. They provide several advantages over the traditional parametric approach: the methods are easy to describe and they apply to arbitrarily complicated situations; distribution assumptions, such as normality, are never made. This monograph connects the jackknife, the bootstrap, and many other related ideas such as cross-validation, random subsampling, and balanced repeated replications into a unified exposition. The theoretical development is at an easy mathematical level and is supplemented by a large number of numerical examples. The methods described in this monograph form a useful set of tools for the applied statistician. They are particularly useful in problem areas where complicated data structures are common, for example, in censoring, missing data, and highly multivariate situations.},
author = {Efron, Bradley},
doi = {10.1137/1.9781611970319},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Efron - 1982 - The Jackknife, the Bootstrap and Other Resampling Plans.pdf:pdf},
isbn = {978-0-89871-179-0},
issn = {01621459},
keywords = {bias,bootstrap,consistency,jacknife},
mendeley-tags = {bias,bootstrap,consistency,jacknife},
pages = {103},
pmid = {428247},
title = {{The Jackknife, the Bootstrap and Other Resampling Plans}},
url = {http://epubs.siam.org/doi/book/10.1137/1.9781611970319},
year = {1982}
}
@article{Baldi1989,
author = {Baldi, P and Rinott, Yosef},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/Random{\_}Field{\_}Theory/Other/Lattice/Coupled map lattices as spatio-temporal fitness functions$\backslash$: Landscape measures and evolutionary optimization.pdf:pdf},
number = {4},
pages = {1646--1650},
title = {{On Normal Approximations of Distributions in Terms of Dependency Graphs}},
volume = {17},
year = {1989}
}
@article{Knapp1963,
author = {Knapp, Sandy},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Knapp - 1963 - Contact(2).pdf:pdf},
isbn = {4034383364},
number = {February},
title = {{Contact :}},
year = {1963}
}
@article{ChengSupp2017,
author = {Cheng, By Dan and Schwartzman, Armin},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/Random{\_}Field{\_}Theory/Schwartzman/Supplem for Local Maxima.pdf:pdf},
number = {1},
pages = {1--12},
title = {{SUPPLEMENT TO “MULTIPLE TESTING OF LOCAL MAXIMA FOR DETECTION OF PEAKS IN RANDOM FIELDS}}
}
@article{Smith1982,
author = {Smith, Norman and Tong, Y. L.},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Order statistics/INEQUALITIES FOR FUNCTIONS OF ORDER STATISTICS UNDER AN ADDITIVE MODEL.pdf:pdf},
keywords = {and phrases,inequalities,location parameter families,majorization and rearrangements,or heterogeneous,order statistics from dependent,populations},
pages = {255--265},
title = {{Inequalities for functions of order statistics under an additive model}},
volume = {35},
year = {1983}
}
@article{Tierney2016,
abstract = {Random Field Theory has been used in the fMRI literature to address the multiple comparisons problem. The method provides an analytical solution for the computation of precise p-values when its assumptions are met. When its assumptions are not met the thresholds generated by Random Field Theory can be more conservative than Bonferroni corrections, which are arguably too stringent for use in fMRI. As this has been well documented theoretically it is surprising that a majority of current studies ({\~{}}80{\%}) would not meet the assumptions of Random Field Theory and therefore would have reduced sensitivity. Specifically most data is not smooth enough to meet the good lattice assumption. Current studies smooth data on average by twice the voxel size which is rarely sufficient to meet the good lattice assumption. The amount of smoothing required for Random Field Theory to produce accurate p-values increases with image resolution and decreases with degrees of freedom. There is no rule of thumb that is valid for all study designs but for typical data (3mm resolution, and greater than 20 subjects) residual smoothness with FWHM = 4 times voxel size should produce valid results. However, it should be stressed that for higher spatial resolution and lower degrees of freedom the critical smoothness required will increase sharply. This implies that researchers should carefully choose appropriate smoothing kernels. This can be facilitated by the simulations we provide that identify the critical smoothness at which the application of RFT becomes appropriate. For some applications such as presurgical mapping or, imaging of small structures, probing the laminar/columnar structure of the cortex these smoothness requirements may be too great to preserve spatial structure. As such, this study suggests developments are needed in Random Field Theory to fully exploit the resolution of modern neuroimaging.},
archivePrefix = {arXiv},
arxivId = {1607.08205},
author = {Tierney, Tim M. and Clark, Christopher A. and Carmichael, David W.},
eprint = {1607.08205},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tierney, Clark, Carmichael - 2016 - Is Bonferroni correction more sensitive than Random Field Theory for most fMRI studies.pdf:pdf},
journal = {arXiv preprint},
number = {0},
title = {{Is Bonferroni correction more sensitive than Random Field Theory for most fMRI studies?}},
volume = {44},
year = {2016}
}
@article{Efron1997,
abstract = {A study investigates the error rate of a rule for predicting future responses constructed from a training set of data. Results are nonparametric and apply to any possible prediction rule.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Efron, B. and Tibshirani, R.},
doi = {10.1080/01621459.1997.10474007},
eprint = {arXiv:1011.1669v3},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Resampling/Improvements on Cross Validation The 632 Bootstrap Method.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {classification,cross-validation bootstrap,prediction rule},
number = {438},
pages = {548},
pmid = {370},
title = {{Improvements on cross-validation: The .632 plus bootstrap method}},
volume = {92},
year = {1997}
}
@article{Robertson2015,
abstract = {When developing a new diagnostic test for a disease, there are often multiple candidate classifiers to choose from, and it is unclear if any will offer an improvement in performance compared with current technology. A two-stage design can be used to select a promising classifier (if one exists) in stage one for definitive validation in stage two. However, estimating the true properties of the chosen classifier is complicated by the first stage selection rules. In particular, the usual maximum likelihood estimator (MLE) that combines data from both stages will be biased high. Consequently, confidence intervals and p-values flowing from the MLE will also be incorrect. Building on the results of Pepe et al. (SIM 28:762-779), we derive the most efficient conditionally unbiased estimator and exact confidence intervals for a classifier's sensitivity in a two-stage design with arbitrary selection rules; the condition being that the trial proceeds to the validation stage. We apply our estimation strategy to data from a recent family history screening tool validation study by Walter et al. (BJGP 63:393-400) and are able to identify and successfully adjust for bias in the tool's estimated sensitivity to detect those at higher risk of breast cancer.},
author = {Robertson, David S. and Prevost, A. Toby and Bowden, Jack},
doi = {10.1002/sim.6413},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Robertson, Prevost, Bowden - 2015 - Correcting for bias in the selection and validation of informative diagnostic tests.pdf:pdf},
isbn = {0277-6715},
issn = {10970258},
journal = {Statistics in Medicine},
keywords = {Diagnostic tests,Family history,Group sequential design,Uniformly minimum variance unbiased estimator},
number = {8},
pages = {1417--1437},
pmid = {25645331},
title = {{Correcting for bias in the selection and validation of informative diagnostic tests}},
volume = {34},
year = {2015}
}
@article{Hadwiger1959,
author = {Hadwiger, H},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hadwiger - 1959 - Normale Korper im euklidischen Raum und ihre topologischen und metrischen Eigenschaften.pdf:pdf},
pages = {124--140},
title = {{Normale Korper im euklidischen Raum und ihre topologischen und metrischen Eigenschaften}},
volume = {71},
year = {1959}
}
@article{James2002,
abstract = {To cite this article: Gareth M. (2002) with functional predictors Journal of the Royal Statistical Society: Series B},
author = {James, Gareth M.},
doi = {10.1111/1467-9868.00342},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/James - 2002 - Generalized linear models with functional predictors(2).pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Censored regression,Functional data analysis,Functional principal components,Generalized linear models,Logistic regression},
number = {3},
pages = {411--432},
title = {{Generalized linear models with functional predictors}},
volume = {64},
year = {2002}
}
@article{Benjamini2001,
abstract = {Benjamini and Hochberg suggest that the false discovery rate may be the appropriate error rate to control in many applied multiple testing problems. A simple procedure was given there as an FDR controlling procedure for independent test statistics and was shown to be much more powerful than comparable procedures which control the traditional family wise error rate. We prove that this same procedure also controls the false discovery rate when the test statistics have positive regression dependency on each of the test statistics corresponding to the true null hypotheses. This condition for positive dependency is general enough to cover many problems of practical interest, including the comparisons of many treatments with a single control, multivariate normal test statistics with positive correlation matrix and multivariate t. Furthermore, the test statistics may be discrete, and the tested hypotheses composite without posing special difficulties. For all other forms of dependency, a simple conservative modification of the procedure controls the false discovery rate. Thus the range of problems for which a procedure with proven FDR control can be offered is greatly increased.},
archivePrefix = {arXiv},
arxivId = {0801.1095},
author = {Benjamini, Yoav and Yekutieli, Daniel},
doi = {10.1214/aos/1013699998},
eprint = {0801.1095},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Benjamini, Yekutieli - 2001 - The control of the false discovery rate in multiple testing under depencency.pdf:pdf},
isbn = {0090-5364},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {4},
pages = {1165--1188},
pmid = {18298808},
title = {{The control of the false discovery rate in multiple testing under depencency}},
volume = {29},
year = {2001}
}
@article{Tibishrani2015,
author = {Tibshirani, Robert J},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tibshirani - 2015 - Additive Models Generalized(2).pdf:pdf},
number = {3},
pages = {297--310},
title = {{Additive Models Generalized}},
volume = {1},
year = {2015}
}
@article{Sen1970,
author = {Sen, P. K},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Order statistics/A Note on Order Statistics for Heterogeneous Distributions.pdf:pdf},
number = {6},
pages = {2137--2139},
title = {{A Note on Order Statistics for Heterogeneous Distributions}},
volume = {41},
year = {2017}
}
@article{Battiti1994,
abstract = {This paper investigates the application of the mutual information criterion to evaluate a set of candidate features and to select an informative subset to be used as input data for a neural network classifier. Because the mutual information measures arbitrary dependencies between random variables, it is suitable for assessing the `'information content'' of features in complex classification tasks, where methods bases on linear relations (like the correlation) are prone to mistakes. The fact that the mutual information is independent of the coordinates chosen permits a robust estimation. Nonetheless, the use of the mutual information for tasks characterized by high input dimensionality requires suitable approximations because of the prohibitive demands on computation and samples. An algorithm is proposed that is based on a `'greedy'' selection of the features and that takes both the mutual information with respect to the output class and with respect to the already-selected features into account. Finally the results of a series of experiments are discussed.},
author = {Battiti, R},
doi = {10.1109/72.298224},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Battiti - 1994 - Using Mutual Information for Selecting Features in Supervised Neural-Net Learning(2).pdf:pdf},
isbn = {1045-9227},
issn = {1045-9227},
journal = {Ieee Transactions on Neural Networks},
number = {4},
pages = {537--550},
pmid = {18267827},
title = {{Using Mutual Information for Selecting Features in Supervised Neural-Net Learning}},
volume = {5},
year = {1994}
}
@article{Lauritzen2004,
author = {Lauritzen, Steffen},
doi = {10.1007/BF01893286},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Unbiased Estimation/Sufficiency and Unbiased Estimation.pdf:pdf},
issn = {00261335},
title = {{Sufficiency and unbiased estimation}},
year = {2004}
}
@article{Algeri2016,
author = {Algeri, Sara and Prof, Supervisors and Dyk, David Van and Conrad, Prof Jan},
file = {:data/fireback/davenpor/davenpor/Referreeing/Testing One Hypothesis Multiple Times- The Multidimensional Case/Presentation.pdf:pdf},
title = {{MOTH Slides}},
year = {2016}
}
@article{Westfall2011,
abstract = {There are many ways to bootstrap data for multiple comparisons procedures. Methods described here include (i) bootstrap (parametric and nonparametric) as a generalization of classical normal-based MaxT methods, (ii) bootstrap as an approximation to exact permutation methods, (iii) bootstrap as a generator of realistic null data sets, and (iv) bootstrap as a generator of realistic non-null data sets. Resampling of MinP versus MaxT is discussed, and the use of the bootstrap for closed testing is also presented. Applications to biopharmaceutical statistics are given.},
author = {Westfall, Peter H.},
doi = {10.1080/10543406.2011.607751},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Westfall - 2011 - On Using the Bootstrap for Multiple Comparisons.pdf:pdf},
isbn = {1520-5711 (Electronic)$\backslash$r1054-3406 (Linking)},
issn = {1054-3406},
journal = {Journal of Biopharmaceutical Statistics},
number = {6},
pages = {1187--1205},
pmid = {22023686},
title = {{On Using the Bootstrap for Multiple Comparisons}},
url = {http://www.tandfonline.com/doi/abs/10.1080/10543406.2011.607751},
volume = {21},
year = {2011}
}
@article{Ross2010,
abstract = {We compute upper and lower bounds on the expected maximum of correlated normal variables (up to a few hundred in number) with arbitrary means, variances, and correlations. Two types of bounding processes are used: perfectly dependent normal variables, and independent normal variables, both with arbitrary mean values. The expected maximum for the perfectly dependent variables can be evaluated in closed form; for the independent variables, a single numerical integration is required. Higher moments are also available. We use mathematical programming to find parameters for the processes, so they will give bounds on the expected maximum, rather than approximations of unknown accuracy. Our original application is to the maximum number of people on-line simultaneously during the day in an infinite-server queue with a time-varying arrival rate. The upper and lower bounds are tighter than previous bounds, and in many of our examples are within 5{\%} or 10{\%} of each other. We also demonstrate the bounds' performance on some PERT models, AR/MA time series, Brownian motion, and product-form correlation matrices.},
author = {Ross, Andrew M.},
doi = {10.1007/s11009-008-9097-z},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/EVT/Useful Bounds on the Expected Maximum of.pdf:pdf},
issn = {13875841},
journal = {Methodology and Computing in Applied Probability},
keywords = {Computing bounds,Multivariate normal expectations,Singular distribution},
number = {1},
pages = {111--138},
title = {{Computing bounds on the expected maximum of correlated normal variables}},
volume = {12},
year = {2010}
}
@article{Mittal1974,
author = {Mittal, Y and Ylvisaker, D},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mittal, Ylvisaker - 1974 - Limit Distributions for the Maxima of Stationary Gaussian Processes(2).pdf:pdf},
journal = {Stochastic Processes and their Applications},
keywords = {Normal Maxima},
mendeley-tags = {Normal Maxima},
pages = {1--18},
title = {{Limit Distributions for the Maxima of Stationary Gaussian Processes}},
volume = {3},
year = {1974}
}
@article{Benjamini2011,
abstract = {In many large multiple testing problems the hypotheses are divided into families. Given the data, families with evidence for true discoveries are selected, and hypotheses within them are tested. Neither controlling the error-rate in each family separately nor controlling the error-rate over all hypotheses together can assure that an error-rate is controlled in the selected families. We formulate this concern about selective inference in its generality, for a very wide class of error-rates and for any selection criterion, and present an adjustment of the testing level inside the selected families that retains the average error-rate over the selected families.},
archivePrefix = {arXiv},
arxivId = {1106.3670},
author = {Benjamini, Yoav and Bogomolov, Marina},
eprint = {1106.3670},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Multiple Testing/Selection Bias/Adjusting for selection bias in testing multiple families of hypotheses.pdf:pdf},
journal = {arXiv preprint arXiv:1106.3670},
pages = {1--32},
title = {{Adjusting for selection bias in testing multiple families of hypotheses}},
year = {2011}
}
@article{Hall1986,
author = {Hall, Peter},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hall - 1986 - On the Bootstrap and Confidence Intervals(2).pdf:pdf},
journal = {Annals of Statistics},
number = {4},
pages = {1431--1452},
title = {{On the Bootstrap and Confidence Intervals}},
volume = {14},
year = {1986}
}
@article{Davison2003,
	title={Recent developments in bootstrap methodology},
	author={Davison, Anthony C and Hinkley, David V and Young, G Alastair},
	journal={Statistical Science},
	pages={141--157},
	year={2003},
	publisher={JSTOR}
}
@article{Singh1981,
author = {Singh, Kesar},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Theory/On the Asymptotic Accuracy of Efron's Bootstrap.pdf:pdf},
journal = {Annals of Statistics},
number = {6},
pages = {1187--1195},
title = {{On the Asymptotic Accuracy of Efron's Bootstrap}},
volume = {9},
year = {1981}
}
@article{VanWieringen2015,
abstract = {The linear regression model cannot be fitted to high-dimensional data, as the high-dimensionality brings about empirical non-identifiability. Penalized regression overcomes this non-identifiability by augmentation of the loss function by a penalty (i.e. a function of regression coefficients). The ridge penalty is the sum of squared regression coefficients, giving rise to ridge regression. Here many aspect of ridge regression are reviewed e.g. moments, mean squared error, its equivalence to constrained estimation, and its relation to Bayesian regression. Finally, its behaviour and use are illustrated in simulation and on omics data.},
archivePrefix = {arXiv},
arxivId = {1509.09169},
author = {van Wieringen, Wessel N.},
eprint = {1509.09169},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/van Wieringen - 2015 - Lecture notes on ridge regression(2).pdf:pdf},
pages = {0--30},
title = {{Lecture notes on ridge regression}},
year = {2015}
}
@article{McHale2011,
abstract = {The paper introduces a model for forecasting match results for the top tier of men's professional tennis, the ATP tour. Employing a Bradley-Terry type model, and utilising the data available on players' past results and the surface of the contest, we predict match winners for the coming week's matches, having updated the model parameters to take the previous week's results into account. We compare the model to two logit models: one using official rankings and another using the official ranking points of the two competing players. Our model provides superior forecasts according to each of five criteria measuring the predictive performance, two of which relate to betting returns. ?? 2010 International Institute of Forecasters.},
archivePrefix = {arXiv},
arxivId = {arXiv:1210.1016v1},
author = {McHale, Ian and Morton, Alex},
doi = {10.1016/j.ijforecast.2010.04.004},
eprint = {arXiv:1210.1016v1},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Tennis/A Bradley-Terry type model for forecasting tennis match results.pdf:pdf},
isbn = {9781605585161},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Betting,Bradley-Terry model,Logit,Ranking evaluation,Sport},
number = {2},
pages = {619--630},
publisher = {Elsevier B.V.},
title = {{A Bradley-Terry type model for forecasting tennis match results}},
url = {http://dx.doi.org/10.1016/j.ijforecast.2010.04.004},
volume = {27},
year = {2011}
}
@article{Efron1983,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. We construct a prediction rule on the basis of some data, and then wish to estimate the error rate of this rule in classifying future observations. Cross-validation pro-vides a nearly unbiased estimate, using only the original data. Cross-validation turns out to be related closely to the bootstrap estimate of the error rate. This article has two purposes: to understand better the theoretical basis of the prediction problem, and to investigate some related estimators, which seem to offer considerably improved estimation in small samples.},
author = {Efron, Bradley},
doi = {10.1080/01621459.1983.10477973},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Theory/Estimating the Error Rate of a Prediction Rule$\backslash$: Improvement on Cross-Validation.pdf:pdf},
isbn = {01621459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {ANOVA decomposition,Bootstrap,Logistic regression,Prediction problem,anova,bootstrap,decomposition,logistic regression,prediction problem},
number = {382},
pages = {316--331},
title = {{Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation}},
volume = {78},
year = {1983}
}
@article{Penny2005,
abstract = {We describe a Bayesian estimation and inference procedure for fMRI time series based on the use of General Linear Models (GLMs). Importantly, we use a spatial prior on regression coefficients which embodies our prior knowledge that evoked responses are spatially contiguous and locally homogeneous. Further, using a computationally efficient Variational Bayes framework, we are able to let the data determine the optimal amount of smoothing. We assume an arbitrary order Auto-Regressive (AR) model for the errors. Our model generalizes earlier work on voxel-wise estimation of GLM-AR models and inference in GLMs using Posterior Probability Maps (PPMs). Results are shown on simulated data and on data from an event-related fMRI experiment. {\textcopyright} 2004 Elsevier Inc. All rights reserved.},
author = {Penny, William D. and Trujillo-Barreto, Nelson J. and Friston, Karl J.},
doi = {10.1016/j.neuroimage.2004.08.034},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Penny, Trujillo-Barreto, Friston - 2005 - Bayesian fMRI time series analysis with spatial priors.pdf:pdf},
isbn = {1053-8119},
issn = {10538119},
journal = {NeuroImage},
keywords = {Autoregressive model,Effect-size,General linear model,Laplacian,Smoothing,Spatial priors,Variational Bayes,fMRI},
number = {2},
pages = {350--362},
pmid = {15627578},
title = {{Bayesian fMRI time series analysis with spatial priors}},
volume = {24},
year = {2005}
}
@article{Anders1999,
abstract = {In this article, we examine how model selection in neural networks can be guided by statistical procedures such as hypothesis tests, information criteria and cross validation. The application of these methods in neural network models is discussed, paying attention especially to the identification problems encountered. We then propose five specification strategies based on different statistical procedures and compare them in a simulation study. As the results of the study are promising, it is suggested that a statistical analysis should become an integral part of neural network modeling.},
author = {Anders, Ulrich and Korn, Olaf},
doi = {10.1016/S0893-6080(98)00117-8},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Anders, Korn - 1999 - Model selection in neural networks(2).pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Cross validation,Identification,Information criteria,Model selection,Monte Carlo simulation,Neural networks,Pruning,Statistical inference},
number = {2},
pages = {309--323},
pmid = {12662706},
title = {{Model selection in neural networks}},
volume = {12},
year = {1999}
}
@article{Tibshirani2016,
abstract = {We propose new inference tools for forward stepwise regression, least angle regression, and the lasso. Assuming a Gaussian model for the observation vector y, we first describe a general scheme to perform valid inference after any selection event that can be characterized as y falling into a polyhedral set. This framework allows us to derive conditional (post-selection) hypothesis tests at any step of forward stepwise or least angle regression, or any step along the lasso regularization path, because, as it turns out, selection events for these procedures can be expressed as polyhedral constraints on y. The p-values associated with these tests are exactly uniform under the null distribution, in finite samples, yielding exact type I error control. The tests can also be inverted to produce confidence intervals for appropriate underlying regression parameters. The R package "selectiveInference", freely available on the CRAN repository, implements the new inference tools described in this paper.},
archivePrefix = {arXiv},
arxivId = {1401.3889},
author = {Tibshirani, Ryan J. and Taylor, Jonathan and Lockhart, Richard and Tibshirani, Robert J},
doi = {10.1080/01621459.2015.1108848},
eprint = {1401.3889},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tibshirani et al. - 2016 - Exact Post-Selection Inference for Sequential Regression Procedures(2).pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {confidence interval,forward stepwise regression,lasso,least angle regression,p-value,post-},
number = {514},
pages = {600--620},
title = {{Exact Post-Selection Inference for Sequential Regression Procedures}},
volume = {111},
year = {2016}
}
@article{Tian2015,
abstract = {Inspired by sample splitting and the reusable holdout introduced in the field of differential privacy, we consider selective inference with a randomized response. Using a randomized response can ensure that the leftover information of Fithian et al.(2014) is bounded below ensuring that selective intervals are better behaved than without randomization. Under independent sampling, we prove a selective (or privatized) central limit theorem that transfers procedures valid under asymptotic normality without selection to their corresponding selective counterparts. This allows selective inference in the nonparametric settings. Finally, we describe a method for selective inference following cross-validation using slightly more randomization than the split into groups of standard cross-validation. We focus on the classical asymptotic setting, leaving the interesting high-dimensional asymptotic questions for future work.},
archivePrefix = {arXiv},
arxivId = {1507.06739},
author = {Tian, Xiaoying and Taylor, Jonathan},
eprint = {1507.06739},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tian, Taylor - 2015 - Selective inference with a randomized response(2).pdf:pdf},
keywords = {and phrases,differential privacy,nonparametric,selective inference},
number = {2014},
pages = {1--34},
title = {{Selective inference with a randomized response}},
year = {2015}
}
@article{Stephens2017,
abstract = {We introduce a new Empirical Bayes approach for large-scale hypothesis testing, including estimating false discovery rates (FDRs), and effect sizes. This approach has two key differences from existing approaches to FDR analysis. First, it assumes that the distribution of the actual (unobserved) effects is unimodal, with a mode at 0. This “unimodal assumption” (UA), although natural in many contexts, is not usually incorporated into standard FDR analysis, and we demonstrate how incorporating it brings many benefits. Specifically, the UA facilitates efficient and robust computation—estimating the unimodal distribution involves solving a simple convex optimization problem—and enables more accurate inferences provided that it holds. Second, the method takes as its input two numbers for each test (an effect size estimate and corresponding standard error), rather than the one number usually used ({\$}{\$}p{\$}{\$} value or {\$}{\$}z{\$}{\$} score). When available, using two numbers instead of one helps account for variation in measurement precision across tests. It also facilitates estimation of effects, and unlike standard FDR methods, our approach provides interval estimates (credible regions) for each effect in addition to measures of significance. To provide a bridge between interval estimates and significance measures, we introduce the term “local false sign rate” to refer to the probability of getting the sign of an effect wrong and argue that it is a superior measure of significance than the local FDR because it is both more generally applicable and can be more robustly estimated. Our methods are implemented in an R package ashr available from http://github.com/stephens999/ashr.},
author = {Stephens, Matthew},
doi = {10.1093/biostatistics/kxw041},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Multiple Testing/Bayesian/False discovery rates$\backslash$: a new deal.pdf:pdf},
issn = {14684357},
journal = {Biostatistics},
keywords = {Empirical Bayes,False discovery rates,Multiple testing,Shrinkage,Unimodal},
number = {2},
pages = {275--294},
pmid = {27756721},
title = {{False discovery rates: A new deal}},
volume = {18},
year = {2017}
}
@article{Hupe2015,
abstract = {Published studies using functional and structural MRI include many errors in the way data are analyzed and conclusions reported. This was observed when working on a comprehensive review of the neural bases of synesthesia, but these errors are probably endemic to neuroimaging studies. All studies reviewed had based their conclusions using Null Hypothesis Significance Tests (NHST). NHST have yet been criticized since their inception because they are more appropriate for taking decisions related to a Null hypothesis (like in manufacturing) than for making inferences about behavioral and neuronal processes. Here I focus on a few key problems of NHST related to brain imaging techniques, and explain why or when we should not rely on "significance" tests. I also observed that, often, the ill-posed logic of NHST was even not correctly applied, and describe what I identified as common mistakes or at least problematic practices in published papers, in light of what could be considered as the very basics of statistical inference. MRI statistics also involve much more complex issues than standard statistical inference. Analysis pipelines vary a lot between studies, even for those using the same software, and there is no consensus which pipeline is the best. I propose a synthetic view of the logic behind the possible methodological choices, and warn against the usage and interpretation of two statistical methods popular in brain imaging studies, the false discovery rate (FDR) procedure and permutation tests. I suggest that current models for the analysis of brain imaging data suffer from serious limitations and call for a revision taking into account the "new statistics" (confidence intervals) logic.},
author = {Hup{\'{e}}, Jean Michel},
doi = {10.3389/fnins.2015.00018},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/fMRI/NHST.pdf:pdf},
isbn = {1662-4548 (Print)$\backslash$r1662-453X (Linking)},
issn = {1662453X},
journal = {Frontiers in Neuroscience},
keywords = {False discovery rate,Null hypothesis significance test,Permutation tests,Random field theory,Statistical inference},
number = {FEB},
pages = {1--9},
pmid = {25745383},
title = {{Statistical inferences under the Null hypothesis: Common mistakes and pitfalls in neuroimaging studies}},
volume = {9},
year = {2015}
}
@article{Baldi1989,
abstract = {We consider the problem of learning from examples in layered linear feed-forward neural networks using optimization methods, such as back propagation, with respect to the usual quadratic error function E of the connection weights. Our main result is a complete description of the landscape attached to E in terms of principal component analysis. We show that E has a unique minimum corresponding to the projection onto the subspace generated by the first principal vectors of a covariance matrix associated with the training patterns. All the additional critical points of E are saddle points (corresponding to projections onto subspaces generated by higher order vectors). The auto-associative case is examined in detail. Extensions and implications for the learning algorithms are discussed. {\textcopyright} 1989.},
author = {Baldi, Pierre and Hornik, Kurt},
doi = {10.1016/0893-6080(89)90014-2},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Neural Nets/Neural Networks and Principal Component Analysis.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back propagation,Learning,Neural networks,Principal component analysis},
number = {1},
pages = {53--58},
title = {{Neural networks and principal component analysis: Learning from examples without local minima}},
volume = {2},
year = {1989}
}
@article{Deemer2017,
author = {Deemer, Walter L and Olkin, Ingram},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deemer, Olkin - 2017 - The Jacobians of Certain Matrix Transformations Useful in Multivariate Analysis(2).pdf:pdf},
number = {3},
pages = {345--367},
title = {{The Jacobians of Certain Matrix Transformations Useful in Multivariate Analysis}},
volume = {38},
year = {2017}
}
@article{Ambroise2002,
abstract = {In the context of cancer diagnosis and treatment, we consider the problem of constructing an accurate prediction rule on the basis of a relatively small number of tumor tissue samples of known type containing the expression data on very many (possibly thousands) genes. Recently, results have been presented in the literature suggesting that it is possible to construct a prediction rule from only a few genes such that it has a negligible prediction error rate. However, in these results the test error or the leave-one-out cross-validated error is calculated without allowance for the selection bias. There is no allowance because the rule is either tested on tissue samples that were used in the first instance to select the genes being used in the rule or because the cross-validation of the rule is not external to the selection process; that is, gene selection is not performed in training the rule at each stage of the cross-validation process. We describe how in practice the selection bias can be assessed and corrected for by either performing a cross-validation or applying the bootstrap external to the selection process. We recommend using 10-fold rather than leave-one-out cross-validation, and concerning the bootstrap, we suggest using the so-called .632+ bootstrap error estimate designed to handle overfitted prediction rules. Using two published data sets, we demonstrate that when correction is made for the selection bias, the cross-validated error is no longer zero for a subset of only a few genes.},
author = {Ambroise, Christophe and McLachlan, Geoffrey J},
doi = {10.1073/pnas.102102699},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ambroise, McLachlan - 2002 - Selection bias in gene extraction on the basis of microarray gene-expression data.pdf:pdf},
isbn = {0027-8424 (Print)$\backslash$r0027-8424 (Linking)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
number = {10},
pages = {6562--6},
pmid = {11983868},
title = {{Selection bias in gene extraction on the basis of microarray gene-expression data.}},
volume = {99},
year = {2002}
}
@article{Brown1977,
author = {Brown, G and Rutemiller, H},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/statistics/Means and Variances of Stochastic Vector Products with Applications to Random Linear.pdf:pdf},
journal = {Management Science},
keywords = {management},
mendeley-tags = {management},
number = {2},
pages = {210--216},
title = {{Means and Variances of Stochastic Vector Products with Applications to Random Linear Models}},
volume = {24},
year = {2016}
}
@article{Yau2011,
abstract = {We consider the development of Bayesian Nonparametric methods for product partition models such as Hidden Markov Models and change point models. Our approach uses a Mixture of Dirichlet Process (MDP) model for the unknown sampling distribution (likelihood) for the observations arising in each state and a computationally efficient data augmentation scheme to aid inference. The method uses novel MCMC methodology which combines recent retrospective sampling methods with the use of slice sampler variables. The methodology is computationally efficient, both in terms of MCMC mixing properties, and robustness to the length of the time series being investigated. Moreover, the method is easy to implement requiring little or no user-interaction. We apply our methodology to the analysis of genomic copy number variation.},
author = {Yau, Christopher and Papaspiliopoulos, O and Roberts, Gareth O. and Holmes, Chris C.},
doi = {10.1111/j.1467-9868.2010.00756.x},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yau et al. - 2011 - Bayesian Nonparametric Hidden Markov Models with application to the analysis of copy-number-variation in mammalia(2).pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society. Series B, Statistical methodology},
keywords = {block gibbs sampler,copy number variation,exchangeability,local and global clustering,partial,partition models,retrospective sampling},
pages = {37--57},
pmid = {21687778},
title = {{Bayesian Nonparametric Hidden Markov Models with application to the analysis of copy-number-variation in mammalian genomes.}},
volume = {73},
year = {2011}
}
@article{Zhao2013,
abstract = {Influence diagnosis is important since presence of influential observations could lead to distorted analysis and misleading interpretations. For high-dimensional data, it is particularly so, as the increased dimensionality and complexity may amplify both the chance of an observation being influential, and its potential impact on the analysis. In this article, we propose a novel high-dimensional influence measure for regressions with the number of predictors far exceeding the sample size. Our proposal can be viewed as a high-dimensional counterpart to the classical Cook's distance. However, whereas the Cook's distance quantifies the individual observation's influence on the least squares regression coefficient estimate, our new diagnosis measure captures the influence on the marginal correlations, which in turn exerts serious influence on downstream analysis including coefficient estimation, variable selection and screening. Moreover, we establish the asymptotic distribution of the proposed influence measure by letting the predictor dimension go to infinity. Availability of this asymptotic distribution leads to a principled rule to determine the critical value for influential observation detection. Both simulations and real data analysis demonstrate usefulness of the new influence diagnosis measure.},
archivePrefix = {arXiv},
arxivId = {arXiv:1311.6636v1},
author = {Zhao, Junlong and Leng, Chenlei and Li, Lexin and Wang, Hansheng},
doi = {10.1214/13-AOS1165},
eprint = {arXiv:1311.6636v1},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2013 - High-dimensional influence measure(2).pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Cook's distance,High-dimensional diagnosis,Influential observation,LASSO,Marginal correlations,Variable screening},
number = {5},
pages = {2639--2667},
title = {{High-dimensional influence measure}},
volume = {41},
year = {2013}
}
@article{Romano2008,
abstract = {Abstract This paper considers the problem of testing s null hypotheses simultaneously while controlling the false discovery rate (FDR). Benjamini and Hochberg (J. R. Stat. Soc. Ser. B 57(1):289300, 1995) provide a method for controlling the FDR based on p-values for each of the null hypotheses under the assumption that the p-values are independent. Subsequent research has since shown that this procedure is valid under weaker assumptions on the joint distribution of the p-values. Related procedures that are valid under no assumptions on the joint distribution of the p-values have also been developed. None of these procedures, however, incorporate information about the dependence structure of the test statistics. This paper develops methods for control of the FDR under weak assumptions that incorporate such information and, by doing so, are better able to detect false null hypotheses. We illustrate this property via a simulation study and two empirical applications. In particular, the bootstrap method is competitive with methods that require independence if independence holds, but it outperforms these methods under dependence.},
author = {Romano, Joseph P. and Shaikh, Azeem M. and Wolf, Michael},
doi = {10.1007/s11749-008-0126-6},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Romano, Shaikh, Wolf - 2008 - Control of the false discovery rate under dependence using the bootstrap and subsampling.pdf:pdf},
isbn = {1174900801346},
issn = {11330686},
journal = {Test},
keywords = {Bootstrap,False discovery rate,Multiple testing,Stepdown procedure,Subsampling},
number = {3},
pages = {417--442},
title = {{Control of the false discovery rate under dependence using the bootstrap and subsampling}},
volume = {17},
year = {2008}
}
@article{Goeman2011a,
abstract = {Motivated by the practice of exploratory research, we formulate an approach to multiple testing that reverses the conventional roles of the user and the multiple testing procedure. Traditionally, the user chooses the error criterion, and the procedure the resulting rejected set. Instead, we propose to let the user choose the rejected set freely, and to let the multiple testing procedure return a confidence statement on the number of false rejections incurred. In our approach, such confidence statements are simultaneous for all choices of the rejected set, so that post hoc selection of the rejected set does not compromise their validity. The proposed reversal of roles requires nothing more than a review of the familiar closed testing procedure, but with a focus on the non-consonant rejections that this procedure makes. We suggest several shortcuts to avoid the computational problems associated with closed testing.},
archivePrefix = {arXiv},
arxivId = {arXiv:1208.2841v2},
author = {Goeman, Jelle J. and Solari, Aldo},
doi = {10.1214/11-STS356},
eprint = {arXiv:1208.2841v2},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/Multiple Testing/Multiple Testing for Exploratory Research 1.pdf:pdf},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Closed testing, confidence set, false discovery pr,and phrases},
number = {4},
pages = {584--597},
title = {{Multiple Testing for Exploratory Research}},
url = {http://projecteuclid.org/euclid.ss/1330437937},
volume = {26},
year = {2011}
}
@article{Berman1964,
author = {Berman, Simeon M},
doi = {10.1214/aoms/1177703551},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Berman - 1964 - Limit theorems for the maximum term in stationary sequences(2).pdf:pdf},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
keywords = {Normal Maxima},
mendeley-tags = {Normal Maxima},
pages = {502--516},
title = {{Limit theorems for the maximum term in stationary sequences}},
year = {1964}
}
@article{Kovalchik2016,
abstract = {Sports forecasting models – beyond their interest to bettors – are important resources for sports analysts and coaches. Like the best athletes, the best forecasting models should be rigorously tested and judged by how well their performance holds up against top competitors. Although a number of models have been proposed for predicting match outcomes in professional tennis, their comparative performance is largely unknown. The present paper tests the predictive performance of 11 published forecasting models for predicting the outcomes of 2395 singles matches during the 2014 season of the Association of Tennis Professionals Tour. The evaluated models fall into three categories: regression-based, point-based, and paired comparison models. Bookmaker predictions were used as a performance benchmark. Using only 1 year of prior performance data, regression models based on player ranking and an Elo approach developed by FiveThirtyEight were the most accurate approaches. The FiveThirtyEight model predictions had an accuracy of 75{\%} for matches of the most highly-ranked players, which was competitive with the bookmakers. The inclusion of career-to-date improved the FiveThirtyEight model predictions for lower-ranked players (from 59{\%} to 64{\%}) but did not change the performance for higher-ranked players. All models were 10–20 percentage points less accurate at predicting match outcomes among lower-ranked players than matches with the top players in the sport. The gap in performance according to player ranking and the simplicity of the information used in Elo ratings highlight directions for further model development that could improve the practical utility and generalizability of forecasting in tennis.},
author = {Kovalchik, Stephanie Ann},
doi = {10.1515/jqas-2015-0059},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Tennis/Searching for the GOAT of tennis win prediction.pdf:pdf},
issn = {15590410},
journal = {Journal of Quantitative Analysis in Sports},
keywords = {Betting,Sport,Tennis,probit models,sports forecasting,validation},
mendeley-tags = {Betting,Sport,Tennis},
number = {3},
pages = {127--138},
title = {{Searching for the GOAT of tennis win prediction}},
volume = {12},
year = {2016}
}
@article{García-Soidán2014,
abstract = {Generation of replicates of the available data enables the researchers to solve different statistical problems, such as the estimation of standard errors, the inference of parameters or even the approximation of distribution functions. With this aim, Bootstrap approaches are suggested in the current work, specifically designed for their application to spatial data, as they take into account the dependence structure of the underlying random process. The key idea is to construct nonparametric distribution estimators, adapted to the spatial setting, which are distribution functions themselves, associated to discrete or continuous random variables. Then, the Bootstrap samples are obtained by drawing at random from the estimated distribution. Consistency of the suggested approaches will be proved by assuming stationarity from the random process or by relaxing the latter hypothesis to admit a deterministic trend. Numerical studies for simulated data and a real data set, obtained from environmental monitoring, are included to illustrate the application of the proposed Bootstrap methods.},
author = {Garc{\'{i}}a-Soid{\'{a}}n, Pilar and Menezes, Raquel and Rubi{\~{n}}os, {\'{O}}scar},
doi = {10.1007/s00477-013-0808-9},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Garc{\'{i}}a-Soid{\'{a}}n, Menezes, Rubi{\~{n}}os - 2014 - Bootstrap approaches for spatial data.pdf:pdf},
issn = {14363259},
journal = {Stochastic Environmental Research and Risk Assessment},
keywords = {Distribution estimation,Resampling method,Spatial data,Stationarity,Trend},
number = {5},
pages = {1207--1219},
title = {{Bootstrap approaches for spatial data}},
volume = {28},
year = {2014}
}
@article{Taylor2015,
	title={Statistical learning and selective inference},
	author={Taylor, Jonathan and Tibshirani, Robert J},
	journal={Proceedings of the National Academy of Sciences},
	volume={112},
	number={25},
	pages={7629--7634},
	year={2015},
	publisher={National Acad Sciences}
}
@article{Mao1995,
abstract = {A number of networks and learning algorithms which provide$\backslash$nnew or alternative tools for feature extraction and data$\backslash$nprojection is proposed. The networks include a network$\backslash$n(SAMANN) for Sammon's nonlinear projection, a linear$\backslash$ndiscriminant analysis (LDA) network, a nonlinear$\backslash$ndiscriminant analysis (NDA) network, and a network for$\backslash$nnonlinear projection (NP-SOM) based on Kohonen's$\backslash$nself-organizing map. Five representative neural networks$\backslash$nfor feature extraction and data projection based on a$\backslash$nvisual judgement of two-dimensional projection maps and$\backslash$nquantitative criteria on data sets with various properties$\backslash$nare evaluated.},
author = {Mao, Jianchang and Jain, Anil K.},
doi = {10.1109/72.363467},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mao, Jain - 1995 - Artificial neural networks for feature extraction and multivariate data projection(2).pdf:pdf},
isbn = {1045-9227},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pages = {296--317},
pmid = {18263314},
title = {{Artificial neural networks for feature extraction and multivariate data projection}},
volume = {6},
year = {1995}
}
@article{Kokaram1995,
abstract = {This paper presents a number of model based interpolation schemes tailored to the problem of interpolating missing regions in image sequences. These missing regions may be of arbitrary size and of random, but known, location. This problem occurs regularly with archived film material. The film is abraded or obscured in patches, giving rise to bright and dark flashes, known as "dirt and sparkle" in the motion picture industry. Both 3-D autoregressive models and 3-D Markov random fields are considered in the formulation of the different reconstruction processes. The models act along motion directions estimated using a multiresolution block matching scheme. It is possible to address this sort of impulsive noise suppression problem with median filters, and comparisons with earlier work using multilevel median filters are performed. These comparisons demonstrate the higher reconstruction fidelity of the new interpolators.},
author = {Kokaram, Anil C. and Morris, Robin D. and Fitzgerald, William J. and Rayner, Peter J W},
doi = {10.1109/83.469932},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kokaram et al. - 1995 - Interpolation of Missing Data in Image Sequences.pdf:pdf},
issn = {19410042},
journal = {IEEE Transactions on Image Processing},
number = {11},
pages = {1509--1519},
pmid = {18291983},
title = {{Interpolation of Missing Data in Image Sequences}},
volume = {4},
year = {1995}
}
@article{Hansen1996,
author = {Hansen, B Y Bruce E},
file = {:data/fireback/davenpor/davenpor/Referreeing/Testing One Hypothesis Multiple Times- The Multidimensional Case/Proof under null - Inference When a Nuisance Parameter Is Not Identified Under the Null Hypothesis.pdf:pdf},
journal = {Econometrica},
keywords = {asymptotic theory,identification,nonlinear models,p-values,thresholds},
number = {2},
pages = {413--430},
title = {{Inference When a Nuisance Parameter Is Not Identified Under the Null Hypothesis}},
volume = {64},
year = {1996}
}
@article{Robbins1956,
abstract = {Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics Conference: Third Berkeley Symposium on Mathematical Statistics and Probability Date: December 1954 and July-August 1955 Location: Statistical Laboratory of the University of California, Berkeley Editor: Jerzy Neyman Berkeley, Calif.: University of California Press, 1956 208 pp.},
archivePrefix = {arXiv},
arxivId = {0812.4648},
author = {Robbins, Herbert},
doi = {10.1214/aoms/1177703729},
eprint = {0812.4648},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Empirical Bayes/An Empirical Bayes Approach to Statistics.pdf:pdf},
isbn = {978-0-387-94037-3},
issn = {0003-4851},
pmid = {540267},
title = {{The Empirical Bayes Approach to Statistics}},
year = {1956}
}
@article{Is,
author = {Is, What and Process, Stochastic},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Is, Process - Unknown - Basic Definitions Indexed Collections and Random Functions(2).pdf:pdf},
pages = {3--14},
title = {{Basic Definitions : Indexed Collections and Random Functions}},
volume = {2}
}
@article{Evans2016,
author = {Evans, Robin},
file = {:data/fireback/davenpor/davenpor/General Notes/Statistics/Graphical Models.pdf:pdf},
journal = {Oxford Lecture Notes},
number = {17},
pages = {1--67},
title = {{Graphical Models}},
year = {2017}
}
@article{Benjamini2014,
abstract = {In many complex multiple-testing problems the hypotheses are divided into families. Given the data, families with evidence for true discoveries are selected, and hypotheses within them are tested. Neither controlling the error rate in each family separately nor controlling the error rate over all hypotheses together can assure some level of confidence about the filtration of errors within the selected families. We formulate this concern about selective inference in its generality, for a very wide class of error rates and for any selection criterion, and present an adjustment of the testing level inside the selected families that retains control of the expected average error over the selected families. {\textcopyright} 2013 Royal Statistical Society.},
author = {Benjamini, Yoav and Bogomolov, Marina},
doi = {10.1111/rssb.12028},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Multiple Testing/Selection Bias/Selective inference on multiple families of hypotheses.pdf:pdf},
isbn = {1467-9868},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {False discovery rate,Familywise error rate,Hierarchical testing,Multiple testing,Selective inference},
number = {1},
pages = {297--318},
title = {{Selective inference on multiple families of hypotheses}},
volume = {76},
year = {2014}
}
@article{Libgober,
author = {Libgober, J},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Libgober - Unknown - The Euler Characteristic, Poincare-Hopf Theorem and Applications.pdf:pdf},
pages = {1--19},
title = {{The Euler Characteristic, Poincare-Hopf Theorem and Applications}}
}
@article{Marcus,
author = {Marcus, M and Shepp, L},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/EVT/euclid.bsmsp.1200514231.pdf:pdf},
title = {{Sample Behavior of Gaussian Processes}}
}
@article{Chen2017d,
author = {Chen, Kun and Ma, Yanyuan},
doi = {10.1111/sjos.12238},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Ma - 2017 - Analysis of Double Single Index Models(2).pdf:pdf},
issn = {03036898},
journal = {Scandinavian Journal of Statistics},
keywords = {CCA,SDR,Semiparametrics,canonical correlation analysis,reduced rank regression,semiparametric efficiency,single index models,sufficient dimension reduction},
mendeley-tags = {CCA,SDR,Semiparametrics},
number = {1},
pages = {1--20},
title = {{Analysis of Double Single Index Models}},
volume = {44},
year = {2017}
}
@article{Neyman1933,
abstract = {The likelihood principle of Bayesian statistics implies that information about the stopping rule used to collect evidence does not enter into the statistical analysis. This consequence confers an apparent advantage on Bayesian statistics over frequentist ...},
author = {Neyman, J. and Pearson, E. S.},
doi = {10.1098/rsta.1933.0009},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Hypothesis Testing/On the Problem of the Most Efficient Tests of Statistical Hypotheses.pdf:pdf},
isbn = {02643952},
issn = {1364-503X},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
number = {694-706},
pages = {289--337},
title = {{On the Problem of the Most Efficient Tests of Statistical Hypotheses}},
volume = {231},
year = {1933}
}
@article{Robertson2016,
abstract = {The problem of selection bias has long been recognized in the analysis of two-stage trials, where promising candidates are selected in stage 1 for confirmatory analysis in stage 2. To efficiently correct for bias, uniformly minimum variance conditionally unbiased estimators (UMVCUEs) have been proposed for a wide variety of trial settings, but where the population parameter estimates are assumed to be independent. We relax this assumption and derive the UMVCUE in the multivariate normal setting with an arbitrary known covariance structure. One area of application is the estimation of odds ratios (ORs) when combining a genome-wide scan with a replication study. Our framework explicitly accounts for correlated single nucleotide polymorphisms, as might occur due to linkage disequilibrium. We illustrate our approach on the measurement of the association between 11 genetic variants and the risk of Crohn's disease, as reported in Parkes and others (2007. Sequence variants in the autophagy gene IRGM and multiple other replicating loci contribute to Crohn's disease susceptibility. Nat. Gen. 39: (7), 830-832.), and show that the estimated ORs can vary substantially if both selection and correlation are taken into account.},
author = {Robertson, David S. and Prevost, A. Toby and Bowden, Jack},
doi = {10.1093/biostatistics/kxw012},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Robertson, Prevost, Bowden - 2016 - Accounting for selection and correlation in the analysis of two-stage genome-wide association stu(2).pdf:pdf},
issn = {14684357},
journal = {Biostatistics},
keywords = {Correlated outcomes,Genome-wide scan,Selection bias,Two-stage sample,Uniformly minimum variance conditionally unbiased},
number = {4},
pages = {634--649},
pmid = {26993061},
title = {{Accounting for selection and correlation in the analysis of two-stage genome-wide association studies}},
volume = {17},
year = {2016}
}
@article{Fisher1990,
author = {Fisher, Nicholas I and Hall, Peter},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Hypothesis Testing/On bootstrap hypothesis testing.pdf:pdf},
keywords = {analysis of variance,behrens-fisher problem,bootstrap,hy-,level error,monte car10 test,pivotal,pothesis test,resample},
number = {April 1989},
pages = {177--190},
title = {{On Bootstrap Hypothesis Testing}},
volume = {32},
year = {1990}
}
@article{Mason2001,
abstract = {Let X1,..., Xn, n ≥ 1, be independent, identically distributed random variables and consider the Student t-statistic Tn based upon these random variables. Gine, Gotze and Mason (1997) proved that Tn converges in distribution to a standard normal random variable if and only if X is in the domain of attraction of a normal random variable and EX = 0. We shall show that roughly the same holds true for the bootstrapped Student t-statistic T*n. In the process we shall disclose all the possible subsequential limiting laws of T*n. The proofs introduce a number of amusing tricks that may be of independent interest.},
author = {Mason, David M. and Shao, Qi Man},
doi = {10.1214/aop/1015345757},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/T-stat/BOOTSTRAPPING THE STUDENT t-STATISTIC.pdf:pdf},
isbn = {00911798},
issn = {00911798},
journal = {Annals of Probability},
keywords = {Bootstrap,Order statistics,Student t-statistic},
number = {4},
pages = {1435--1450},
title = {{Bootstrapping the student t-statistic}},
volume = {29},
year = {2001}
}
@article{Harville1977,
abstract = {Recent developments promise to increase greatly the popularity of maximum likelihood (ML) as a technique for estimating variance components. Patterson and Thompson (1971) proposed a restricted maximum likelihood (REML) approach which takes into account the loss in degrees of freedom resulting from estimating fixed effects. Miller (1973) developed a satisfactory asymptotic theory for ML estimators of variance components. There are many iterative algorithms that can be considered for computing the ML or REML estimates. The computations on each iteration of these algorithms are those associated with computing estimates of fixed and random effects for given values of the variance components.},
author = {Harville, David A.},
doi = {10.1080/01621459.1977.10480998},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/MLEs/Maximum Likelihood Approaches to Variance Component Estimation and to Related Problems.pdf:pdf},
isbn = {01621459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {hood,maxi-,maximum likeli-,mixed linear models,mum likelihood computations,restricted maximum likelihood,tivity constraints on the,variance component estimation,variance components or other},
number = {358},
pages = {320--338},
pmid = {12949110},
title = {{Maximum Likelihood Approaches to Variance Component Estimation and to Related Problems}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1977.10480998},
volume = {72},
year = {1977}
}
@article{Laird1982,
author = {Laird, Nan M and Ware, James H},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Random Effects/Random-effects models for longitudinal data.pdf:pdf},
issn = {0006-341X},
journal = {Biometrics},
number = {4},
pages = {963--974},
title = {{Random-Effects Models for Longitudinal Data}},
volume = {38},
year = {2007}
}
@article{Zollner2007,
abstract = {Genomewide association studies are now a widely used approach in the search for loci that affect complex traits. After detection of significant association, estimates of penetrance and allele-frequency parameters for the associated variant indicate the importance of that variant and facilitate the planning of replication studies. However, when these estimates are based on the original data used to detect the variant, the results are affected by an ascertainment bias known as the "winner's curse." The actual genetic effect is typically smaller than its estimate. This overestimation of the genetic effect may cause replication studies to fail because the necessary sample size is underestimated. Here, we present an approach that corrects for the ascertainment bias and generates an estimate of the frequency of a variant and its penetrance parameters. The method produces a point estimate and confidence region for the parameter estimates. We study the performance of this method using simulated data sets and show that it is possible to greatly reduce the bias in the parameter estimates, even when the original association study had low power. The uncertainty of the estimate decreases with increasing sample size, independent of the power of the original test for association. Finally, we show that application of the method to case-control data can improve the design of replication studies considerably.},
author = {Z{\"{o}}llner, Sebastian and Pritchard, Jonathan K.},
doi = {10.1086/512821},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Z{\"{o}}llner, Pritchard - 2007 - Overcoming the Winner's Curse Estimating Penetrance Parameters from Case-Control Data(2).pdf:pdf},
isbn = {0002-9297 (Print)$\backslash$r0002-9297 (Linking)},
issn = {00029297},
journal = {The American Journal of Human Genetics},
keywords = {Winner's Curse},
mendeley-tags = {Winner's Curse},
number = {4},
pages = {605--615},
pmid = {17357068},
title = {{Overcoming the Winner's Curse: Estimating Penetrance Parameters from Case-Control Data}},
volume = {80},
year = {2007}
}
@article{Ghosal2012,
author = {Ghosal, Subhashis and van der Vaart, Aad},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghosal, van der Vaart - 2012 - Nonparametric Bayesian Statistics Part I some classical results(2).pdf:pdf},
journal = {Fundamentals of Nonparametric Bayesian Inference},
title = {{Nonparametric Bayesian Statistics Part I: some classical results}},
year = {2012}
}
@article{Gelman2000,
abstract = {Gelman, A., and F. Tuerlinckx. 2000. Type S error rates for classical and Bayesian single and multiple comparison procedures. Computational Statistics 15: 373-390},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Gelman, Andrew and Tuerlinckx, Francis},
doi = {10.1007/s001800000040},
eprint = {arXiv:1011.1669v3},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelman, Tuerlinckx - 2000 - Type S error rates for classical and Bayesian single and multiple comparison procedures.pdf:pdf},
isbn = {0943-4062},
issn = {09434062},
journal = {Computational Statistics},
number = {3},
pages = {373--390},
pmid = {25246403},
title = {{Type S error rates for classical and Bayesian single and multiple comparison procedures}},
volume = {15},
year = {2000}
}
@article{Eisenhauer,
abstract = {This article describes situations in which regression through the origin is appropriate, derives the normal equation for such a regression and explains the controversy regarding its evaluative statistics. Differences between three popular software packages that allow regression through the origin are illustrated using examples from previous issues of Teaching Statistics.},
author = {Eisenhauer, Joseph G.},
doi = {10.1111/1467-9639.00136},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Regularized Regression/Regression through the Origin.pdf:pdf},
isbn = {- 1467-9639},
issn = {0141-982X},
journal = {Teaching Statistics},
number = {3},
pages = {76--80},
title = {{Regression through the Origin}},
volume = {25},
year = {2003}
}
@article{Cheng2016,
abstract = {Let {\$}X = {\{}X(t), t\backslashin \backslashR{\^{}}{\{}N{\}}{\}}{\$} be a centered Gaussian random field with stationary increments and let {\$}T \backslashsubset \backslashR{\^{}}N{\$} be a compact rectangle. Under {\$}X(\backslashcdot) \backslashin C{\^{}}2(\backslashR{\^{}}N){\$} and certain additional regularity conditions, the mean Euler characteristic of the excursion set {\$}A{\_}u = {\{}t\backslashin T: X(t)\backslashgeq u{\}}{\$}, denoted by {\$}\backslashE{\{}\backslashvarphi(A{\_}u){\}}{\$}, is derived. By applying the Rice method, it is shown that, as {\$}u \backslashto \backslashinfty{\$}, the excursion probability {\$}\backslashP{\{}\backslashsup{\_}{\{}t\backslashin T{\}} X(t) \backslashgeq u{\}}{\$} can be approximated by {\$}\backslashE{\{}\backslashvarphi(A{\_}u){\}}{\$} such that the error is exponentially smaller than {\$}\backslashE{\{}\backslashvarphi(A{\_}u){\}}{\$}. This verifies the expected Euler characteristic heuristic for a large class of Gaussian random fields with stationary increments.},
archivePrefix = {arXiv},
arxivId = {1211.6693},
author = {Cheng, Dan and Xiao, Yimin},
doi = {10.1214/15-AAP1101},
eprint = {1211.6693},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng, Xiao - 2016 - The mean Euler characteristic and excursion probability of Gaussian random fields with stationary increments.pdf:pdf},
issn = {10505164},
journal = {Annals of Applied Probability},
keywords = {Euler characteristic,Excursion probability,Excursion set,Gaussian random fields with stationary increments,Super-exponentially small},
number = {2},
pages = {722--759},
title = {{The mean Euler characteristic and excursion probability of Gaussian random fields with stationary increments}},
volume = {26},
year = {2016}
}
@misc{Mitchell1988,
abstract = {This article is concerned with the selection of subsets of predictor variables in a linear regression model for the prediction of a dependent variable. It is based on a Bayesian approach, intended to be as objective as possible. A probability distribution is first assigned to the dependent variable through the specification of a family of prior distributions for the unknown parameters in the regression model. The method is not fully Bayesian, however, because the ultimate choice of prior distribution from this family is affected by the data. It is assumed that the predictors represent distinct observables; the corresponding regression coefficients are assigned independent prior distributions. For each regression coefficient subject to deletion from the model, the prior distribution is a mixture of a point mass at 0 and a diffuse uniform distribution elsewhere, that is, a "spike and slab" distribution. The random error component is assigned a normal distribution with mean 0 and standard deviation $\sigma$, where ln($\sigma$) has a locally uniform noninformative prior distribution. The appropriate posterior probabilities are derived for each submodel. If the regression coefficients have identical priors, the posterior distribution depends only on the data and the parameter $\gamma$, which is the height of the spike divided by the height of the slab for the common prior distribution. This parameter is not assigned a probability distribution; instead, it is considered a parameter that indexes the members of a class of Bayesian methods. Graphical methods are proposed as informal guides for choosing $\gamma$, assessing the complexity of the response function and the strength of the individual predictor variables, and assessing the degree of uncertainty about the best submodel. The following plots against $\gamma$ are suggested: (a) posterior probability that a particular regression coefficient is 0; (b) posterior expected number of terms in the model; (c) posterior entropy of the submodel distribution; (d) posterior predictive error; and (e) posterior probability of goodness of fit. Plots (d) and (e) are suggested as ways to choose $\gamma$. The predictive error is determined using a Bayesian cross-validation approach that generates a predictive density for each observation, given all of the data except that observation, that is, a type of "leave one out" approach. The goodness-of-fit measure is the sum of the posterior probabilities of all submodels that pass a standard F test for goodness of fit relative to the full model, at a specified level of significance. The dependence of the results on the scaling of the variables is discussed, and some ways to choose the scaling constants are suggested. Examples based on a large data set arising from an energy-conservation study are given to demonstrate the application of the methods.},
author = {Mitchell, Tj J. and Beauchamp, Jj J.},
booktitle = {Journal of the American Statistical Association},
doi = {10.2307/2290129},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bayesian/Variable Selection/Bayesian Variable Selection in Linear Regression.pdf:pdf},
isbn = {Journal of the American Statistical Association, Vol. 83, No. 404, December 1988: pp. 1023–1032},
issn = {01621459},
keywords = {Bansal2018},
number = {404},
pages = {1023--1032},
title = {{Bayesian Variable Selection in Linear Regression}},
url = {http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1988.10478694{\%}5Cnhttp://www.jstor.org/stable/2290129},
volume = {83},
year = {1988}
}
@article{Hughes2014,
author = {Hughes, John},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Distance Functions/Estimating the Cumulative Distribution Function.pdf:pdf},
pages = {1--7},
title = {{Estimating the Cumulative Distribution Function and Statistical Functionals}},
year = {2014}
}
@article{Smith1994,
author = {Smith, Richard and Weissman, Ishay},
doi = {10.1079/IVPt200454()IN},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/EVT/Extremal Index - Stationary Dependent Extreme Value Theory/Estimating the Extremal Index.pdf:pdf},
keywords = {analysis of variance,choice of variables,crossvalidation,doublecross,modelmix,multiple regression,prediction,prescription,univariate estimation},
number = {3},
pages = {515--528},
title = {{Estimating the Extremal Index}},
volume = {56},
year = {1994}
}
@article{Chung2007,
author = {Chung, Moo K},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chung - 2007 - A brief introduction to Conditional Random Fields.pdf:pdf},
number = {x},
pages = {1--3},
title = {{A brief introduction to Conditional Random Fields}},
volume = {1},
year = {2007}
}
@article{Hogben,
	title={The moments of the non-central t-distribution},
	author={Hogben, David and Pinkham, RS and Wilk, MB},
	journal={Biometrika},
	volume={48},
	number={3/4},
	pages={465--468},
	year={1961},
	publisher={JSTOR}
}
@misc{Monestiez,
author = {Monestiez, P and Sampson, P and Guttorp, Peter},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/Random{\_}Field{\_}Theory/Other/Non-Stationarity/Modelling of heterogeneous spatial correlation struture by spatial deformation.pdf:pdf},
title = {{Modelling of Heterogeneous Spatial Correlation Structure by Spatial Deformation}}
}
@misc{Putter1968,
author = {Putter, Joseph and Rubinstein, David},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Putter, Rubinstein - 1968 - On estimating the mean of a selected population.pdf:pdf},
title = {{On estimating the mean of a selected population}},
year = {1968}
}
@article{Rosenblatt2017a,
abstract = {The most prevalent approach to activation localization in neuroimaging is to identify brain regions as contiguous supra-threshold clusters, check their significance using random field theory, and correct for the multiple clusters being tested. Besides recent criticism on the validity of the random field assumption, a spatial specificity paradox remains: the larger the detected cluster, the less we know about the location of activation within that cluster. This is because cluster inference implies "there exists at least one voxel with an evoked response in the cluster", and not that "all the voxels in the cluster have an evoked response". Inference on voxels within selected clusters is considered bad practice, due to the voxel-wise false positive rate inflation associated with this circular inference. Here, we propose a remedy to the spatial specificity paradox. By applying recent results from the multiple testing statistical literature, we are able to quantify the proportion of truly active voxels within selected clusters, an approach we call All-Resolutions Inference (ARI). If this proportion is high, the paradox vanishes. If it is low, we can further "drill down" from the cluster level to sub-regions, and even to individual voxels, in order to pinpoint the origin of the activation. In fact, ARI allows inference on the proportion of activation in all voxel sets, no matter how large or small, however these have been selected, all from the same data. We use two fMRI datasets to demonstrate the non-triviality of the spatial specificity paradox, and its resolution using ARI. One of these datasets is large enough for us to split it and validate the ARI estimates. The conservatism of ARI inference permits circularity without losing error guarantees, while still returning informative estimates.},
author = {Rosenblatt, Jonathan D. and Finos, Livio and Weeda, Wouter D. and Solari, Aldo and Goeman, Jelle J.},
doi = {10.1101/226126},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/fMRI/ARI.pdf:pdf},
issn = {10538119},
journal = {bioRxiv},
pages = {226126},
publisher = {Elsevier Inc.},
title = {{All-Resolutions Inference for Brain Imaging}},
url = {https://www.biorxiv.org/content/early/2017/11/28/226126},
year = {2017}
}
@article{Lee2014,
abstract = {We develop a framework for post model selection inference, via marginal screening, in linear regression. At the core of this framework is a result that characterizes the exact distribution of linear functions of the response {\$}y{\$}, conditional on the model being selected (``condition on selection" framework). This allows us to construct valid confidence intervals and hypothesis tests for regression coefficients that account for the selection procedure. In contrast to recent work in high-dimensional statistics, our results are exact (non-asymptotic) and require no eigenvalue-like assumptions on the design matrix {\$}X{\$}. Furthermore, the computational cost of marginal regression, constructing confidence intervals and hypothesis testing is negligible compared to the cost of linear regression, thus making our methods particularly suitable for extremely large datasets. Although we focus on marginal screening to illustrate the applicability of the condition on selection framework, this framework is much more broadly applicable. We show how to apply the proposed framework to several other selection procedures including orthogonal matching pursuit, non-negative least squares, and marginal screening+Lasso.},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.5596v2},
author = {Lee, Jason D and Taylor, Jonathan},
eprint = {arXiv:1402.5596v2},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Taylor - 2014 - Exact Post Model Selection Inference for Marginal Screening(2).pdf:pdf},
journal = {In Advances in Neural Information Processing Systems},
number = {2},
pages = {1--9},
title = {{Exact Post Model Selection Inference for Marginal Screening}},
volume = {1},
year = {2014}
}
@article{Stallard2008,
abstract = {This paper considers the problem of estimation when one of a number of populations, assumed normal with known common variance, is selected on the basis of it having the largest observed mean. Conditional on selection of the population, the observed mean is a biased estimate of the true mean. This problem arises in the analysis of clinical trials in which selection is made between a number of experimental treatments that are compared with each other either with or without an additional control treatment. Attempts to obtain approximately unbiased estimates in this setting have been proposed by Shen [2001. An improved method of evaluating drug effect in a multiple dose clinical trial. Statist. Medicine 20, 1913-1929] and Stallard and Todd [2005. Point estimates and confidence regions for sequential trials involving selection. J. Statist. Plann. Inference 135, 402-419]. This paper explores the problem in the simple setting in which two experimental treatments are compared in a single analysis. It is shown that in this case the estimate of Stallard and Todd is the maximum-likelihood estimate (m.l.e.), and this is compared with the estimate proposed by Shen. In particular, it is shown that the m.l.e. has infinite expectation whatever the true value of the mean being estimated. We show that there is no conditionally unbiased estimator, and propose a new family of approximately conditionally unbiased estimators, comparing these with the estimators suggested by Shen. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
author = {Stallard, Nigel and Todd, Susan and Whitehead, John},
doi = {10.1016/j.jspi.2007.05.045},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stallard, Todd, Whitehead - 2008 - Estimation following selection of the largest of two normal means.pdf:pdf},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Clinical trial analysis,Select and test designs,Treatment selection},
number = {6},
pages = {1629--1638},
title = {{Estimation following selection of the largest of two normal means}},
volume = {138},
year = {2008}
}
@article{Dasgupta2002,
abstract = {co-training},
author = {Dasgupta, Sanjoy and Littman, Ml and McAllester, D},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Multiviewlearning/PAC Generalization Bounds for Co-training.pdf:pdf},
isbn = {0262042088},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 14},
pages = {375--382},
title = {{PAC generalization bounds for co-training}},
year = {2002}
}
@article{Mallows1972,
abstract = {The use of transformations to stabilize the variance of binomial or Poisson data is familiar(Anscombe 1, Bartlett 2, 3, Curtiss 4, Eisenhart 5). The comparison of transformed binomial or Poisson data with percentage points of the normal distribution to make approximate significance tests or to set approximate confidence intervals is less familiar. Mosteller and Tukey 6 have recently made a graphical application of a transformation related to the square-root transformation for such purposes, where the use of "binomial probability paper" avoids all computation. We report here on an empirical study of a number of approximations, some intended for significance and confidence work and others for variance stabilization. For significance testing and the setting of confidence limits, we should like to use the normal deviate K exceeded with the same probability as the number of successes x from n in a binomial distribution with expectation np, which is defined by frac12pi int{\^{}}K-infty e{\^{}}-frac12t{\^{}}2 dt = operatornameProb x leq k mid operatornamebinomial, n, p. The most useful approximations to K that we can propose here are N (very simple), N{\^{}}+ (accurate near the usual percentage points), and N{\^{}}astast (quite accurate generally), where N = 2 (sqrt(k + 1)q - sqrt(n - k)p). (This is the approximation used with binomial probability paper.) N{\^{}}+ = N + fracN + 2p - 112sqrtE,quad E = textlesser of np textand nq, N{\^{}}ast = N + frac(N - 2)(N + 2)12 big(frac1sqrtnp + 1 - frac1sqrtnq + 1big), N{\^{}}astast = N{\^{}}ast + fracN{\^{}}ast + 2p - 112 sqrtEcdotquad E = textlesser of np textand nq. For variance stabilization, the averaged angular transformation sin{\^{}}-1sqrtfracxn + 1 + sin{\^{}}-1 sqrtfracx + 1n+1 has variance within pm 6{\%} of frac1n + frac12 text(angles in radians), frac821n + frac12 text(angles in degrees), for almost all cases where np geq 1. In the Poisson case, this simplifies to using sqrtx + sqrtx + 1 as having variance 1.},
author = {Mallows, C. L.},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Asymptotics/A Note on Asymptotic Joint Normality.pdf:pdf},
issn = {00034851},
journal = {The Annals of Mathematical Statistics},
number = {2},
pages = {508--515},
title = {{A note on Asymptotic Joint Normality}},
url = {http://www.jstor.org/stable/2238700},
volume = {43},
year = {1972}
}
@article{Brett2003,
abstract = {This chapter is an introduction to the multiple comparison problem in func- tional imaging, and the way it can be solved using Random field theory (RFT).},
author = {Brett, Matthew and Penny, Will and Kiebel, Stefan},
doi = {10.1049/sqj.1969.0076},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brett, Penny, Kiebel - 2003 - 14. Introduction to Random Field Theory(2).pdf:pdf},
isbn = {0122648412},
issn = {00392871},
journal = {Human Brain Function},
pages = {1--23},
title = {{14. Introduction to Random Field Theory}},
year = {2003}
}
@article{Barbour2006,
author = {Barbour, A. D. and Xia, Aihua},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/Random{\_}Field{\_}Theory/Other/Lattice/NORMAL APPROXIMATION FOR RANDOM SUMS.pdf:pdf},
number = {20},
pages = {693--728},
title = {{Normal approximation for random sums}},
volume = {728},
year = {2018}
}
@book{Muirhead1982,
abstract = {A classical mathematical treatment of the techniques, distributions, and inferences based on the multivariate normal distribution. Introduces noncentral distribution theory, decision theoretic estimation of the parameters of a multivariate normal distribution, and the uses of spherical and elliptical distributions in multivariate analysis. Discusses recent advances in multivariate analysis, including decision theory and robustness. Also includes tables of percentage points of many of the standard likelihood statistics used in multivariate statistical procedures},
author = {Muirhead, Robb J},
doi = {10.1002/9780470316559},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Muirhead - 1982 - Aspects of multivariate statistical theory(2).pdf:pdf},
isbn = {0-471-09442-0},
pages = {xix+673},
title = {{Aspects of multivariate statistical theory}},
year = {1982}
}
@article{Chernoff1954,
abstract = {The use of transformations to stabilize the variance of binomial or Poisson data is familiar(Anscombe 1, Bartlett 2, 3, Curtiss 4, Eisenhart 5). The comparison of transformed binomial or Poisson data with percentage points of the normal distribution to make approximate significance tests or to set approximate confidence intervals is less familiar. Mosteller and Tukey 6 have recently made a graphical application of a transformation related to the square-root transformation for such purposes, where the use of "binomial probability paper" avoids all computation. We report here on an empirical study of a number of approximations, some intended for significance and confidence work and others for variance stabilization. For significance testing and the setting of confidence limits, we should like to use the normal deviate K exceeded with the same probability as the number of successes x from n in a binomial distribution with expectation np, which is defined by frac12pi int{\^{}}K-infty e{\^{}}-frac12t{\^{}}2 dt = operatornameProb x leq k mid operatornamebinomial, n, p. The most useful approximations to K that we can propose here are N (very simple), N{\^{}}+ (accurate near the usual percentage points), and N{\^{}}astast (quite accurate generally), where N = 2 (sqrt(k + 1)q - sqrt(n - k)p). (This is the approximation used with binomial probability paper.) N{\^{}}+ = N + fracN + 2p - 112sqrtE,quad E = textlesser of np textand nq, N{\^{}}ast = N + frac(N - 2)(N + 2)12 big(frac1sqrtnp + 1 - frac1sqrtnq + 1big), N{\^{}}astast = N{\^{}}ast + fracN{\^{}}ast + 2p - 112 sqrtEcdotquad E = textlesser of np textand nq. For variance stabilization, the averaged angular transformation sin{\^{}}-1sqrtfracxn + 1 + sin{\^{}}-1 sqrtfracx + 1n+1 has variance within pm 6{\%} of frac1n + frac12 text(angles in radians), frac821n + frac12 text(angles in degrees), for almost all cases where np geq 1. In the Poisson case, this simplifies to using sqrtx + sqrtx + 1 as having variance 1.},
author = {Chernoff, Herman},
doi = {10.1214/aoms/1177728725},
file = {:data/fireback/davenpor/davenpor/Referreeing/Testing One Hypothesis Multiple Times- The Multidimensional Case/On the Distribution of the Likelihood Ratio.pdf:pdf},
isbn = {00034851},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
number = {3},
pages = {573--578},
title = {{On the Distribution of the Likelihood Ratio}},
volume = {25},
year = {1954}
}
@article{Kim1988,
author = {Kim, Song-ho},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Order statistics/Non-Homogeneous/Stochastic comparisons of order statistics.pdf:pdf},
title = {{Stochastic comparisons of order statistics}},
year = {1988}
}
@article{Klami2013,
abstract = {Canonical correlation analysis (CCA) is a classical method for seeking correlations between two multivariate data sets. During the last ten years, it has received more and more attention in the machine learning community in the form of novel computational formulations and a plethora of applications. We review recent developments in Bayesian models and inference methods for CCA which are attractive for their potential in hierarchical extensions and for coping with the combina-tion of large dimensionalities and small sample sizes. The existing methods have not been partic-ularly successful in fulfilling the promise yet; we introduce a novel efficient solution that imposes group-wise sparsity to estimate the posterior of an extended model which not only extracts the statistical dependencies (correlations) between data sets but also decomposes the data into shared and data set-specific components. In statistics literature the model is known as inter-battery factor analysis (IBFA), for which we now provide a Bayesian treatment.},
author = {Klami, Arto and Fi, Arto Klami@hiit and Fi, Seppo J Virtanen@aalto and Kaski, Samuel and Fi, Samuel Kaski@aalto},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Klami et al. - 2013 - Bayesian Canonical Correlation Analysis.pdf:pdf},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Bayesian modeling,canonical correlation analysis,group-wise sparsity,inter-battery factor analysis,variational Bayesian approximation},
pages = {965--1003},
title = {{Bayesian Canonical Correlation Analysis}},
volume = {14},
year = {2013}
}
@article{James1961,
abstract = {It has long been customary to measure the adequacy of an estimator by the smallness of its mean squared error. The least squares estimators were studied by Gauss and by other authors later in the nineteenth century. A proof that the best unbiased estimator of a linear function of the means of a set of observed random variables is the least squares estimator was given by Markov [12], a modified version of whose proof is given by David and Neyman [4]. A slightly more general theorem is given by Aitken [1]. Fisher [5] indicated that for large samples the maximum likelihood estimator approximately minimizes the mean squared error when compared with other reasonable estimators. This paper will be concerned with optimum properties or failure of optimum properties of the natural estimator in certain special problems with the risk usually measured by the mean squared error or, in the case of several parameters, by a quadratic function of the estimators. We shall first mention some recent papers on this subject and then give some results, mostly unpublished, in greater detail.},
author = {James, W and Stein, Charles},
journal = {Proceedings of the fourth Berkeley symposium on mathematical statistics and probability},
pages = {361-- 379},
title = {{Estimation with quadratic loss}},
year = {1961}
}
@article{DiCiccio1996,
abstract = {This article surveys bootstrap methods for producing good approximate confidence intervals. The goal is to improve by an order of magnitude upon the accuracy of the standard intervals $\theta$ˆ±z($\alpha$)$\sigma$ˆ, in a way that allows routine application even to very complicated problems. Both theory and examples are used to show how this is done. The first seven sections provide a heuristic overview of four bootstrap confidence interval procedures: BCa, bootstrap-t , ABC and calibration. Sections 8 and 9 describe the theory behind these methods, and their close connection with the likelihood-based confidence interval theory developed by Barndorff-Nielsen, Cox and Reid and others.},
author = {DiCiccio, Thomas J. and Efron, Bradley},
doi = {10.1214/ss/1032280214},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Bootstrap Confidence Intervals.pdf:pdf},
isbn = {0883-4237},
issn = {01677152},
journal = {Statistical Science},
keywords = {BCa and ABC methods,bootstrap-t,calibration,second-order accuracy},
number = {3},
pages = {189--228},
pmid = {21432879},
title = {{Bootstrap confidence intervals}},
volume = {11},
year = {1996}
}
@article{Robert2013,
abstract = {This paper discusses the dual interpretation of the Jeffreys--Lindley's paradox associated with Bayesian posterior probabilities and Bayes factors, both as a differentiation between frequentist and Bayesian statistics and as a pointer to the difficulty of using improper priors while testing. We stress the considerable impact of this paradox on the foundations of both classical and Bayesian statistics. While assessing existing resolutions of the paradox, we focus on a critical viewpoint of the paradox discussed by Spanos (2013) in Philosophy of Science.},
archivePrefix = {arXiv},
arxivId = {1303.5973},
author = {Robert, Christian},
eprint = {1303.5973},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bayesian/Lindley Paradox/On the Jeffreys–Lindley's.pdf:pdf},
keywords = {Bayesian inference,Testing statistical hypotheses,Type I error,and phrases,bayesian inference,p-value,ses,significance level,testing statistical hypothe-,type i error},
pages = {1--15},
title = {{On the Jeffreys-Lindley's paradox}},
year = {2013}
}
@book{Dudoit2008,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Dudoit, Sandrine and van der Laan, Mark J.},
doi = {10.1007/978-0-387-98135-2},
eprint = {arXiv:1011.1669v3},
file = {:homes/davenpor/global/TomsMiniProject/This Week/Sandrine Dudoit, Mark J. van der Laan Multiple testing procedures with applications to genomics.pdf:pdf},
isbn = {9781461382447},
issn = {01727397},
pmid = {15772297},
title = {{Multiple Testing Procedures with Applications to Genomics}},
year = {2008}
}
@article{Winkler2014,
abstract = {Permutation methods can provide exact control of false positives and allow the use of non-standard statistics, making only weak assumptions about the data. With the availability of fast and inexpensive computing, their main limitation would be some lack of flexibility to work with arbitrary experimental designs. In this paper we report on results on approximate permutation methods that are more flexible with respect to the experimental design and nuisance variables, and conduct detailed simulations to identify the best method for settings that are typical for imaging research scenarios. We present a generic framework for permutation inference for complex general linear models (glms) when the errors are exchangeable and/or have a symmetric distribution, and show that, even in the presence of nuisance effects, these permutation inferences are powerful while providing excellent control of false positives in a wide range of common and relevant imaging research scenarios. We also demonstrate how the inference on glm parameters, originally intended for independent data, can be used in certain special but useful cases in which independence is violated. Detailed examples of common neuroimaging applications are provided, as well as a complete algorithm - the "randomise" algorithm - for permutation inference with the GLM. {\textcopyright} 2014 The Authors.},
author = {Winkler, Anderson M. and Ridgway, Gerard R. and Webster, Matthew A. and Smith, Stephen M. and Nichols, Thomas E.},
doi = {10.1016/j.neuroimage.2014.01.060},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/Toms Papers/Permutation/Permutation inference for the general linear model.pdf:pdf},
isbn = {1053-8119},
issn = {10959572},
journal = {NeuroImage},
keywords = {General linear model,Multiple regression,Permutation inference,Randomise},
pages = {381--397},
pmid = {24530839},
title = {{Permutation inference for the general linear model}},
volume = {92},
year = {2014}
}
@article{Efron2008a,
abstract = {Modern statisticians are often presented with hundreds or thousands of hypothesis testing problems to evaluate at the same time, generated from new scientific technologies such as microarrays, medical and satellite imaging devices, or flow cytometry counters. The relevant statistical literature tends to begin with the tacit assumption that a single combined analysis, for instance, a False Discovery Rate assessment, should be applied to the entire set of problems at hand. This can be a dangerous assumption, as the examples in the paper show, leading to overly conservative or overly liberal conclusions within any particular subclass of the cases. A simple Bayesian theory yields a succinct description of the effects of separation or combination on false discovery rate analyses. The theory allows efficient testing within small subclasses, and has applications to ``enrichment,'' the detection of multi-case effects.},
archivePrefix = {arXiv},
arxivId = {0803.3863},
author = {Efron, Bradley},
doi = {10.1214/07-AOAS141},
eprint = {0803.3863},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Efron - 2008 - Simultaneous inference When should hypothesis testing problems be combined.pdf:pdf},
isbn = {1932-6157},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {Enrichment,False discovery rates,Separate-class model},
number = {1},
pages = {197--223},
title = {{Simultaneous inference: When should hypothesis testing problems be combined?}},
volume = {2},
year = {2008}
}
@article{Fryzlewicz2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1411.0858v1},
author = {Fryzlewicz, Piotr},
doi = {10.1214/14-AOS1245},
eprint = {arXiv:1411.0858v1},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fryzlewicz - 2014 - Wild Binary Segmentation for multiple change-point detection Segmentation in a simple function noise model(2).pdf:pdf},
issn = {00905364},
keywords = {bayesian information criterion,binary segmentation,change-point detection,multiple change-points,randomized algorithms,thresholding},
number = {January},
title = {{Wild Binary Segmentation for multiple change-point detection Segmentation in a simple function + noise model}},
year = {2014}
}
@article{Li2017,
abstract = {In this paper, we prove that under proper conditions, bootstrap can further debias the debiased Lasso estimator for statistical inference of low-dimensional parameters in high-dimensional linear regression. We prove that the required sample size for inference with bootstrapped debiased Lasso, which involves the number of small coefficients, can be of smaller order than the existing ones for the debiased Lasso. Therefore, our results reveal the benefits of having strong signals. Our theory is supported by results of simulation experiments, which compare coverage probabilities and lengths of confidence intervals with and without bootstrap, with and without debiasing.},
archivePrefix = {arXiv},
arxivId = {1711.03613},
author = {Li, Sai},
eprint = {1711.03613},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Selective Inference/Debiasing the Debiased Lasso with Bootstrap.pdf:pdf},
number = {2014},
pages = {1--33},
title = {{Debiasing the Debiased Lasso with Bootstrap}},
year = {2017}
}
@article{Nichols2003,
author = {Nichols, T. E and Hayasaka, S},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/Toms Papers/Multiple Testing/Controlling the familywise error rate in functional neuroimaging- a comparative review.pdf:pdf},
journal = {Stat Methods Med Res.},
number = {5},
pages = {419--446},
title = {{Controlling the familywise error rates in functional Neuroimaging: a comparative review}},
volume = {23},
year = {2003}
}
@article{Hall1988,
author = {Hall, Peter and Martin, Michael},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Hall{\_}et{\_}al-1988-Australian{\_}Journal{\_}of{\_}Statistics.pdf:pdf},
keywords = {behewfisher problem,bootstrap,cornish-fisher expansion,difference between,edgeworth expansion,means,percentile method,percentile-t method,two-sample problem},
pages = {179--192},
title = {{On the Bootstrap and Two-Sample Problems}},
year = {1988}
}
@article{PeronePacifico2004,
abstract = {This article extends false discovery rates to random fields, for which there are uncountably many hypothesis tests. We develop a method for finding regions in the field's domain where there is a significant signal while controlling either the proportion of area or the proportion of clusters in which false rejections occur. The method produces confidence envelopes for the proportion of false discoveries as a function of the rejection threshold. From the confidence envelopes, we derive threshold procedures to control either the mean or the specified tail probabilities of the false discovery proportion. An essential ingredient of this construnction is a new algorithm to compute a confidence superset for the set of all true-null locations. We demonstrate our method with applications to scan statistics and functional neuroimaging. This article extends false discovery rates to random fields, for which there are uncountably many hypothesis tests. We develop a method for finding regions in the field's domain where there is a significant signal while controlling either the proportion of area or the proportion of clusters in which false rejections occur. The method produces confidence envelopes for the proportion of false discoveries as a function of the rejection threshold. From the confidence envelopes, we derive threshold procedures to control either the mean or the specified tail probabilities of the false discovery proportion. An essential ingredient of this construnction is a new algorithm to compute a confidence superset for the set of all true-null locations. We demonstrate our method with applications to scan statistics and functional neuroimaging.},
author = {{Perone Pacifico}, M. and Genovese, C. and Verdinelli, I. and Wasserman, L.},
doi = {10.1198/0162145000001655},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Perone Pacifico et al. - 2004 - False discovery control for random fields.pdf:pdf},
isbn = {0162-1459},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {False discovery rate,Multiple hypothesis test,Random field,Scan statistic},
number = {468},
pages = {1002--1014},
title = {{False discovery control for random fields}},
volume = {99},
year = {2004}
}
@article{Newman1974,
author = {Newman, Morris},
doi = {10.6028/jres.078B.009},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Newman - 1974 - How to determine accuracy of the output of a matrix inversion program(2).pdf:pdf},
issn = {0098-8979},
journal = {Journal of Research of the National Bureau of Standards, Section B: Mathematical Sciences},
keywords = {approx im ate soluti,approximate inve rses,e rro r bound,ed di gital co,hi gh s pe,mputation,ons,s},
number = {2},
pages = {65},
title = {{How to determine accuracy of the output of a matrix inversion program}},
volume = {78B},
year = {1974}
}
@article{Hansen1996,
author = {Hansen, B Y Bruce E},
file = {:data/fireback/davenpor/davenpor/Referreeing/Testing One Hypothesis Multiple Times- The Multidimensional Case/MArk2/References/Hansen 1991.pdf:pdf},
keywords = {asymptotic theory,identification,nonlinear models,p-values,thresholds},
number = {2},
pages = {413--430},
title = {{Inference When a Nuisance Parameter Is Not Identified Under the Null Hypothesis}},
volume = {64},
year = {1996}
}
@article{Murdoch2008,
abstract = {P-values are taught in introductory statistics classes in a way that confuses many of the students, leading to common misconceptions about their meaning. In this article, we argue that p-values should be taught through simulation, emphasizing that p-values are random variables. By means of elementary examples we illustrate how to teach students valid interpretations of p-values and give them a deeper understanding of hypothesis testing.},
author = {Murdoch, Duncan J and Tsai, Yu-Ling and Adcock, James},
doi = {10.1198/000313008X332421},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Murdoch, Tsai, Adcock - 2008 - P-Values are Random Variables(2).pdf:pdf},
isbn = {0003-1305},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {Empirical cumulative distribution function (ECDF),Histograms,Hypothesis testing,Teaching statistics},
number = {3},
pages = {242--245},
title = {{P-Values are Random Variables}},
volume = {62},
year = {2008}
}
@article{Efron1979,
abstract = {We discuss the following problem: given a random sample X=(X1,X2,⋯,Xn)X=(X1,X2,⋯,Xn)$\backslash$mathbf{\{}X{\}} = (X{\_}1, X{\_}2, $\backslash$cdots, X{\_}n) from an unknown probability distribution FFF, estimate the sampling distribution of some prespecified random variable R(X,F)R(X,F)R($\backslash$mathbf{\{}X{\}}, F), on the basis of the observed data xx$\backslash$mathbf{\{}x{\}}. (Standard jackknife theory gives an approximate mean and variance in the case R(X,F)=$\theta$(F{\^{}})−$\theta$(F),$\theta$R(X,F)=$\theta$(F{\^{}})−$\theta$(F),$\theta$R($\backslash$mathbf{\{}X{\}}, F) = $\backslash$theta($\backslash$hat{\{}F{\}}) - $\backslash$theta(F), $\backslash$theta some parameter of interest.) A general method, called the "bootstrap," is introduced, and shown to work satisfactorily on a variety of estimation problems. The jackknife is shown to be a linear approximation method for the bootstrap. The exposition proceeds by a series of examples: variance of the sample median, error rates in a linear discriminant analysis, ratio estimation, estimating regression parameters, etc.},
archivePrefix = {arXiv},
arxivId = {arXiv:1306.3979v1},
author = {Efron, B.},
doi = {10.1214/aos/1176344552},
eprint = {arXiv:1306.3979v1},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Classical Papers/Bootstrap Methods$\backslash$: Another Look at the Jackknife.pdf:pdf},
isbn = {00905364},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {Bootstrap},
mendeley-tags = {Bootstrap},
number = {1},
pages = {1--26},
pmid = {369},
title = {{Bootstrap Methods: Another Look at the Jackknife}},
url = {http://projecteuclid.org/euclid.aos/1176344552},
volume = {7},
year = {1979}
}
@article{Goeman2012,
abstract = {Hypotheses tests in bioinformatics can often be set in a tree structure in a very natural way, e.g. when tests are performed at probe, gene, and chromosome level. Exploiting this graph structure in a multiple testing procedure may result in a gain in power or increased interpretability of the results.We present the inheritance procedure, a method of familywise error control for hypotheses structured in a tree. The method starts testing at the top of the tree, following up on those branches in which it finds significant results, and following up on leaf nodes in the neighborhood of those leaves. The method is a uniform improvement over a recently proposed method by Meinshausen. The inheritance procedure has been implemented in the globaltest package which is available on www.bioconductor.org.},
author = {Goeman, Jelle J. and Finos, Livio},
doi = {10.1515/1544-6115.1554},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Multiple Testing/Sequences and Trees/The inheritance principle.pdf:pdf},
isbn = {1544-6115 (Electronic)$\backslash$r1544-6115 (Linking)},
issn = {15446115},
journal = {Statistical Applications in Genetics and Molecular Biology},
keywords = {Hierarchical testing,Sequential testing},
number = {1},
pmid = {22499687},
title = {{The inheritance procedure: Multiple testing of tree-structured hypotheses}},
volume = {11},
year = {2012}
}
@article{Yekutieli1998,
abstract = {A new false discovery rate controlling procedure is proposed for multiple hypotheses testing. The procedure makes use of resampling-based p-value adjustment, and is designed to cope with correlated test statistics. Some properties of the proposed procedure are investigated theoretically, and further properties are investigated using a simulation study. According to the results of the simulation study, the new procedure ooers false discovery rate control and greater power. The motivation for developing this resampling-based procedure was an actual problem in meteorology, in which almost 2000 hypotheses are tested simultaneously using highly correlated test statistics. When applied to this problem the increase in power was evident. The same procedure can be used in many other large problems of multiple testing, for example multiple endpoints. The procedure is also extended to serve as a general diagnostic tool in model selection.},
author = {Yekutieli, Daniel and Benjamini, Yoav},
doi = {10.1016/S0378-3758(99)00041-5},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yekutieli, Benjamini - 1999 - Resampling-based false discovery rate controlling multiple test procedures for correlated test statistics.pdf:pdf},
isbn = {0378-3758},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {62G09,62G10,62H11,62H15 Keywords,62J15,MSC,Meteorology,Model selection,Multiple comparisons,Multiple endpoints},
pages = {171--196},
pmid = {83580500015},
title = {{Resampling-based false discovery rate controlling multiple test procedures for correlated test statistics}},
volume = {82},
year = {1999}
}
@article{Deng2017,
abstract = {The Bonferroni adjustment, or the union bound, is commonly used to study rate optimality properties of statistical methods in high-dimensional problems. However, in practice, the Bonferroni adjustment is overly conservative. The extreme value theory has been proven to provide more accurate multiplicity adjustments in a number of settings, but only on ad hoc basis. Recently, Gaussian approximation has been used to justify bootstrap adjustments in large scale simultaneous inference in some general settings when {\$}n \backslashgg (\backslashlog p){\^{}}7{\$}, where {\$}p{\$} is the multiplicity of the inference problem and {\$}n{\$} is the sample size. The thrust of this theory is the validity of the Gaussian approximation for maxima of sums of independent random vectors in high-dimension. In this paper, we reduce the sample size requirement to {\$}n \backslashgg (\backslashlog p){\^{}}5{\$} for the consistency of the empirical bootstrap and the multiplier/wild bootstrap in the Kolmogorov-Smirnov distance, possibly in the regime where the Gaussian approximation is not available. New comparison and anti-concentration theorems, which are of considerable interest in and of themselves, are developed as existing ones interweaved with Gaussian approximation are no longer applicable.},
archivePrefix = {arXiv},
arxivId = {1705.09528},
author = {Deng, Hang and Zhang, Cun-Hui},
eprint = {1705.09528},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Deng, Zhang - 2017 - Beyond Gaussian Approximation Bootstrap for Maxima of Sums of Independent Random Vectors.pdf:pdf},
keywords = {1,anti-concentration,comparison theorem,empirical bootstrap,gaussian approximation,introduction,let x,lindeberg interpolation,matrix with independent rows,maxima of sums,multiple testing,multiplier bootstrap,p be a random,r n,simultaneous confidence intervals,t,wild bootstrap,x 1,x n},
number = {1},
pages = {1--53},
title = {{Beyond Gaussian Approximation: Bootstrap for Maxima of Sums of Independent Random Vectors}},
year = {2017}
}
@article{Pollard2002,
abstract = {Current methods for analysis of gene expression data are mostly based on clustering and classification of either genes or samples. We offer support for the idea that more complex patterns can be identified in the data if genes and samples are considered simultaneously. We formalize the approach and propose a statistical framework for two-way clustering. A simultaneous clustering parameter is defined as a function ?? = ??(P) of the true data generating distribution P, and an estimate is obtained by applying this function to the empirical distribution Pn. We illustrate that a wide range of clustering procedures, including generalized hierarchical methods, can be defined as parameters which are compositions of individual mappings for clustering patients and genes. This framework allows one to assess classical properties of clustering methods, such as consistency, and to formally study statistical inference regarding the clustering parameter. We present results of simulations designed to assess the asymptotic validity of different bootstrap methods for estimating the distribution of ??(Pn). The method is illustrated on a publicly available data set. ?? 2002 Published by Elsevier Science Inc.},
author = {Pollard, Katherine S. and {Van der Laan}, Mark J.},
doi = {10.1016/S0025-5564(01)00116-X},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pollard, Van der Laan - 2002 - Statistical inference for simultaneous clustering of gene expression data.pdf:pdf},
issn = {00255564},
journal = {Mathematical Biosciences},
keywords = {Bootstrap,Clustering,Drug discovery,Gene expression,Parameter},
number = {1},
pages = {99--121},
pmid = {11867086},
title = {{Statistical inference for simultaneous clustering of gene expression data}},
volume = {176},
year = {2002}
}
@misc{Troendle1995,
abstract = {In quantitative proteomics work, the differences in expression of many separate proteins are routinely examined to test for significant differences between treatments. This leads to the multiple hypothesis testing problem: when many separate tests are performed many will be significant by chance and be false positive results. Statistical methods such as the false discovery rate method that deal with this problem have been disseminated for more than one decade. However a survey of proteomics journals shows that such tests are not widely implemented in one commonly used technique, quantitative proteomics using two-dimensional electrophoresis. We outline a selection of multiple hypothesis testing methods, including some that are well known and some lesser known, and present a simple strategy for their use by the experimental scientist in quantitative proteomics work generally. The strategy focuses on the desirability of simultaneous use of several different methods, the choice and emphasis dependent on research priorities and the results in hand. This approach is demonstrated using case scenarios with experimental and simulated model data.},
author = {Troendle, J F},
booktitle = {Jasa},
doi = {10.1074/mcp.M110.004374},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Troendle - 1995 - A stepwise resampling method of multiple hypothesis testing.pdf:pdf},
issn = {1535-9484},
pages = {370--378},
pmid = {21364085},
title = {{A stepwise resampling method of multiple hypothesis testing}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21788105},
volume = {90},
year = {1995}
}
@article{Barbour2006a,
abstract = {In this paper, we adapt the very effective Berry�Esseen theorems of Chen and Shao (2004), which apply to sums of locally dependent random variables, for use with randomly indexed sums. Our particular interest is in random variables resulting from integrating a random field with respect to a point process. We illustrate the use of our theorems in three examples: in a rather general model of the insurance collective; in problems in geometrical probability involving stabilizing functionals; and in counting the maximal points in a two-dimensional region. ABSTRACT FROM AUTHOR},
author = {Barbour, A. D. and Xia, Aihua},
doi = {10.1239/aap/1158684998},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/Random{\_}Field{\_}Theory/Other/Lattice/normal{\_}approximation{\_}for{\_}random{\_}sums.pdf:pdf},
issn = {00018678},
journal = {Advances in Applied Probability},
keywords = {Berry-Esseen bound,Local dependence,Point process,Random field,Stein's method,Two-dimensional maximum},
number = {3},
pages = {693--728},
title = {{Normal approximation for random sums}},
volume = {38},
year = {2006}
}
@article{Chernozhukov2013,
abstract = {We derive a Gaussian approximation result for the maximum of a sum of high-dimensional random vectors. Specifically, we establish conditions under which the distribution of the maximum is approximated by that of the maximum of a sum of the Gaussian random vectors with the same covariance matrices as the original vectors. This result applies when the dimension of random vectors ({\$}p{\$}) is large compared to the sample size ({\$}n{\$}); in fact, {\$}p{\$} can be much larger than {\$}n{\$}, without restricting correlations of the coordinates of these vectors. We also show that the distribution of the maximum of a sum of the random vectors with unknown covariance matrices can be consistently estimated by the distribution of the maximum of a sum of the conditional Gaussian random vectors obtained by multiplying the original vectors with i.i.d. Gaussian multipliers. This is the Gaussian multiplier (or wild) bootstrap procedure. Here too, {\$}p{\$} can be large or even much larger than {\$}n{\$}. These distributional approximations, either Gaussian or conditional Gaussian, yield a high-quality approximation to the distribution of the original maximum, often with approximation error decreasing polynomially in the sample size, and hence are of interest in many applications. We demonstrate how our Gaussian approximations and the multiplier bootstrap can be used for modern high-dimensional estimation, multiple hypothesis testing, and adaptive specification testing. All these results contain nonasymptotic bounds on approximation errors.},
archivePrefix = {arXiv},
arxivId = {1212.6906},
author = {Chernozhukov, Victor and Chetverikov, Denis and Kato, Kengo},
doi = {10.1214/13-AOS1161},
eprint = {1212.6906},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Chernozukov/GAUSSIAN APPROXIMATIONS AND MULTIPLIER BOOTSTRAP FOR MAXIMA OF SUMS OF HIGH-DIMENSIONAL RANDOM VECTORS.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Anti-concentration,Dantzig selector,High dimensionality,Maximum of vector sums,Slepian,Stein method},
number = {6},
pages = {2786--2819},
title = {{Gaussian approximations and multiplier bootstrap for maxima of sums of high-dimensional random vectors}},
volume = {41},
year = {2013}
}
@misc{Koller2009,
author = {Koller, Daphne and Friedman, Nir},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Koller, Friedman - 2009 - Probabilistic Graphical Models- Principles and Techniques.pdf(2).pdf:pdf},
isbn = {9780262013192},
pages = {1231},
title = {{Probabilistic Graphical Models- Principles and Techniques.pdf}},
year = {2009}
}
@article{Dixon1996,
author = {Dixon, Mark and Coles, Stuart},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Tennis/soccer{\_}betting.pdf:pdf},
keywords = {Betting,Football,Sport},
mendeley-tags = {Betting,Football,Sport},
title = {{Modelling Association Football Scores and Inefficiences in the Football Betting Market}},
year = {1996}
}
@article{Beran1986,
author = {Beran, Rudolf},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Testing/Simulated Power Functions.pdf:pdf},
journal = {Annals of Statistics},
number = {1},
pages = {151--173},
title = {{Simulated Power Functions}},
volume = {14},
year = {1986}
}
@article{Becher2017,
author = {Becher, Heiko and Hall, Peter and Wilson, Susan R.},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Becher, Hall, Wilson - 2017 - Bootstrap Hypothesis Testing.pdf:pdf},
journal = {Biometrics},
number = {3},
pages = {969--970},
title = {{Bootstrap Hypothesis Testing}},
volume = {48},
year = {2017}
}
@article{Bissiri2016,
abstract = {We propose a framework for general Bayesian inference. We argue that a valid update of a prior belief distribution to a posterior can be made for parameters which are connected to observations through a loss function rather than the traditional likelihood function, which is recovered under the special case of using self information loss. Modern application areas make it is increasingly challenging for Bayesians to attempt to model the true data generating mechanism. Moreover, when the object of interest is low dimensional, such as a mean or median, it is cumbersome to have to achieve this via a complete model for the whole data distribution. More importantly, there are settings where the parameter of interest does not directly index a family of density functions and thus the Bayesian approach to learning about such parameters is currently regarded as problematic. Our proposed framework uses loss-functions to connect information in the data to functionals of interest. The updating of beliefs then follows from a decision theoretic approach involving cumulative loss functions. Importantly, the procedure coincides with Bayesian updating when a true likelihood is known, yet provides coherent subjective inference in much more general settings. Connections to other inference frameworks are highlighted.},
archivePrefix = {arXiv},
arxivId = {1306.6430},
author = {Bissiri, P. G. and Holmes, C. C. and Walker, S. G.},
doi = {10.1111/rssb.12158},
eprint = {1306.6430},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bayesian/A General Framework for Updating Belief.pdf:pdf},
issn = {14679868},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Decision theory,General Bayesian updating,Generalized estimating equations,Gibbs posteriors,Information,Loss function,Maximum entropy,Provably approximately correct Bayes methods,Self-information loss function},
number = {5},
pages = {1103--1130},
title = {{A general framework for updating belief distributions}},
volume = {78},
year = {2016}
}
@article{Brill1992,
abstract = {ABSTRACT - The task of classifiers is to determine the appropriate class name when presented with a sample from one of several classes. In forming the sample to present to the classifier, there may be a large number of measurements one can make. Feature selection addresses the problem of determining which of these measurements are the most useful for determining the pattern's class. In this paper, we describe experiments using a genetic algorithm for feature selection in the context of neural network classifiers, specifically, counterpropagation networks. We present two novel techniques in our application of genetic algorithms. First, we configure our genetic algorithm to use an approximate evaluation in order to reduce significantly the computation required. In particular, though our desired classifiers are counterpropagation networks, we use a nearest-neighbor classifier to evaluate feature sets. We show that the features selected by this method are effective in the context of counterpropagation networks. Second, we propose a method we call training set sampling, in which only a portion of the training set is used on any given evaluation. Again, significant computational savings can be made by using this method, i.e., evaluations can be made over an order of magnitude faster. This method selects feature sets that are as good as and occasionally better for counterpropagation than those chosen by an evaluation that uses the entire training set.},
author = {Brill, Frank Z. and Brown, Donald E. and Martin, Worthy N.},
doi = {10.1109/72.125874},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brill, Brown, Martin - 1992 - Fast Genetic Selection of Features for Neural Network Classifiers(2).pdf:pdf},
isbn = {1045-9227},
issn = {19410093},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pages = {324--328},
pmid = {18276434},
title = {{Fast Genetic Selection of Features for Neural Network Classifiers}},
volume = {3},
year = {1992}
}
@article{Kato2012,
abstract = {This paper studies estimation in functional linear quantile regression in which the dependent variable is scalar while the covariate is a function, and the conditional quantile for each fixed quantile index is modeled as a linear functional of the covariate. Here we suppose that covariates are discretely observed and sampling points may differ across subjects, where the number of measurements per subject increases as the sample size. Also, we allow the quantile index to vary over a given subset of the open unit interval, so the slope function is a function of two variables: (typically) time and quantile index. Likewise, the conditional quantile function is a function of the quantile index and the covariate. We consider an estimator for the slope function based on the principal component basis. An estimator for the conditional quantile function is obtained by a plug-in method. Since the so-constructed plug-in estimator not necessarily satisfies the monotonicity constraint with respect to the quantile index, we also consider a class of monotonized estimators for the conditional quantile function. We establish rates of convergence for these estimators under suitable norms, showing that these rates are optimal in a minimax sense under some smoothness assumptions on the covariance kernel of the covariate and the slope function. Empirical choice of the cutoff level is studied by using simulations.},
archivePrefix = {arXiv},
arxivId = {arXiv:1202.4850v2},
author = {Kato, Kengo},
doi = {10.1214/12-AOS1066},
eprint = {arXiv:1202.4850v2},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/FunctionalDataAnlaysis/Estimation in functional linear quantile regression.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Functional data,Nonlinear ill-posed problem,Principal component analysis,Quantile regression},
number = {6},
pages = {3108--3136},
title = {{Estimation in functional linear quantile regression}},
volume = {40},
year = {2012}
}
@article{Weber2015,
author = {Weber, Richard},
file = {:data/fireback/davenpor/davenpor/General Notes/Statistics/stats1b.pdf:pdf},
isbn = {0534504795},
journal = {Cambridge Lecture Notes},
title = {{Statistics IB}}
}
@article{Vitells2011,
abstract = {In experiments that are aimed at detecting astrophysical sources such as neutrino telescopes, one usually performs a search over a continuous parameter space (e.g. the angular coordinates of the sky, and possibly time), looking for the most significant deviation from the background hypothesis. Such a procedure inherently involves a "look elsewhere effect", namely, the possibility for a signal-like fluctuation to appear anywhere within the search range. Correctly estimating the p-value of a given observation thus requires repeated simulations of the entire search, a procedure that may be prohibitively expansive in terms of CPU resources. Recent results from the theory of random fields provide powerful tools which may be used to alleviate this difficulty, in a wide range of applications. We review those results and discuss their implementation, with a detailed example applied for neutrino point source analysis in the IceCube experiment. {\textcopyright} 2011 Published by Elsevier B.V.},
archivePrefix = {arXiv},
arxivId = {1105.4355},
author = {Vitells, Ofer and Gross, Eilam},
doi = {10.1016/j.astropartphys.2011.08.005},
eprint = {1105.4355},
file = {:data/fireback/davenpor/davenpor/Referreeing/Testing One Hypothesis Multiple Times- The Multidimensional Case/Estimating the significance of a signal in a multi-dimensional search.pdf:pdf},
issn = {09276505},
journal = {Astroparticle Physics},
keywords = {Look-elsewhere effect,Neutrino telescope,Random fields,Statistical significance},
number = {5},
pages = {230--234},
title = {{Estimating the significance of a signal in a multi-dimensional search}},
volume = {35},
year = {2011}
}
@article{Csardi2006,
author = {Csardi, Gabor and Nepusz, Tamas},
journal = {InterJournal},
pages = {1695},
title = {{iGraph}},
url = {http://igraph.org},
volume = {Complex Sy},
year = {2006}
}
@article{Algeri2016,
abstract = {The search for new significant peaks over a energy spectrum often involves a statistical multiple hypothesis testing problem. Separate tests of hypothesis are conducted at different locations producing an ensemble of local p-values, the smallest of which is reported as evidence for the new resonance. Unfortunately, controlling the false detection rate (type I error rate) of such procedures may lead to excessively stringent acceptance criteria. In the recent physics literature, two promising statistical tools have been proposed to overcome these limitations. In 2005, a method to "find needles in haystacks" was introduced by Pilla et al. [1], and a second method was later proposed by Gross and Vitells [2] in the context of the "look elsewhere effect" and trial factors. We show that, for relatively small sample sizes, the former leads to an artificial inflation of statistical power that stems from an increase in the false detection rate, whereas the two methods exhibit similar performance for large sample sizes. We apply the methods to realistic simulations of the Fermi Large Area Telescope data, in particular the search for dark matter annihilation lines. Further, we discuss the counter-intutive scenario where the look-elsewhere corrections are more conservative than much more computationally efficient corrections for multiple hypothesis testing. Finally, we provide general guidelines for navigating the tradeoffs between statistical and computational efficiency when selecting a statistical procedure for signal detection.},
archivePrefix = {arXiv},
arxivId = {1602.03765},
author = {Algeri, S. and {Van Dyk}, D. A. and Conrad, J. and Anderson, B.},
doi = {10.1088/1748-0221/11/12/P12010},
eprint = {1602.03765},
file = {:data/fireback/davenpor/davenpor/Referreeing/Testing One Hypothesis Multiple Times- The Multidimensional Case/On methods for correcting for the look-elsewhere effect in searches for new physics.pdf:pdf},
issn = {17480221},
journal = {Journal of Instrumentation},
number = {12},
title = {{On methods for correcting for the look-elsewhere effect in searches for new physics}},
volume = {11},
year = {2016}
}
@article{Bohrnstedt2016,
author = {Bohrnstedt, G and Goldberger, A},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/statistics/On the Covariance of Products of Random Variables.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {328},
pages = {1439--1442},
title = {{On the Exact Covariance of Products of Random Variables}},
volume = {64},
year = {2016}
}
@article{Dudoit2003,
abstract = {DNA microarrays are part of a new and promising class of biotechnologies that allow the monitoring of expression levels in cells for thousands of genes simultaneously. An important and common question in DNA microarray experiments is the identification of differentially expressed genes, that is, genes whose expression levels are associated with a response or covariate of interest. The biological question of differential expression can be restated as a problem in multiple hypothesis testing: the simultaneous test for each gene of the null hypothesis of no association between the expression levels and the responses or covariates. As a typical microarray experiment measures expression levels for thousands of genes simultaneously, large multiplicity problems are generated. This article discusses different approaches to multiple hypothesis testing in the context of DNA microarray experiments and compares the procedures on microarray and simulated data sets.},
author = {Dudoit, Sandrine and Shaffer, Juliet Popper and Boldrick, Jennifer C.},
doi = {10.1214/ss/1056397487},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dudoit, Shaffer, Boldrick - 2003 - Multiple Hypothesis Testing in Microarray Experiments.pdf:pdf},
isbn = {08834237},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {adjusted p -value,and phrases,dna,false discovery rate,family-wise type i error,multiple hypothesis testing,permutation,rate},
number = {1},
pages = {71--103},
title = {{Multiple Hypothesis Testing in Microarray Experiments}},
volume = {18},
year = {2003}
}
@article{Loftus2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1405.3920v1},
author = {Loftus, Joshua R. and Taylor, Jonathan},
eprint = {arXiv:1405.3920v1},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Loftus, Taylor - 2014 - A significance test for forward stepwise model selection(2).pdf:pdf},
journal = {arXiv preprint},
keywords = {forward stepwise,model selection,significance},
title = {{A significance test for forward stepwise model selection}},
year = {2014}
}
@article{Saeb2014,
abstract = {Extreme value theory is concerned with probabilistic and statistical questions related to very high or very low values in sequences of random variables and in stochastic processes. The subject has a rich mathematical theory and also a long tradition of applications in a variety of areas. Among many excellent books on the subject, Coles [2] while the book by concentrates on data analysis and statistical inference for extremes. In this article, we present a case study wherein we model annual maximum yearly rainfall data using the generalized extreme value distribution. Also, we use R software for data analysis and give the R codes in the appendix.},
archivePrefix = {arXiv},
arxivId = {1402.0944},
author = {Saeb, Ali},
eprint = {1402.0944},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/EVT/General Extreme Value Modeling and Application of Bootstrap on Rainfall Data - A Case Study.pdf:pdf},
keywords = {bootstrap,general extreme value distribution,jackknife method,return},
pages = {1--11},
title = {{General extreme value modeling and application of bootstrap on rainfall data - A case study}},
year = {2014}
}
@article{Ghahramani2005,
author = {Ghahramani, Zoubin},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghahramani - 2005 - Non-parametric Bayesian Methods(2).pdf:pdf},
number = {July 2005},
pages = {1--55},
title = {{Non-parametric Bayesian Methods}},
year = {2005}
}
@misc{Golub1979,
abstract = {Consider the ridge estimate $\beta$̂($\lambda$) for $\beta$ in the model y=X$\beta$ +$\epsilon$,$\epsilon$ ∼ N(0,$\sigma$ 2I), $\sigma$ 2 unknown, $\beta$̂($\lambda$)=(XTX+n$\lambda$ I)-1 XTy. We study the method of generalized cross-validation (GCV) for choosing a good value $\lambda$̂ for $\lambda$, from the data. The estimate $\lambda$̂ is the minimizer of V($\lambda$) given by {\$}V(\backslashlambda)=\backslashfrac{\{}1{\}}{\{}n{\}}\backslash|(I-A(\backslashlambda))y\backslash|{\^{}}{\{}2{\}}/\backslashleft[\backslashfrac{\{}1{\}}{\{}n{\}}\backslashtext{\{}Trace{\}}(I-A(\backslashlambda))\backslashright]{\^{}}{\{}2{\}}{\$}, where A($\lambda$)=X(XTX+n$\lambda$ I)-1XT. This estimate is a rotation-invariant version of Allen's PRESS, or ordinary cross-validation. This estimate behaves like a risk improvement estimator, but does not require an estimate of $\sigma$ 2, so can be used when n - p is small, or even if p ≥ n in certain cases. The GCV method can also be used in subset selection and singular value truncation methods for regression, and even to choose from among mixtures of these methods.},
author = {Golub, Gene H. and Heath, Michael and Wahba, Grace},
booktitle = {Technometrics},
doi = {10.2307/1268518},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Regularized Regression/Generalized Cross-Validation as a Method for Choosing a Good Ridge Parameter..pdf:pdf},
isbn = {0040-1706},
issn = {00401706},
number = {2},
pages = {215},
title = {{Generalized Cross-Validation as a Method for Choosing a Good Ridge Parameter}},
url = {http://www.jstor.org/stable/1268518?origin=crossref},
volume = {21},
year = {1979}
}
@article{Zablocki1970,
abstract = {Data from a nationwide, longitudinal study conducted 1974-1976, including survey {\&} ethnographic observation data for some 1,500 members of 60 urban communal living groups, are drawn on to explore who joined the communes of the 1970s. The findings challenge many common assumptions about participants {\&} their motivations: not all commune members were from middle-class backgrounds; very few came from broken homes; they were less alienated than other Americans at this time; while many were innovators {\&} experimenters, their involvement in nonconventional political, sexual, {\&} drug-related behaviors actually decreased after joining a communal group; {\&} relatively few were deliberately trying to create a new family form. Consensual community (in response to the unraveling of broader social unity) was the reason most often given for joining a communal group. Thus the commune movement focused on the cognitive {\&} moral, as well as affective dimensions of intentionally created networks of emotional support. 8 Tables, 37 References. Adapted from the source document.},
author = {Aidala, Angela A and Zablocki, Benjamin D},
doi = {10.1300/J002v17n01_06},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Social Statistics Papers/Communes/The Communes of the 1970s.pdf:pdf},
issn = {0149-4929, 0149-4929},
journal = {Marriage and Family Review},
keywords = {*Communes,*Social Movements,*Urban Population,0826: mass phenomena,1974-1976 national longitudinal data,article,members' characteris,social movements,urban communal living groups},
number = {1-2},
pages = {87--116},
title = {{The Communes of the 1970s: Who Joined and Why?}},
volume = {17},
year = {1991}
}
@article{Crainiceanu2009,
abstract = {We introduce Generalized Multilevel Functional Linear Models (GMFLMs), a novel statistical framework for regression models where exposure has a multilevel functional structure. We show that GMFLMs are, in fact, generalized multilevel mixed models (GLMMs). Thus, GMFLMs can be analyzed using the mixed effects inferential machinery and can be generalized within a well researched statistical framework. We propose and compare two methods for inference: 1) a two-stage frequentist approach; and 2) a joint Bayesian analysis. Our methods are motivated by and applied to the Sleep Heart Health Study (SHHS), the largest community cohort study of sleep. However, our methods are general and easy to apply to a wide spectrum of emerging biological and medical data sets. Supplemental materials for this article are available online.},
author = {Crainiceanu, Ciprian M. and Staicu, Ana Maria and Di, Chong Zhi},
doi = {10.1198/jasa.2009.tm08564},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/FunctionalDataAnlaysis/Generalized Multilevel Functional Regression.pdf:pdf},
isbn = {0162-1459 (Print)$\backslash$r0162-1459 (Linking)},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Functional principal components,Sleep EEG,Smoothing},
number = {488},
pages = {1550--1561},
pmid = {20625442},
title = {{Generalized multilevel functional regression}},
volume = {104},
year = {2009}
}
@article{Roberts2004,
abstract = {This paper surveys various results about Markov chains on general (non-countable) state spaces. It begins with an introduction to Markov chain Monte Carlo (MCMC) algorithms, which provide the motivation and context for the theory which follows. Then, sufficient conditions for geometric and uniform ergodicity are presented, along with quantitative bounds on the rate of convergence to stationarity. Many of these results are proved using direct coupling constructions based on minorisation and drift conditions. Necessary and sufficient conditions for Central Limit Theorems (CLTs) are also presented, in some cases proved via the Poisson Equation or direct regeneration constructions. Finally, optimal scaling and weak convergence results for Metropolis-Hastings algorithms are discussed. None of the results presented is new, though many of the proofs are. We also describe some Open Problems.},
archivePrefix = {arXiv},
arxivId = {arXiv:math/0404033v4},
author = {Roberts, Gareth O and Rosenthal, Jeffrey S},
doi = {10.1214/154957804100000024},
eprint = {0404033v4},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Roberts, Rosenthal - 2004 - General state space Markov chains and MCMC algorithms(2).pdf:pdf},
isbn = {1549-5787},
issn = {1549-5787},
journal = {Probability Surveys},
keywords = {qa mathematics},
pages = {20--71},
primaryClass = {arXiv:math},
title = {{General state space Markov chains and MCMC algorithms.}},
volume = {1},
year = {2004}
}
@article{Ambrosio2009,
author = {Ambrosio, Luigi and Gigli, Nicola},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Metrics/Mallows Metric.pdf:pdf},
keywords = {Wasserstein Distance},
mendeley-tags = {Wasserstein Distance},
pages = {1--128},
title = {{A user's guide to optimal transport}},
year = {2009}
}
@article{Young1994,
author = {Young, Alastair G.},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Classical Papers/Summaries/Bootstrap- More than a Stab in the Dark?.pdf:pdf},
journal = {Statistical Science},
number = {3},
pages = {382--415},
title = {{Bootstrap: More than a Stab in the Dark?}},
volume = {9},
year = {1994}
}
@article{Gross2010,
abstract = {When searching for a new resonance somewhere in a possible mass range, the significance of observing a local excess of events must take into account the probability of observing such an excess anywhere in the range. This is the so called "look elsewhere effect". The effect can be quantified in terms of a trial factor, which is the ratio between the probability of observing the excess at some fixed mass point, to the probability of observing it anywhere in the range. We propose a simple and fast procedure for estimating the trial factor, based on earlier results by Davies. We show that asymptotically, the trial factor grows linearly with the (fixed mass) significance.},
archivePrefix = {arXiv},
arxivId = {1005.1891},
author = {Gross, Eilam and Vitells, Ofer},
doi = {10.1140/epjc/s10052-010-1470-8},
eprint = {1005.1891},
file = {:data/fireback/davenpor/davenpor/Referreeing/Testing One Hypothesis Multiple Times- The Multidimensional Case/Gross and Ofer Vitells.pdf:pdf},
issn = {14346044},
journal = {European Physical Journal C},
number = {1},
pages = {525--530},
title = {{Trial factors for the look elsewhere effect in high energy physics}},
volume = {70},
year = {2010}
}
@misc{Westfall1993,
author = {Westfall, Peter H and Young, S Stanley},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Westfall, Young - 1993 - Resampling-based multiple testing. Examples and methods for p-value adjustment.pdf:pdf},
isbn = {0471557617},
pages = {1--184},
title = {{Resampling-based multiple testing. Examples and methods for p-value adjustment.}},
url = {papers2://publication/uuid/21ABA9C5-67BB-4C6D-BE4C-9EA8CCA16E9D},
year = {1993}
}
@article{Flandin2016a,
abstract = {This technical report revisits the analysis of family-wise error rates in statistical parametric mapping - using random field theory - reported in (Eklund et al., 2015). Contrary to the understandable spin that these sorts of analyses attract, a review of their results suggests that they endorse the use of parametric assumptions - and random field theory - in the analysis of functional neuroimaging data. We briefly rehearse the advantages parametric analyses offer over nonparametric alternatives and then unpack the implications of (Eklund et al., 2015) for parametric procedures.},
archivePrefix = {arXiv},
arxivId = {1606.08199},
author = {Flandin, Guillaume and Friston, Karl J.},
doi = {10.1073/pnas.1602413113},
eprint = {1606.08199},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Flandin, Friston - 2016 - Analysis of family-wise error rates in statistical parametric mapping using random field theory(2).pdf:pdf},
isbn = {0027-8424},
issn = {0027-8424},
pages = {1--4},
pmid = {27357684},
title = {{Analysis of family-wise error rates in statistical parametric mapping using random field theory}},
url = {http://arxiv.org/abs/1606.08199},
year = {2016}
}
@article{Teh2010,
abstract = {Sammut, C and Webb, GI, (eds.) . (280 - 287). Springer},
author = {Teh, Yee Whye},
doi = {10.1007/978-0-387-30164-8_219},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Teh - 2010 - Dirichlet Process(2).pdf:pdf},
isbn = {978-0-387-30768-8},
issn = {1941-7330},
journal = {Encyclopedia of Machine Learning},
pages = {280--287},
pmid = {21031154},
title = {{Dirichlet Process}},
year = {2010}
}
@article{Holmes2015,
abstract = {In this article we describe Bayesian nonparametric procedures for two-sample hypothesis testing. Namely, given two sets of samples y{\^{}}{\{}(1){\}} iid F{\^{}}{\{}(1){\}} and y{\^{}}{\{}(2){\}} iid F{\^{}}{\{}(2){\}}, with F{\^{}}{\{}(1){\}}, F{\^{}}{\{}(2){\}} unknown, we wish to evaluate the evidence for the null hypothesis H{\_}{\{}0{\}}:F{\^{}}{\{}(1){\}} = F{\^{}}{\{}(2){\}} versus the alternative. Our method is based upon a nonparametric Polya tree prior centered either subjectively or using an empirical procedure. We show that the Polya tree prior leads to an analytic expression for the marginal likelihood under the two hypotheses and hence an explicit measure of the probability of the null Pr(H{\_}{\{}0{\}}|y{\^{}}{\{}(1){\}},y{\^{}}{\{}(2){\}}).},
archivePrefix = {arXiv},
arxivId = {0910.5060},
author = {Holmes, Chris C. and Caron, Fran{\c{c}}ois and Griffin, Jim E. and Stephens, David A.},
doi = {10.1214/14-BA914},
eprint = {0910.5060},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Holmes et al. - 2015 - Two-sample Bayesian nonparametric hypothesis testing(2).pdf:pdf},
issn = {19316690},
journal = {Bayesian Analysis},
keywords = {Bayesian nonparametrics,Hypothesis testing,P??lya tree},
number = {2},
pages = {297--320},
title = {{Two-sample Bayesian nonparametric hypothesis testing}},
volume = {10},
year = {2015}
}
@book{Leadbetter1983,
author = {Leadbetter, M.R and Lindgren, G and Rootzen, H},
title = {{Extremes and Related Properties of Random Sequences and Processes}},
year = {1983}
}
@article{Catanese2006,
author = {Catanese, Fabrizio and Hoşten, Serkan and Khetan, Amit and Sturmfels, Bernd},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Catanese et al. - 2006 - The Maximum Likelihood Degree(2).pdf:pdf},
number = {3},
pages = {671--697},
title = {{The Maximum Likelihood Degree}},
volume = {128},
year = {2006}
}
@article{Adler2000,
author = {Adler, Robert J.},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Adler - 2000 - On Excursion Sets, Tube Formulas and Maxima of Random Fields.pdf:pdf},
journal = {The Annals of Applied Probabiliity},
number = {1},
pages = {1--74},
title = {{On Excursion Sets, Tube Formulas and Maxima of Random Fields}},
volume = {10},
year = {2000}
}
@article{Berger1982,
author = {Berger, Roger L},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Berger - 1996 - Likelihood Ratio Tests and Intersection- Union Tests.pdf:pdf},
keywords = {and phrases,intersection-union test,likelihood ratio test,normal mean,power,sample size,size},
number = {2288},
title = {{Likelihood Ratio Tests and Intersection- Union Tests}},
year = {1996}
}
@article{Hall2016,
author = {Hall, B Y Peter and Horowitz, Joel L},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hall, Horowitz - 2016 - Bootstrap Critical Values for Tests Based on Generalized-Method-of-Moments Estimators Author ( s ) Peter Hall an.pdf:pdf},
number = {4},
pages = {891--916},
title = {{Bootstrap Critical Values for Tests Based on Generalized-Method-of-Moments Estimators Author ( s ): Peter Hall and Joel L . Horowitz Published by : The Econometric Society Stable URL : http://www.jstor.org/stable/2171849 Accessed : 04-06-2016 13 : 27 UTC }},
volume = {64},
year = {2016}
}
@article{Nasseroleslami2018,
author = {Nasseroleslami, Bahman},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/Statistical fMRI papers/An Implementation of Empirical Bayesian Inference and Non-Null Bootstrapping for Threshold Selection and Power Estimation in Multiple and Single Statistical Testing.pdf:pdf},
title = {{An Implementation of Empirical Bayesian Inference and Non-Null Bootstrapping for Threshold Selection and Power Estimation in Multiple and Single Statistical Testing}},
year = {2018}
}
@article{Sommerfeld2017,
archivePrefix = {arXiv},
arxivId = {1501.07000},
author = {Sommerfeld, Max and Sain, Stephan and Schwartzman, Armin},
doi = {10.1080/01621459.2017.1341838},
eprint = {1501.07000},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sommerfeld, Sain, Schwartzman - 2017 - Confidence regions for spatial excursion sets from repeated random field observations, with an ap.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
pages = {0--0},
title = {{Confidence regions for spatial excursion sets from repeated random field observations, with an application to climate}},
volume = {1459},
year = {2017}
}
@book{Gibilisco2009,
author = {Gibilisco, P and Riccomagno, E and Rogantin, MP},
booktitle = {Algebraic and geometric},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gibilisco, Riccomagno, Rogantin - 2009 - Algebraic and geometric methods in statistics(2).pdf:pdf},
isbn = {9780521896191},
pmid = {16441027},
title = {{Algebraic and geometric methods in statistics}},
year = {2009}
}
@article{Adams2007,
abstract = {Changepoints are abrupt variations in the generative parameters of a data sequence. Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics. While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem. Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint. We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. Our implementation is highly modular so that the algorithm may be applied to a variety of types of data. We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.},
archivePrefix = {arXiv},
arxivId = {0710.3742},
author = {Adams, Ryan Prescott and Mackay, David J. C.},
doi = {arXiv:0710.3742v1},
eprint = {0710.3742},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Adams, Mackay - 2007 - Bayesian Online Changepoint Detection(2).pdf:pdf},
isbn = {9781509028337},
keywords = {changepoints,geometric prior,updating},
mendeley-tags = {changepoints,geometric prior,updating},
pages = {7},
title = {{Bayesian Online Changepoint Detection}},
year = {2007}
}
@article{Sam:MeetingswithTom,
author = {Davenport, Sam},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davenport - 2017 - Meetings with Tom During the MiniProject(2).pdf:pdf},
pages = {2017},
title = {{Meetings with Tom During the MiniProject}},
year = {2017}
}
@article{Fithian2014,
abstract = {To perform inference after model selection, we propose controlling the selective type I error; i.e., the error rate of a test given that it was performed. By doing so, we recover long-run frequency properties among selected hypotheses analogous to those that apply in the classical (non-adaptive) context. Our proposal is closely related to data splitting and has a similar intuitive justification, but is more powerful. Exploiting the classical theory of Lehmann and Scheffe (1955), we derive most powerful unbiased selective tests and confidence intervals for inference in exponential family models after arbitrary selection procedures. For linear regression, we derive new selective z-tests that generalize recent proposals for inference after model selection and improve on their power, and new selective t-tests that do not require knowledge of the error variance sigma{\^{}}2.},
archivePrefix = {arXiv},
arxivId = {arXiv:1410.2597v2},
author = {Fithian, William and Sun, Dennis and Taylor, Jonathan},
doi = {10.1111/j.1467-UMVU.},
eprint = {arXiv:1410.2597v2},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fithian, Sun, Taylor - 2014 - Optimal Inference After Model Selection(3).pdf:pdf},
pages = {1--36},
title = {{Optimal Inference After Model Selection}},
url = {http://arxiv.org/abs/1410.2597{\%}5Cnhttp://www.arxiv.org/pdf/1410.2597.pdf},
year = {2014}
}
@article{Faraway1997,
author = {Faraway, J.J.},
doi = {10.2307/1271130},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Faraway - 1997 - Regression analysis for a functional response(2).pdf:pdf},
issn = {00401706},
journal = {Technometrics},
keywords = {curve estimation,designed for the situ-,ergonomics,functional data analysis,functional regression analysis is,longitudinal data anal-,nonparametric regression,repeated measures,ysis},
number = {3},
pages = {254--261},
pmid = {220},
title = {{Regression analysis for a functional response}},
volume = {39},
year = {1997}
}
@article{Taylor2014,
abstract = {In this paper we propose new inference tools for forward stepwise and least angle regression. We first present a general scheme to perform valid inference after any selection event that can be characterized as the observation vector y falling into some polyhedral set. This framework then allows us to derive conditional (post-selection) hypothesis tests at any step of the forward stepwise and least angle regression procedures. We derive an exact null distribution for our proposed test statistics in finite samples, yielding p-values with exact type I error control. The tests can also be inverted to produce confidence intervals for appropriate underlying regression parameters. Application of this framework to general likelihood-based regression models (e.g., generalized linear models and the Cox model) is also discussed.},
archivePrefix = {arXiv},
arxivId = {1401.3889},
author = {Taylor, Jonathan and Lockhart, Richard and Tibshirani, Ryan J. and Tibshirani, Robert J},
eprint = {1401.3889},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Taylor et al. - 2014 - Exact Post-selection Inference for Forward Stepwise and Least Angle Regression(2).pdf:pdf},
journal = {ArXiv},
keywords = {confidence interval,cox model,forward stepwise regression,generalized linear model,lasso,least angle regression,p-value},
number = {September},
pages = {32},
title = {{Exact Post-selection Inference for Forward Stepwise and Least Angle Regression}},
volume = {1},
year = {2014}
}
@misc{Tong1989,
author = {Tong, Y. L.},
booktitle = {The Annals of Statistics},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Order statistics/Inequalities for a Class of Positively Dependent Random Variables with a Common Marginal.pdf:pdf},
issn = {2168-8966},
number = {1},
pages = {429--435},
title = {{Inequalities for a Class of Positively Dependent Random Variables with a Common Marginal}},
volume = {17},
year = {1989}
}
@article{Shen2001,
abstract = {In drug development, finding an optimal dose is normally carried out in a phase II trial. A phase III trial will then be conducted to demonstrate that the selected dose is efficacious and safe. As choosing a dose from the phase II trial which has the highest observed response rate could overestimate the true response rate of the selected dose, the data from the phase II study cannot be simply pooled with the data from the phase III study in a final analysis. Therefore, a solution to the overestimation problem needs to be found so that the information obtained from phase II dose finding clinical trials can appropriately be combined with the data in the phase III study. In this paper, the potential overestimation in a multiple dose clinical trial is assessed and a method for correcting this bias is proposed. Simulations show that stepwise over-correction, the proposed method, is better than methods such as Bonferroni's procedure.},
author = {Shen, Liji},
doi = {10.1002/sim.842},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shen - 2001 - An improved method of evaluating drug effect in a multiple dose clinical trial.pdf:pdf},
issn = {02776715},
journal = {Statistics in Medicine},
number = {13},
pages = {1913--1929},
pmid = {11427949},
title = {{An improved method of evaluating drug effect in a multiple dose clinical trial}},
volume = {20},
year = {2001}
}
@article{Rahimi2007,
abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. Our randomized features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms that use these features outperform state-of-the-art large-scale kernel machines.},
author = {Rahimi, Ali and Recht, Ben},
doi = {10.1.1.145.8736},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rahimi, Recht - 2007 - Random Features for Large-Scale Kernel Machines(2).pdf:pdf},
isbn = {160560352X},
issn = {0033-6599},
journal = {Advances in neural information {\ldots}},
keywords = {Kernels,Random Fourier Features},
mendeley-tags = {Kernels,Random Fourier Features},
number = {1},
pages = {1--8},
pmid = {4519940},
title = {{Random Features for Large-Scale Kernel Machines}},
year = {2007}
}
@article{GSell2015,
abstract = {We consider a multiple hypothesis testing setting where the hypotheses are ordered and one is only permitted to reject an initial contiguous block, H{\_}1,$\backslash$dots,H{\_}k, of hypotheses. A rejection rule in this setting amounts to a procedure for choosing the stopping point k. This setting is inspired by the sequential nature of many model selection problems, where choosing a stopping point or a model is equivalent to rejecting all hypotheses up to that point and none thereafter. We propose two new testing procedures, and prove that they control the false discovery rate in the ordered testing setting. We also show how the methods can be applied to model selection using recent results on p-values in sequential model selection settings.},
archivePrefix = {arXiv},
arxivId = {1309.5352},
author = {G'Sell, Max Grazier and Wager, Stefan and Chouldechova, Alexandra and Tibshirani, Robert J},
doi = {10.1111/rssb.12122},
eprint = {1309.5352},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/G'Sell et al. - 2015 - Sequential selection procedures and false discovery rate control(2).pdf:pdf},
issn = {1467-9868},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {False discovery rate,Multiple-hypothesis testing,Sequential testing,Stopping rule,false discovery rate,multiple hypothesis testing,sequential testing,stopping rule},
pages = {n/a--n/a},
title = {{Sequential selection procedures and false discovery rate control}},
year = {2015}
}
@book{Vander1998,
author = {van der Vaart, A.W.},
title = {{Asymptotic Statistics}},
year = {1998}
}
@article{OHara2009,
abstract = {The selection of variables in regression problems has occupied the minds of many statisticians. Several Bayesian variable selection methods have been developed, and we concentrate on the following methods: Kuo {\&} Mallick, Gibbs Variable Selection (GVS), Stochastic Search Variable Selection (SSVS), adaptive shrinkage with Je{\AE}reys' prior or a Laplacian prior, and reversible jump MCMC. We review these methods, in the context of their di{\AE}erent properties. We then implement the methods in BUGS, using both real and simulated data as examples, and investigate how the di{\AE}erent methods perform in practice. Our results suggest that SSVS, reversible jump MCMC and adaptive shrinkage methods can all work well, but the choice of which method is better will depend on the priors that are used, and also on how they are implemented.},
author = {O'Hara, R. B. and Sillanp{\"{a}}{\"{a}}, M. J.},
doi = {10.1214/09-BA403},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bayesian/Variable Selection/A Review of Bayesian Variable Selection Methods- What, How and Which.pdf:pdf},
isbn = {1936-0975},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {BUGS,MCMC,Variable selection},
number = {1},
pages = {85--118},
pmid = {273483200007},
title = {{A review of bayesian variable selection methods: What, how and which}},
volume = {4},
year = {2009}
}
@article{Bickel1981,
abstract = {A general weak convergence theory is developed for time-sequential censored rank statistics in the two-sample problem of comparing time to failure between two treatment groups, such as in the case of a clinical trial in which patients enter serially and, after being randomly allocated to one of two treatments, are followed until they fail or withdraw from the study or until the study is terminated. Applications of the theory to time-sequential tests based on these censored rank statistics are also discussed.},
author = {Bickel, Peter J and Freedman, David A},
doi = {10.1214/aos/1176342871},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Som eAsymptotic theory for the Bootstrap.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Asymptotics,Bootstrap},
mendeley-tags = {Asymptotics,Bootstrap},
number = {6},
pages = {1196--1217},
title = {{Some Asymptotic Theory for the Bootstrap}},
volume = {9},
year = {1981}
}
@article{Chumbley2009,
abstract = {In this note, we revisit earlier work on false discovery rate (FDR) and evaluate it in relation to topological inference in statistical parametric mapping. We note that controlling the false discovery rate of voxels is not equivalent to controlling the false discovery rate of activations. This is a problem that is unique to inference on images, in which the underlying signal is continuous (i.e., signal which does not have a compact support). In brief, inference based on conventional voxel-wise FDR procedures is not appropriate for inferences on the topological features of a statistical parametric map (SPM), such as peaks or regions of activation. We describe the nature of the problem, illustrate it with some examples and suggest a simple solution based on controlling the false discovery rate of connected excursion sets within an SPM, characterised by their volume. {\textcopyright} 2008 Elsevier Inc. All rights reserved.},
author = {Chumbley, Justin R. and Friston, Karl J.},
doi = {10.1016/j.neuroimage.2008.05.021},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chumbley, Friston - 2009 - False discovery rate revisited FDR and topological inference using Gaussian random fields.pdf:pdf},
isbn = {1095-9572 (Electronic)$\backslash$n1053-8119 (Linking)},
issn = {10538119},
journal = {NeuroImage},
number = {1},
pages = {62--70},
pmid = {18603449},
publisher = {Elsevier Inc.},
title = {{False discovery rate revisited: FDR and topological inference using Gaussian random fields}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2008.05.021},
volume = {44},
year = {2009}
}
@article{Flandin2016,
abstract = {This technical report revisits the analysis of family-wise error rates in statistical parametric mapping - using random field theory - reported in (Eklund et al., 2015). Contrary to the understandable spin that these sorts of analyses attract, a review of their results suggests that they endorse the use of parametric assumptions - and random field theory - in the analysis of functional neuroimaging data. We briefly rehearse the advantages parametric analyses offer over nonparametric alternatives and then unpack the implications of (Eklund et al., 2015) for parametric procedures.},
archivePrefix = {arXiv},
arxivId = {1606.08199},
author = {Flandin, Guillaume and Friston, Karl J.},
doi = {10.1073/pnas.1602413113},
eprint = {1606.08199},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Flandin, Friston - 2016 - Analysis of family-wise error rates in statistical parametric mapping using random field theory.pdf:pdf},
isbn = {0027-8424},
issn = {0027-8424},
keywords = {fMRI, functional magnetic resonance imaging, false,false positive error,false positive results,family-wise error rate,fmri,functional magnetic resonance imaging,random field,theory},
number = {June},
pages = {10--12},
pmid = {27357684},
title = {{Analysis of family-wise error rates in statistical parametric mapping using random field theory}},
url = {http://arxiv.org/abs/1606.08199},
volume = {11},
year = {2016}
}
@article{Politis1994,
abstract = {A general weak convergence theory is developed for time-sequential censored rank statistics in the two-sample problem of comparing time to failure between two treatment groups, such as in the case of a clinical trial in which patients enter serially and, after being randomly allocated to one of two treatments, are followed until they fail or withdraw from the study or until the study is terminated. Applications of the theory to time-sequential tests based on these censored rank statistics are also discussed.},
author = {{Gu, M; Lai}, T.L.},
doi = {10.1214/aos/1176342871},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Theory/Large Sample Confidence Regions Based on Subsamples under Minimal Assumptions.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Bootstrap,Subsampling},
mendeley-tags = {Bootstrap,Subsampling},
number = {4},
pages = {2031--2050},
title = {{Large Sample confidence Regions Based on Subsaples Under Minimal Assumptions}},
volume = {22},
year = {1994}
}
@article{Greene2010,
author = {Greene},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Asymptotic Statistics/LARGE-SAMPLE DISTRIBUTION Theory.pdf:pdf},
title = {{Multivariate CLT}},
year = {2010}
}
@article{Shaked1984,
author = {Tong, Y L},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Stochastic Orderings/STOCHASTIC ORDERING OF SPACINGS FROM DEPENDENT RANDOM VARIABLES.pdf:pdf},
journal = {Inequalities in Statistics and Probability},
pages = {141--149},
title = {{Stochastic Ordering of Spacings From Dependent Random Variables}},
volume = {5},
year = {1984}
}
@article{Balmer2008,
author = {Balmer, Prof Paul and Lewark, Lukas and Schieder, Simon and Schmidt, Andrin},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Balmer et al. - 2008 - Commutative Algebra(2).pdf:pdf},
title = {{Commutative Algebra}},
year = {2008}
}
@article{Janson2011,
abstract = {Some asymptotic notions for random variables are discussed. In particular, different versions of O and o for sequences of random variables are studied. The results are elementary and more or less well-known, but collected here for future use and easy reference.},
archivePrefix = {arXiv},
arxivId = {1108.3924},
author = {Janson, Svante},
eprint = {1108.3924},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Janson - 2011 - Probability asymptotics notes on notation(2).pdf:pdf},
keywords = {Fun,Landau notation},
number = {April},
pages = {1--10},
title = {{Probability asymptotics: notes on notation}},
volume = {math.PR},
year = {2011}
}
@article{Gareth2009,
abstract = {Regression models to relate a scalar {\$}Y{\$} to a functional predictor {\$}X(t){\$} are becoming increasingly common. Work in this area has concentrated on estimating a coefficient function, {\$}\backslashbeta(t){\$}, with {\$}Y{\$} related to {\$}X(t){\$} through {\$}\backslashint\backslashbeta(t)X(t) dt{\$}. Regions where {\$}\backslashbeta(t)\backslashne0{\$} correspond to places where there is a relationship between {\$}X(t){\$} and {\$}Y{\$}. Alternatively, points where {\$}\backslashbeta(t)=0{\$} indicate no relationship. Hence, for interpretation purposes, it is desirable for a regression procedure to be capable of producing estimates of {\$}\backslashbeta(t){\$} that are exactly zero over regions with no apparent relationship and have simple structures over the remaining regions. Unfortunately, most fitting procedures result in an estimate for {\$}\backslashbeta(t){\$} that is rarely exactly zero and has unnatural wiggles making the curve hard to interpret. In this article we introduce a new approach which uses variable selection ideas, applied to various derivatives of {\$}\backslashbeta(t){\$}, to produce estimates that are both interpretable, flexible and accurate. We call our method "Functional Linear Regression That's Interpretable" (FLiRTI) and demonstrate it on simulated and real-world data sets. In addition, non-asymptotic theoretical bounds on the estimation error are presented. The bounds provide strong theoretical motivation for our approach.},
archivePrefix = {arXiv},
arxivId = {0908.2918},
author = {James, Gareth M. and Wang, Jing and Zhu, Ji},
doi = {10.1214/08-AOS641},
eprint = {0908.2918},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/James, Wang, Zhu - 2009 - Functional linear regression that's interpretable(2).pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Dantzig selector,Functional linear regression,Interpretable regression,Lasso},
number = {5 A},
pages = {2083--2108},
title = {{Functional linear regression that's interpretable}},
volume = {37},
year = {2009}
}
@article{Ben-tal2001,
archivePrefix = {arXiv},
arxivId = {hep-th/9806199},
author = {Ben-tal, Aharon and Nemirovski, Arkadi},
doi = {10.1137/1.9780898718829},
eprint = {9806199},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ben-tal, Nemirovski - 2001 - Lectures on modern convex optimization Analysis, algorithms, and engineering applications(2).pdf:pdf},
isbn = {978-0898714913},
issn = {01406736},
journal = {Society for Industrial and Applied Mathematics},
keywords = {Ben-Tal,Nemirovski},
pages = {377--442},
primaryClass = {hep-th},
title = {{Lectures on modern convex optimization: Analysis, algorithms, and engineering applications}},
year = {2001}
}
@article{Hsing1988,
author = {Hsing, T. and H{\"{u}}sler, J. and Leadbetter, M. R.},
doi = {10.1007/BF00718038},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/EVT/Extremal Index - Stationary Dependent Extreme Value Theory/On the Exceedance Point Process.pdf:pdf},
issn = {01788051},
journal = {Probability Theory and Related Fields},
number = {1},
pages = {97--112},
title = {{On the exceedance point process for a stationary sequence}},
volume = {78},
year = {1988}
}
@article{Worsley1992,
abstract = {Many studies of brain function with positron emission tomography (PET) involve the interpretation of a subtracted PET image, usually the difference between two images under baseline and stimulation conditions. The purpose of these studies is to see which areas of the brain are activated by the stimulation condition. In many cognitive studies, the activation is so slight that the experiment must be repeated on several subjects and the subtracted images are averaged to improve the signal-to-noise ratio. The averaged image is then standardized to have unit variance and then searched for local maxima. The main problem facing investigators is which of these local maxima are statistically significant. We describe a simple method for determining an approximate p value for the global maximum based on the theory of Gaussian random fields. The p value is proportional to the volume searched divided by the product of the full widths at half-maximum of the image reconstruction process or number of resolution elements. Rather than working with local maxima, our method focuses on the Euler characteristic of the set of voxels with a value larger than a given threshold. The Euler characteristic depends only on the topology of the regions of high activation, irrespective of their shape. For large threshold values this is approximately the same as the number of isolated regions of activation above the threshold. We can thus not only determine if any activation has taken place, but we can also estimate how many isolated regions of activation are present.},
author = {Worsley, K. J. and Evans, A C and Marrett, S and Neelin, P},
doi = {10.1038/jcbfm.1992.127},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Worsley et al. - 1992 - A three-dimensional statistical analysis for CBF activation studies in human brain(2).pdf:pdf},
isbn = {0271-678X (Print)$\backslash$r0271-678X (Linking)},
issn = {0271-678X},
journal = {Journal of cerebral blood flow and metabolism.},
keywords = {Brain,Brain: blood supply,Brain: radionuclide imaging,Cerebrovascular Circulation,Cognition,Computer Simulation,Emission-Computed,Emission-Computed: methods,Humans,Pain,Pain: radionuclide imaging,RFT,Statistics as Topic,Tomography},
mendeley-tags = {RFT},
number = {6},
pages = {900--18},
pmid = {1400644},
title = {{A three-dimensional statistical analysis for CBF activation studies in human brain.}},
volume = {12},
year = {1992}
}
@article{Bentkus2007,
author = {Bentkus, Vidmantas and Jing, Bing-Yi and Shao, Qi-Man and Zhou, Wang},
doi = {10.3150/07-BEJ5073},
file = {:homes/davenpor/global/PeakInference/Peak Papers/Limiting distributions of the non-central
t-statistic and their applications to the
power of t-tests under non-normality.pdf:pdf},
issn = {1350-7265},
journal = {Bernoulli},
keywords = {domain of attraction,limit theorems,non-central t -statistic,power of t -test},
number = {2},
pages = {346--364},
title = {{Limiting distributions of the non-central t -statistic and their applications to the power of t -tests under non-normality}},
volume = {13},
year = {2007}
}
@article{Newey1994,
abstract = {Asymptotic distribution theory is the primary method used to examine the properties of econometric estimators and tests. We present conditions for obtaining cosistency and asymptotic normality of a very general class of estimators (extremum estimators). Consistent asymptotic variance estimators are given to enable approximation of the asymptotic distribution. Asymptotic efficiency is another desirable property then considered. Throughout the chapter, the general results are also specialized to common econometric estimators (e.g. MLE and GMM), and in specific examples we work through the conditions for the various results in detail. The results are also extended to two-step estimators (with finite-dimensional parameter estimation in the first step), estimators derived from nonsmooth objective functions, and semiparametric two-step estimators (with nonparametric estimation of an infinite-dimensional parameter in the first step). Finally, the trinity of test statistics is considered within the quite general setting of GMM estimation, and numerous examples are given. ?? 1994 Elsevier Science B.V. All rights reserved.},
author = {Newey, Whitney K. and McFadden, Daniel},
doi = {10.1016/S1573-4412(05)80005-4},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Newey, McFadden - 1994 - Chapter 36 Large sample estimation and hypothesis testing(2).pdf:pdf},
isbn = {9780444887665},
issn = {15734412},
journal = {Handbook of Econometrics},
keywords = {Semiparametrics},
mendeley-tags = {Semiparametrics},
pages = {2111--2245},
title = {{Chapter 36 Large sample estimation and hypothesis testing}},
volume = {4},
year = {1994}
}
@article{Ramdas2017,
abstract = {We propose a top-down algorithm for multiple testing on directed acyclic graphs (DAGs), where nodes represent hypotheses and edges specify a partial ordering in which hypotheses must be tested. The procedure is guaranteed to reject a sub-DAG with bounded false discovery rate (FDR) while satisfying the logical constraint that a rejected node's parents must also be rejected. It is designed for sequential testing settings, when the DAG structure is known a priori, but the p-values are obtained selectively (such as sequential conduction of experiments), but the algorithm is also applicable in non-sequential settings when all p-values can be calculated in advance (such as variable/model selection). Our DAGGER algorithm, shorthand for Greedily Evolving Rejections on DAGs, allows for independence, positive or arbitrary dependence of the p-values, and is guaranteed to work on two different types of DAGs: (a) intersection DAGs in which all nodes are intersection hypotheses, with parents being supersets of children, or (b) general DAGs in which all nodes may be elementary hypotheses. The DAGGER procedure has the appealing property that it specializes to known algorithms in the special cases of trees and line graphs, and simplifies to the classic Benjamini-Hochberg procedure when the DAG has no edges. We explore the empirical performance of DAGGER using simulations, as well as a real dataset corresponding to a gene ontology DAG, showing that it performs favorably in terms of time and power.},
archivePrefix = {arXiv},
arxivId = {1709.10250},
author = {Ramdas, Aaditya and Chen, Jianbo and Wainwright, Martin J. and Jordan, Michael I.},
eprint = {1709.10250},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Multiple Testing/Sequences and Trees/DAGGER $\backslash$: A sequential algorithm for FDR control on DAGs.pdf:pdf},
keywords = {directed acyclic graph,false discovery rate,multiple testing,partially ordered hypotheses},
pages = {1--24},
title = {{DAGGER: A sequential algorithm for FDR control on DAGs}},
url = {http://arxiv.org/abs/1709.10250},
year = {2017}
}
@article{Efron2008,
abstract = {The classic frequentist theory of hypothesis testing developed by Neyman, Pearson and Fisher has a claim to being the twentieth century's most influential piece of applied mathematics. Something new is happening in the twenty-first century: high-throughput devices, such as microarrays, routinely require simultaneous hypothesis tests for thousands of individual cases, not at all what the classical theory had in mind. In these situations empirical Bayes information begins to force itself upon frequentists and Bayesians alike. The two-groups model is a simple Bayesian construction that facilitates empirical Bayes analysis. This article concerns the interplay of Bayesian and frequentist ideas in the two-groups setting, with particular attention focused on Benjamini and Hochberg's False Discovery Rate method. Topics include the choice and meaning of the null hypothesis in large-scale testing situations, power considerations, the limitations of permutation methods, significance testing for groups of cases (such as pathways in microarray studies), correlation effects, multiple confidence intervals and Bayesian competitors to the two-groups model. CR - Copyright {\&}{\#}169; 2008 Institute of Mathematical Statistics},
archivePrefix = {arXiv},
arxivId = {0808.0603},
author = {Efron, Bradley},
doi = {10.1214/07-STS236},
eprint = {0808.0603},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Empirical Bayes/Microarrays, Empirical Bayes and the.pdf:pdf},
isbn = {0883-4237},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Simultaneous tests,and phrases,empirical null,false discover},
number = {1},
pages = {1--22},
title = {{Microarrays, Empirical Bayes and the Two-Groups Model}},
volume = {23},
year = {2008}
}
@article{Knijnenburg2009,
abstract = {MOTIVATION Permutation tests have become a standard tool to assess the statistical significance of an event under investigation. The statistical significance, as expressed in a P-value, is calculated as the fraction of permutation values that are at least as extreme as the original statistic, which was derived from non-permuted data. This empirical method directly couples both the minimal obtainable P-value and the resolution of the P-value to the number of permutations. Thereby, it imposes upon itself the need for a very large number of permutations when small P-values are to be accurately estimated. This is computationally expensive and often infeasible. RESULTS A method of computing P-values based on tail approximation is presented. The tail of the distribution of permutation values is approximated by a generalized Pareto distribution. A good fit and thus accurate P-value estimates can be obtained with a drastically reduced number of permutations when compared with the standard empirical way of computing P-values. AVAILABILITY The Matlab code can be obtained from the corresponding author on request. SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.},
author = {Knijnenburg, Theo A. and Wessels, Lodewyk F A and Reinders, Marcel J T and Shmulevich, Ilya},
doi = {10.1093/bioinformatics/btp211},
file = {:homes/davenpor/global/TomsMiniProject/This Week/Fewer permutations, more accurate P-values.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$n1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
keywords = {extreme value theory},
mendeley-tags = {extreme value theory},
number = {12},
pages = {161--168},
pmid = {19477983},
title = {{Fewer permutations, more accurate P-values}},
volume = {25},
year = {2009}
}
@article{Wainwright2008,
abstract = {capital * Public equity and IPO's * -owned firms theory has also devel- oped that come really could draw all possible from existing},
author = {Wainwright, M},
doi = {10.1561/2200000001},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wainwright - 2008 - Graphical Models, Exponential Families, and Variational Inference(2).pdf:pdf},
issn = {1935-8237},
pages = {1--305},
title = {{Graphical Models, Exponential Families, and Variational Inference.}},
volume = {1},
year = {2008}
}
@article{Zelterman1993,
author = {Zelterman, Daniel},
doi = {10.1080/01621459.1993.10476298},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Resampling/Order Statistics/A Semiparametric Bootstrap Technique for Simulating Extreme Order Statistics.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Exponential distribution,Extreme value distribution,Gini statistic,Spacings},
number = {422},
pages = {477--485},
title = {{A semiparametric bootstrap technique for simulating extreme order statistics}},
volume = {88},
year = {1993}
}
@book{Gelman1995,
author = {Gelman, Andrew and Carlin, J and Stern, H and Rubin, Donald B.},
title = {{Bayesian Data Analysis}},
year = {1995}
}
@article{Spodarev2009,
author = {Spodarev, Evgeny},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Spodarev - 2009 - Random Fields I.pdf:pdf},
title = {{Random Fields I}},
year = {2009}
}
@article{Stein1962,
	title={Confidence sets for the mean of a multivariate normal distribution},
	author={Stein, Charles M},
	journal={Journal of the Royal Statistical Society: Series B (Methodological)},
	volume={24},
	number={2},
	pages={265--285},
	year={1962},
	publisher={Wiley Online Library}
}
@article{Fraley1998,
author = {Frayley, Chris},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Scott - 2014 - Downloaded 12 01 16 to 192 . 33 . 105 . 150 . Redistribution subject to SIAM license or copyright see httpwww.siam.org(2).pdf:pdf},
keywords = {incomplete cholesky factorization,iterative,positive-definite symmetric systems,preconditioning,solvers,sparse linear systems,sparse matrices},
number = {1},
pages = {270--281},
title = {{Algorithms for Model-Based Gaussian Hierarchical Clustering}},
volume = {20},
year = {1998}
}
@article{Datta2016,
abstract = {IParticulate matter (PM) is a class of malicious environmental pol- lutants known to be detrimental to human health. Regulatory efforts aimed at curbing PM levels in different countries often require high resolution space-time maps that can identify red-flag regions exceed- ing statutory concentration limits. Continuous spatio-temporal Gaus- sian Process (GP) models can deliver maps depicting predicted PM levels and quantify predictive uncertainty. However, GP based ap- proaches are usually thwarted by computational challenges posed by large datasets. We construct a novel class of scalable Dynamic Near- est Neighbor Gaussian Process (DNNGP) models that can provide a sparse approximation to any spatio-temporal GP (e.g., with non- separable covariance structures). TheDNNGP we develop here can be used as a sparsity-inducing prior for spatio-temporal random effects in any Bayesian hierarchical model to deliver full posterior inference. Storage and memory requirements for a DNNGP model are linear in the size of the dataset thereby delivering massive scalability with- out sacrificing inferential richness. Extensive numerical studies reveal that the DNNGP provides substantially superior approximations to the underlying process than low rank approximations. Finally, we use the DNNGP to analyze a massive air quality dataset to substantially improve predictions of PM levels across Europe in conjunction with the LOTOS-EUROS chemistry transport models (CTMs).},
archivePrefix = {arXiv},
arxivId = {0000.0000},
author = {Datta, Abhirup and Banerjee, Sudipto and Finley, Andrew O. and Hamm, Nicholas A S and Schaap, Martijn},
doi = {10.1214/16-AOAS931},
eprint = {0000.0000},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Datta et al. - 2016 - Nonseparable dynamic nearest neighbor Gaussian process models for large spatio-temporal data with an applicatio(2).pdf:pdf},
issn = {19417330},
journal = {Annals of Applied Statistics},
keywords = {Bayesian inference,Environmental pollutants,Markov chain Monte Carlo,Nearest neighbors,Nonseparable spatio-temporal models,Scalable Gaussian process},
number = {3},
pages = {1286--1316},
title = {{Nonseparable dynamic nearest neighbor Gaussian process models for large spatio-temporal data with an application to particulate matter analysis}},
volume = {10},
year = {2016}
}
@article{Beran1988,
author = {Beran, Rudolf},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Prepivoting Test Statistics$\backslash$: A Bootstrap View of Asymptotic Refinements.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {asymptotic},
number = {403},
pages = {687--697},
title = {{Prepivoting a Bootstrap view of test statistics: asymptotic refinements}},
volume = {83},
year = {1988}
}
@article{Wilson2016,
abstract = {We present a set of computing tools and techniques that every researcher can and should adopt. These recommendations synthesize inspiration from our own work, from the experiences of the thousands of people who have taken part in Software Carpentry and Data Carpentry workshops over the past six years, and from a variety of other guides. Unlike some other guides, our recommendations are aimed specifically at people who are new to research computing.},
archivePrefix = {arXiv},
arxivId = {1609.00037},
author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
doi = {10.1371/journal.pcbi.1005510},
eprint = {1609.00037},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Coding/Good enough practices in scientific computing.pdf:pdf},
isbn = {1111111111},
issn = {1553-7358},
journal = {Computational Biology},
pages = {1--20},
pmid = {28640806},
title = {{Good Enough Practices in Scientific Computing}},
year = {2016}
}
@article{Moran1970,
author = {Moran, P.},
file = {:data/fireback/davenpor/davenpor/Referreeing/Testing One Hypothesis Multiple Times- The Multidimensional Case/MArk2/References/Moran 1970.pdf:pdf},
number = {1},
pages = {47--55},
title = {{Biometrika Trust On Asymptotically Optimal Tests of Composite Hypotheses}},
volume = {57},
year = {2018}
}
@article{Romano2005,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. Consider the problem of testing k hypotheses simultaneously. In this article we discuss finite-and large-sample theory of stepdown methods that provide control of the familywise error rate (FWE). To improve on the Bonferroni method or on Holm's stepdown method, Westfall and Young made effective use of resampling to construct stepdown methods that implicitly estimate the dependence structure of the test statistics. However, their methods depend on an assumption known as "subset pivotality." Our goal here is to construct general stepdown methods that do not require such an assumption. To accomplish this, we take a close look at what makes stepdown procedures work; a key component is a monotonicity requirement of critical values. By imposing monotonicity on estimated critical values (which is not an assumption on the model but rather is an assumption on the method), we show how to construct stepdown tests that can be applied in a stagewise fashion so that at most k tests need to be computed. Moreover, at each stage, an intersection test that controls the usual probability of a type 1 error is calculated, which allows us to draw on an enormous resampling literature as a general means of test construction. In addition, it is possible to carry out this method using the same set of resamples (or subsamples) for each of the intersection tests.},
author = {Romano, Joseph P and Wolf, Michael},
doi = {10.1198/016214504000000539},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Romano, Wolf - 2005 - Exact and Approximate Stepdown Methods for Multiple Hypothesis Testing.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {bootstrap,familywise error rate,multiple testing,permutation test,randomization test,stepdown procedure},
number = {469},
pages = {94--108},
title = {{Exact and Approximate Stepdown Methods for Multiple Hypothesis Testing}},
volume = {100},
year = {2005}
}
@book{Diggle2002,
abstract = {The first edition of Analysis for Longitudinal Data has become a classic. Describing the statistical models and methods for the analysis of longitudinal data, it covers both the underlying statistical theory of each method, and its application to a range of examples from the agricultural and biomedical sciences. The main topics discussed are design issues, exploratory methods of analysis, linear models for continuous data, general linear models for discrete data, and models and methods for handling data and missing values. Under each heading, worked examples are presented in parallel with the methodological development, and sufficient detail is given to enable the reader to reproduce the author's results using the data-sets as an appendix. This new edition of Analysis for Longitudinal Data provides a thorough and expanded revision of this important text. It includes two new chapters; the first discusses fully parametric models for discrete repeated measures data, and the second explores statistical models for time-dependent predictors.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Diggle, Peter J. and Heagerty, Patrick and Liang, Kung-Yee and Zeger, Scott L.},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Diggle et al. - 2002 - Analysis of Longitudinal Data.pdf:pdf},
isbn = {978–0–19–852484–7},
issn = {1098-6596},
pages = {379},
pmid = {25246403},
title = {{Analysis of Longitudinal Data}},
year = {2002}
}
@article{Tibishrani20165,
abstract = {Recently, Tibshirani et al. (2016) proposed a method for making inferences about parameters defined by model selection, in a typical regression setting with normally distributed errors. Here, we study the large sample properties of this method, without assuming normality. We prove that the test statistic of Tibshirani et al. (2016) is asymptotically valid, as the number of samples n grows and the dimension d of the regression problem stays fixed. Our asymptotic result holds uniformly over a wide class of nonnormal error distributions. We also propose an efficient bootstrap version of this test that is provably (asymptotically) conservative, and in practice, often delivers shorter intervals than those from the original normality-based approach. Finally, we prove that the test statistic of Tibshirani et al. (2016) does not enjoy uniform validity in a high-dimensional setting, when the dimension d is allowed grow.},
archivePrefix = {arXiv},
arxivId = {1506.06266},
author = {Tibshirani, Ryan J. and Rinaldo, Alessandro and Tibshirani, Robert and Wasserman, Larry},
eprint = {1506.06266},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/PMS/Uniform Asymptotic Inference and the Bootstrap After Model Selection.pdf:pdf},
number = {1},
title = {{Uniform Asymptotic Inference and the Bootstrap After Model Selection}},
year = {2015}
}
@article{Chen2012,
author = {Chen, Kehui and Muller, Hans-Georg},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/FunctionalDataAnlaysis/Conditional quantile analysis when covariates are functions, with application to growth.pdf:pdf},
number = {1},
pages = {67--89},
title = {{Conditional quantile analysis when covariates are functions, with application to growth data}},
volume = {74},
year = {2012}
}
@article{Muller2005,
abstract = {We propose a generalized functional linear regression model for a regression situation where the response variable is a scalar and the predictor is a random function. A linear predictor is obtained by forming the scalar product of the predictor function with a smooth parameter function, and the expected value of the response is related to this linear predictor via a link function. If, in addition, a variance function is specified, this leads to a functional estimating equation which corresponds to maximizing a functional quasi-likelihood. This general approach includes the special cases of the functional linear model, as well as functional Poisson regression and functional binomial regression. The latter leads to procedures for classification and discrimination of stochastic processes and functional data. We also consider the situation where the link and variance functions are unknown and are estimated nonparametrically from the data, using a semiparametric quasi-likelihood procedure. An essential step in our proposal is dimension reduction by approximating the predictor processes with a truncated Karhunen-Loeve expansion. We develop asymptotic inference for the proposed class of generalized regression models. In the proposed asymptotic approach, the truncation parameter increases with sample size, and a martingale central limit theorem is applied to establish the resulting increasing dimension asymptotics. We establish asymptotic normality for a properly scaled distance between estimated and true functions that corresponds to a suitable L{\^{}}2 metric and is defined through a generalized covariance operator.},
archivePrefix = {arXiv},
arxivId = {math/0505638},
author = {M{\"{u}}ller, Hans Georg and Stadtm{\"{u}}ller, Ulrich},
doi = {10.1214/009053604000001156},
eprint = {0505638},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/FunctionalDataAnlaysis/GENERALIZED FUNCTIONAL LINEAR MODELS1.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Classification of stochastic processes,Covariance operator,Eigenfunctions,Functional regression,Generalized linear model,Increasing dimension asymptotics,Karhunen-Lo{\`{e}}ve expansion,Martingale central limit theorem,Order selection},
number = {2},
pages = {774--805},
primaryClass = {math},
title = {{Generalized functional linear models}},
volume = {33},
year = {2005}
}
@article{Hasofer1976,
author = {Hasofer, A. M.},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hasofer - 1976 - The Mean Number of Maxima above High Levels in Gaussian Random Fields.pdf:pdf},
pages = {377--379},
title = {{The Mean Number of Maxima above High Levels in Gaussian Random Fields}},
volume = {13},
year = {1976}
}
@article{Cohen1989,
author = {Cohen, Arthur and Sackrowitz, Harold B},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohen, Sackrowitz - 1989 - Two Stage Conditionally Unbiased Estimators of the Selected Mean.pdf:pdf},
pages = {273--278},
title = {{Two Stage Conditionally Unbiased Estimators of the Selected Mean}},
volume = {8},
year = {1989}
}
@article{Benjamini2000,
abstract = {A new approach to problems of multiple significance testing was presented in Benjamini and Hochberg (1995), which calls for controlling the expected ratio of the number of erroneous rejections to the number of rejections-the "False Discovery Rate" (FDR). The procedure given there was shown to control the FDR for independent test statistics. When some of the hypotheses are in fact false, that procedure is too conservative. We present here an adaptive procedure, where the number of true null hypotheses is estimated first as in Hochberg and Benjamini (1990), and this estimate is used in the procedure of Benjamini and Hochberg (1995). The result is still a simple stepwise procedure, to which we also give a graphical companion. The new procedure is used in several examples drawn from educational and behavioral studies, addressing problems in multi-center studies, subset analysis and meta-analysis. The examples vary in the number of hypotheses tested, and the implication of the new procedure on the conclusions. In a large simulation study of independent test statistics the adaptive procedure is shown to control the FDR and have substantially better power than the previously suggested FDR controlling method, which by itself is more powerful than the traditional familywise error-rate controlling methods. In cases where most of the tested hypotheses are far from being true there is hardly any penalty due to the simultaneous testing of many hypotheses.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Benjamini, Y. and Hochberg, Y.},
doi = {10.3102/10769986025001060},
eprint = {arXiv:1011.1669v3},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Multiple Testing/Adaptive FDR control/On the adaptive control of the false discovery rate in multiple testing with independent statistics.pdf:pdf},
isbn = {10769986},
issn = {1076-9986},
journal = {Journal of Educational and Behavioral Statistics},
keywords = {1995,a new approach to,benjamini and hochberg,bonferroni type procedures,meta-analysis,multi-center,multiple comparison,p-values,problems of multiple significance,procedures,stepwise procedures,subset analysis,testing was presented in,the expected ratio,which calls for controlling},
number = {1},
pages = {60--83},
pmid = {443},
title = {{On the Adaptive Control of the False Discovery Rate in Multiple Testing With Independent Statistics}},
volume = {25},
year = {2000}
}
@article{Kennedy2015,
abstract = {In this paper we review important aspects of semiparametric theory and empirical processes that arise in causal inference problems. We begin with a brief introduction to the general problem of causal inference, and go on to discuss estimation and inference for causal effects under semiparametric models, which allow parts of the data-generating process to be unrestricted if they are not of particular interest (i.e., nuisance functions). These models are very useful in causal problems because the outcome process is often complex and difficult to model, and there may only be information available about the treatment process (at best). Semiparametric theory gives a framework for benchmarking efficiency and constructing estimators in such settings. In the second part of the paper we discuss empirical process theory, which provides powerful tools for understanding the asymptotic behavior of semiparametric estimators that depend on flexible nonparametric estimators of nuisance functions. These tools are crucial for incorporating machine learning and other modern methods into causal inference analyses. We conclude by examining related extensions and future directions for work in semiparametric causal inference.},
archivePrefix = {arXiv},
arxivId = {1510.04740},
author = {Kennedy, Edward H.},
eprint = {1510.04740},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kennedy - 2015 - Semiparametric Theory and Empirical Processes in Causal Inference(2).pdf:pdf},
keywords = {Semiparametrics,chine learning,donsker class,efficient influence function,estimating equation,ma-,nonparametric theory},
mendeley-tags = {Semiparametrics},
pages = {1--26},
title = {{Semiparametric Theory and Empirical Processes in Causal Inference}},
year = {2015}
}
@article{Dunnett1955,
author = {Dunnett, Charles},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dunnett - 1955 - A Multiple Comparison Procedure for Comparing Several Treatments.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {272},
pages = {1096--1121},
title = {{A Multiple Comparison Procedure for Comparing Several Treatments}},
volume = {50},
year = {1955}
}
@article{Berk2013,
	title={Valid post-selection inference},
	author={Berk, Richard and Brown, Lawrence and Buja, Andreas and Zhang, Kai and Zhao, Linda and others},
	journal={The Annals of Statistics},
	volume={41},
	number={2},
	pages={802--837},
	year={2013},
	publisher={Institute of Mathematical Statistics}
}
@book{Book:RandomFieldsandGeometry,
author = {Adler, Robert and Taylor, Jonathan},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Adler, Taylor - 2007 - Random Fields and Geometry(2).pdf:pdf},
isbn = {9780387481128},
title = {{Random Fields and Geometry}},
year = {2007}
}
@book{Fallat2017,
abstract = {We discuss properties of distributions that are multivariate totally positive of order two (MTP2) related to conditional independence. In particular, we show that any independence model generated by an MTP2 distribution is a compositional semigraphoid which is upward-stable and singleton-transitive. In addition, we prove that any MTP2 distribution satisfying an appropriate support condition is faithful to its concentration graph. Finally, we analyze factorization properties of MTP2 distributions and discuss ways of constructing MTP2 distributions; in particular we give conditions on the log-linear parameters of a discrete distribution which ensure MTP2 and characterize conditional Gaussian distributions which satisfy MTP2.},
archivePrefix = {arXiv},
arxivId = {1510.01290},
author = {Fallat, Shaun and Lauritzen, Steffen and Sadeghi, Kayvan and Uhler, Caroline and Wermuth, Nanny and Zwiernik, Piotr},
booktitle = {Annals of Statistics},
doi = {10.1214/16-AOS1478},
eprint = {1510.01290},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/MTP2/TOTAL POSITIVITY IN MARKOV STRUCTURES.pdf:pdf},
isbn = {2011300975},
issn = {00905364},
keywords = {Association,Concentration graph,Conditional Gaussian distribution,Faithfulness,Graphical models,Log-linear interactions,Markov property,Positive dependence},
number = {3},
pages = {1152--1184},
title = {{Total positivity in Markov structures}},
volume = {45},
year = {2017}
}
@article{Dudoit2003,
author = {Sandrine, Dudoit and {Mark J. van der}, Laan and Pollard, Katherine S.},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sandrine, Mark J. van der, Pollard - 2003 - Multiple Testing. Part I. Single-Step Procedures for Control of General Type I Error Rates.pdf:pdf},
title = {{Multiple Testing. Part I. Single-Step Procedures for Control of General Type I Error Rates}},
year = {2003}
}
@article{Dudoit2002,
abstract = {A reliable and precise classification of tumors is essential for successful diagnosis and treatment of cancer. cDNA microarrays and high-density oligonucleotide chips are novel biotechnologies increasingly used in cancer research. By allowing the monitoring of expression levels in cells for thousands of genes simultaneously, microarray experiments may lead to a more complete understanding of the molecular variations among tumors and hence to a finer and more informative classification. The ability to successfully distinguish between tumor classes (already known or yet to be discovered) using gene expression data is an important aspect of this novel approach to cancer classification. This article compares the performance of different discrimination methods for the classification of tumors based on gene expression data. The methods include nearest-neighbor classifiers, linear discriminant analysis, and classification trees. Recent machine learning approaches, such as bagging and boosting, are also considered. The discrimination methods are applied to datasets from three recently published cancer gene expression studies},
author = {Dudoit, Sandrine and Fridlyand, Jane and Speed, Terence P},
doi = {10.1198/016214502753479248},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dudoit, Fridlyand, Speed - 2002 - Comparison of Discrimination Methods for the Classification of Tumors Using Gene Expression Data.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {cancer,discriminant analysis,microarray experiment,supervised learning,tumor classi cation,variable selection},
number = {457},
pages = {77--87},
title = {{Comparison of Discrimination Methods for the Classification of Tumors Using Gene Expression Data}},
volume = {97},
year = {2002}
}
@article{Westfall2009,
author = {Westfall, Peter H.},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Westfall - 2009 - Simultaneous Small-Sample Multivariate Bernoulli Confidence Intervals.pdf:pdf},
journal = {Biometrics},
number = {4},
pages = {1001--1013},
title = {{Simultaneous Small-Sample Multivariate Bernoulli Confidence Intervals}},
volume = {41},
year = {2009}
}
@book{Tobergte2013a,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tobergte, David R. and Curtis, Shirley},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tobergte, Curtis - 2013 - book Real analysis and probability(2).pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{[book] Real analysis and probability}},
volume = {53},
year = {2013}
}
@article{Hastie2009,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting-the first comprehensive treatment of this topic in any book. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit. FROM THE REVIEWS: TECHNOMETRICS "This is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."},
archivePrefix = {arXiv},
arxivId = {1010.3003},
author = {Hastie, Trevor and Tibshirani, Robert J and Friedman, Jerome},
doi = {10.1007/b94608},
eprint = {1010.3003},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hastie, Tibshirani, Friedman - 2009 - The Elements of Statistical Learning(2).pdf:pdf},
isbn = {9780387848570},
issn = {03436993},
journal = {Elements},
pages = {337--387},
pmid = {15512507},
title = {{The Elements of Statistical Learning}},
volume = {1},
year = {2009}
}
@article{Troendle2004,
author = {Troendle, J F and Korn, Edward L. and McShane, Lisa M.},
file = {:homes/davenpor/global/TomsMiniProject/This Week/An Example of Slow Convergence of the Bootstrap in High Dimensions.pdf:pdf},
journal = {The American Statistician},
keywords = {amount of fluorescence at,amount of labeled sample,ate data,each probe,hypothesis test,interrogating different genes,microarray data,multivari,permutation test,probe can be quan,resampling,small sample behavior,that binds to each,the,thousands of probe sequences,tified by measuring the},
number = {1},
pages = {25--29},
title = {{An Example of Slow Convergence of the Bootstrap in High Dimensions}},
volume = {58},
year = {2004}
}
@article{Chen2018a,
abstract = {Mediation analysis has become an important tool in the behavioral sciences for investigating the role of intermediate variables that lie in the path between a randomized treatment and an outcome variable. The influence of the intermediate variable on the outcome is often explored using structural equation models (SEMs), with model coefficients interpreted as possible effects. While there has been significant research on the topic in recent years, little work has been done on mediation analysis when the intermediate variable (mediator) is a high-dimensional vector. In this work we present a new method for exploratory mediation analysis in this setting called the directions of mediation (DMs). The first DM is defined as the linear combination of the elements of a high-dimensional vector of potential mediators that maximizes the likelihood of the SEM. The subsequent DMs are defined as linear combinations of the elements of the high-dimensional vector that are orthonormal to the previous DMs and maximize the likelihood of the SEM. We provide an estimation algorithm and establish the asymptotic properties of the obtained estimators. This method is well suited for cases when many potential mediators are measured. Examples of high-dimensional potential mediators are brain images composed of hundreds of thousands of voxels, genetic variation measured at millions of SNPs, or vectors of thousands of variables in large-scale epidemiological studies. We demonstrate the method using a functional magnetic resonance imaging (fMRI) study of thermal pain where we are interested in determining which brain locations mediate the relationship between the application of a thermal stimulus and self-reported pain.},
archivePrefix = {arXiv},
arxivId = {1511.09354},
author = {Ch{\'{e}}n, Oliver Y. and Crainiceanu, Ciprian and Ogburn, Elizabeth L. and Caffo, Brian S. and Wager, Tor D. and Lindquist, Martin A.},
doi = {10.1093/biostatistics/kxx027},
eprint = {1511.09354},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/Causal Modelling/High-dimensional Multivariate Mediation with Application to Neuroimaging Data.pdf:pdf},
isbn = {1468-4357 (Electronic)
1465-4644 (Linking)},
issn = {14684357},
journal = {Biostatistics},
keywords = {Directions of mediation,High-dimensional data,Principal components analysis,Structural equation models,fMRI, Mediation analysis},
number = {2},
pages = {121--136},
pmid = {28637279},
title = {{High-dimensional multivariate mediation with application to neuroimaging data}},
volume = {19},
year = {2018}
}
@article{Horowitz1989,
abstract = {The bootstrap is a method for estimating the distribution of an estimator or test statistic by resampling one's data or a model estimated from the data. Under conditions that hold in a wide variety of econometric applications, the bootstrap provides approximations to distributions of statistics, coverage probabilities of confidence intervals, and rejection probabilities of hypothesis tests that are more accurate than the approximations of first-order asymptotic distribution theory. The reductions in the differences between true and nominal coverage or rejection probabilities can be very large. The bootstrap is a practical technique that is ready for use in applications. This chapter explains and illustrates the usefulness and limitations of the bootstrap in contexts of interest in econometrics. The chapter outlines the theory of the bootstrap, provides numerical illustrations of its performance, and gives simple instructions on how to implement the bootstrap in applications. The presentation is informal and expository. Its aim is to provide an intuitive understanding of how the bootstrap works and a feeling for its practical value in econometrics.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Horowitz, Joel},
doi = {10.1016/0169-2070(89)90078-2},
eprint = {arXiv:1011.1669v3},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Theory/Horowitz the boostrap.pdf:pdf},
isbn = {9780444823403},
issn = {01692070},
journal = {Handbook of Econometrics},
number = {1},
pages = {143--144},
pmid = {22023681},
title = {{Handbook of econometrics}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0169207089900782},
volume = {5},
year = {1989}
}
@book{Epstein2010,
archivePrefix = {arXiv},
arxivId = {arXiv:1006.5440v1},
author = {Eppstein, David and Loffler, Maarten and Strash, Darren},
eprint = {arXiv:1006.5440v1},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Graphical Models/Listing All Maximal Cliques in Sparse Graphs in.pdf:pdf},
pages = {403--414},
title = {{Algorithms and Computation: Listing All Maximal Cliques in Sparse Graphs in Near-Optimal Time}},
year = {2010}
}
@article{Cam1986,
abstract = {This paper addresses the problem of aggregating a number of expert opinions which have been expressed in some numerical form in order to reflect individual uncertainty vis-a-vis a quantity of interest. The primary focus is consensus belief formation and expert use, although some relevant aspects of group decision making are also reviewed. A taxonomy of solutions is presented which serves as the framework for a survey of recent theoretical developments in the area. A number of current research directions are mentioned and an extensive, current annotated bibliography is included.},
author = {Cam, L. Le},
doi = {10.1214/ss/1177013818},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cam - 1986 - The Central Limit Theorem Around 1935(2).pdf:pdf},
isbn = {0412343908},
issn = {0883-4237},
journal = {Statistical Science},
number = {1},
pages = {78--91},
pmid = {20948974},
title = {{The Central Limit Theorem Around 1935}},
volume = {1},
year = {1986}
}
@article{Rosenblatt2017,
abstract = {The most prevalent approach to activation localization in neuroimaging is to identify brain regions as contiguous supra-threshold clusters, check their significance using random field theory, and correct for the multiple clusters being tested. Besides recent criticism on the validity of the random field assumption, a spatial specificity paradox remains: the larger the detected cluster, the less we know about the location of activation within that cluster. This is because cluster inference implies "there exists at least one voxel with an evoked response in the cluster", and not that "all the voxels in the cluster have an evoked response". Inference on voxels within selected clusters is considered bad practice, due to the voxel-wise false positive rate inflation associated with this circular inference. Here, we propose a remedy to the spatial specificity paradox. By applying recent results from the multiple testing statistical literature, we are able to quantify the proportion of truly active voxels within selected clusters, an approach we call All-Resolutions Inference (ARI). If this proportion is high, the paradox vanishes. If it is low, we can further "drill down" from the cluster level to sub-regions, and even to individual voxels, in order to pinpoint the origin of the activation. In fact, ARI allows inference on the proportion of activation in all voxel sets, no matter how large or small, however these have been selected, all from the same data. We use two fMRI datasets to demonstrate the non-triviality of the spatial specificity paradox, and its resolution using ARI. One of these datasets is large enough for us to split it and validate the ARI estimates. The conservatism of ARI inference permits circularity without losing error guarantees, while still returning informative estimates.},
author = {Rosenblatt, Jonathan D. and Finos, Livio and Weeda, Wouter D. and Solari, Aldo and Goeman, Jelle J.},
doi = {10.1101/226126},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/Cluster inference/All-Resolutions Inference for Brain Imaging.pdf:pdf},
journal = {bioRxiv},
title = {{All-Resolutions Inference for Brain Imaging}},
url = {https://www.biorxiv.org/content/early/2017/11/28/226126},
year = {2017}
}
@article{Romano2007,
abstract = {Consider the problem of testing s hypotheses simultaneously. The usual approach to dealing with the multiplicity problem is to restrict attention to procedures that control the probability of even one false rejection, the familiar familywise error rate (FWER). In many applications, particularly if s is large, one might be willing to tolerate more than one false rejection if the number of such cases is controlled, thereby increasing the ability of the procedure to reject false null hypotheses One possibility is to replace control of the FWER by control of the probability of k or more false rejections, which is called the k-FWER. We derive both single-step and stepdown procedures that control the k-FWER in finite samples or asymptotically, depending on the situation. Lehmann and Romano (2005a) derive some exact methods for this purpose, which apply whenever p-values are available for individual tests; no assumptions are made on the joint dependence of the p-values. In contrast, we construct methods that implicitly take into account the dependence structure of the individual test statistics in order to further increase the ability to detect false null hypotheses. We also consider the false discovery proportion (FDP) defined as the number of false rejections divided by the total number of rejections (and defined to be 0 if there are no rejections). The false discovery rate proposed by Benjamini and Hochberg (1995) controls E(FDP). Here, the goal is to construct methods which satisfy, for a given $\gamma$ and $\alpha$, P{\{}FDP {\textgreater} $\gamma${\}} ≤ $\alpha$, at least asymptotically.},
archivePrefix = {arXiv},
arxivId = {0710.2258},
author = {Romano, Joseph P. and Wolf, Michael},
doi = {10.1214/009053606000001622},
eprint = {0710.2258},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Romano, Wolf - 2007 - Control of generalized error rates in multiple testing.pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Bootstrap,False discovery proportion,False discovery rate,Generalized familywise error rate,Multiple testing,Step-down procedure},
number = {4},
pages = {1378--1408},
pmid = {734},
title = {{Control of generalized error rates in multiple testing}},
volume = {35},
year = {2007}
}
@article{Dimakis2010,
abstract = {Gossip algorithms are attractive for in-network processing in sensor networks because they do not require any specialized routing, there is no bottleneck or single point of failure, and they are robust to unreliable wireless network conditions. Recently, there has been a surge of activity in the computer science, control, signal processing, and information theory communities, developing faster and more robust gossip algorithms and deriving theoretical performance guarantees. This paper presents an overview of recent work in the area. We describe convergence rate results, which are related to the number of transmitted messages and thus the amount of energy consumed in the network for gossiping. We discuss issues related to gossiping over wireless links, including the effects of quantization and noise, and we illustrate the use of gossip algorithms for canonical signal processing tasks including distributed estimation, source localization, and compression.},
archivePrefix = {arXiv},
arxivId = {1003.5309},
author = {Dimakis, Alexandros G. and Kar, Soummya and Moura, Jos{\'{e}} M.F. and Rabbat, Michael G. and Scaglione, Anna},
doi = {10.1109/JPROC.2010.2052531},
eprint = {1003.5309},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dimakis et al. - 2010 - Gossip algorithms for distributed signal processing.pdf:pdf},
isbn = {9550101029},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Consensus protocols,distributed algorithms,distributed processing,gossip protocols,graph theory,information networks distributed averaging,network topology,peer to peer computing,protocols,random topologies,topology design,wireless sensor networks},
number = {11},
pages = {1847--1864},
title = {{Gossip algorithms for distributed signal processing}},
volume = {98},
year = {2010}
}
@article{Disclaimer2008,
author = {Disclaimer, Legal and Krychman, Michael L},
doi = {10.1159/000323281},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Disclaimer, Krychman - 2008 - Contents of This {\{}(2).pdf:pdf}},
isbn = {0470845139},
issn = {1662-4025},
journal = {History},
number = {May},
pages = {1--10},
pmid = {21196786},
title = {{Contents of This {\{}}}},
volume = {8},
year = {2008}
}
@article{Nosko1987,
author = {Nosko, V. P.},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nosko - 1987 - The Investigation of High-Level Excursions of Gaussian Fields a Fresh Approach involving Convexity.pdf:pdf},
title = {{The Investigation of High-Level Excursions of Gaussian Fields: a Fresh Approach involving Convexity}},
year = {1987}
}
@article{Gine1990,
author = {Gin{\'{e}}, Evarist and Zinn, Joel},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Theory/Bootstrapping General Empirical Measures.pdf:pdf},
journal = {The Annals of Probability},
number = {2},
pages = {851--869},
title = {{Bootstrapping General Empirical Measures}},
volume = {18},
year = {1990}
}
@article{Machkouri2013,
author = {El, Mohamed and Voln, Dalibor},
doi = {10.1016/j.spa.2012.08.014},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/El, Voln - 2013 - A central limit theorem for stationary random fields.pdf:pdf},
journal = {Stochastic Processes and their Applications},
keywords = {central limit theorem,m -dependent random fields,spatial processes,weak mixing},
pages = {1--14},
title = {{A central limit theorem for stationary random fields}},
volume = {123},
year = {2013}
}
@article{Taylor2016a,
abstract = {We present a new method for post-selection inference for (lasso)-penalized likelihood models, including generalized regression models. Our approach generalizes the post-selection framework presented in Lee et al. (2013). The method provides p-values and confidence intervals that are asymptotically valid, conditional on the inherent selection done by the lasso. We present applications of this work to (regularized) logistic re-gression, Cox's proportional hazards model and the graphical lasso.},
archivePrefix = {arXiv},
arxivId = {1602.07358},
author = {Taylor, Jonathan and Tibshirani, Robert J},
doi = {10.1002/cjs.11313},
eprint = {1602.07358},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Taylor, Tibshirani - 2016 - Post-selection inference for mathcal{\{}l{\}}1- penalized likelihood models(2).pdf:pdf},
issn = {1708945X},
number = {2015},
pages = {1--23},
title = {{Post-selection inference for $\backslash$mathcal{\{}l{\}}1- penalized likelihood models}},
year = {2016}
}
@article{Westfall1989,
abstract = {Data from rodent carcinogenicity (preclinical) and clinical studies involving new drugs may be modeled as having come from multivariate binomial distributions. In two-year rodent carcinogenicity studies, there are typically 20-50 tissues examined for occurrence of any of several possible lesions. For a particular treatment group, the number of occurrences of a particular lesion at a particular tissue may be modeled as binomial, and the vector of such frequencies may be considered multivariate binomial with unspecified dependence structure. The same model may also apply to clinical side-effects data; in this case the marginal frequencies may represent occurrences of events ranging from headaches to ingrown toenails. Frequently, the goal of such studies is to isolate site-specific significant differences between treatment and control groups. For example, in rodent carcinogenicity analyses it is generally not sufficient to claim that a new compound causes an increase in tumors at some unspecified site; rather, the report should identify the particular sites where unusual increases are noted. Such an analysis requires separate tests for each site. False significance may easily occur when multiple tests are performed. When a marginal significance criterion p {\^{a}}‰¤ .05 is used, experimentwise false significance rates as large as 44{\%} have been reported (Haseman, Winbush, and O'Donnell 1986). Others have reported the experimentwise false significance rate much lower; for example, Gart, Chu, and Tarone (1979) reported 8{\%}-10{\%} for each sex and species combination of a two-sex, two-species experiment. In this article it is proposed that the experimentwise false significance rate be controlled by adjusting all p values for the multiplicity of testing using vector-based resampling methods. This analysis is an extension of the bootstrap method described by Westfall (1985) to the multisample case, with particular application to models useful in clinical and preclinical biopharmaceutical analyses; it is also similar to the methodology proposed by Brown and Fears (1981). Assuming no differences between treatment and control groups (the null case), one may estimate the multivariate binomial distribution or permutation distribution conveniently via vector resampling. Using this estimated distribution, one may easily estimate (via Monte Carlo) the probability that the smallest p value in the study is smaller than any given threshold. An adjusted p value is then defined as the probability that the smallest p value in the study is less than or equal to the observed p value for the given test. This methodology is compared to the usual Bonferroni-style adjustments, and it is demonstrated that these adjustments are grossly conservative in certain instances because of their failure to account for dependence between tests and the discreteness of the data. Results of bootstrap and permutation resampling adjustments tend to be similar, particularly for large sample sizes. The approaches are philosophically different: Bootstrap resampling is preferable if an unconditional analysis is desired [Upton (1982) demonstrated that nominal and actual Type I errors are closer and that statistical power is greater in the univariate two-sample case] whereas permutation resampling gives essentially exact results and is preferable if a conditional analysis is desired [Yates (1984) gave philosophical arguments for favoring the conditional approach].},
author = {Westfall, Peter H. and Young, S. Stanley},
doi = {10.1080/01621459.1989.10478837},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Westfall, Young - 1989 - p Value Adjustments for Multiple Tests in Multivariate Binomial Models.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {bootstrap,clinical trial,permutation test,rodent carcinogenicity study,simultaneous test procedure},
number = {407},
pages = {780--786},
pmid = {2289666},
title = {{p Value Adjustments for Multiple Tests in Multivariate Binomial Models}},
volume = {84},
year = {1989}
}
@article{Rinaldo2016,
abstract = {Several new methods have been proposed for performing valid inference after model selection. An older method is sampling splitting: use part of the data for model selection and part for inference. In this paper we revisit sample splitting combined with the bootstrap (or the Normal approximation). We show that this leads to a simple, assumption-free approach to inference and we establish results on the accuracy of the method. In fact, we find new bounds on the accuracy of the bootstrap and the Normal approximation for general nonlinear parameters with increasing dimension which we then use to assess the accuracy of regression inference. We show that an alternative, called the image bootstrap, has higher coverage accuracy at the cost of more computation. We define new parameters that measure variable importance and that can be inferred with greater accuracy than the usual regression coefficients. There is a inference-prediction tradeoff: splitting increases the accuracy and robustness of inference but can decrease the accuracy of the predictions.},
archivePrefix = {arXiv},
arxivId = {1611.05401},
author = {Rinaldo, Alessandro and Wasserman, Larry and G'Sell, Max and Lei, Jing and Tibshirani, Ryan},
eprint = {1611.05401},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Selective Inference/Bootstrapping and Sample Splitting.pdf:pdf},
pages = {1--52},
title = {{Bootstrapping and Sample Splitting For High-Dimensional, Assumption-Free Inference}},
year = {2016}
}
@article{Woolrich2006,
abstract = {Mixture models are commonly used in the statistical segmentation of images. For example, they can be used for the segmentation of structural medical images into different matter types, or of statistical parametric maps into activating and nonactivating brain regions in functional imaging. Spatial mixture models have been developed to augment histogram information with spatial regularization using Markov random fields (MRFs). In previous work, an approximate model was developed to allow adaptive determination of the parameter controlling the strength of spatial regularization. Inference was performed using Markov Chain Monte Carlo (MCMC) sampling. However, this approach is prohibitively slow for large datasets. In this work, a more efficient inference approach is presented. This combines a variational Bayes approximation with a second-order Taylor expansion of the components of the posterior distribution, which would otherwise be intractable to Variational Bayes. This provides inference on fully adaptive spatial mixture models an order of magnitude faster than MCMC. We examine the behavior of this approach when applied to artificial data with different spatial characteristics, and to functional magnetic resonance imaging statistical parametric maps.},
author = {Woolrich, Mark W. and Behrens, Timothy E.},
doi = {10.1109/TMI.2006.880682},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Woolrich, Behrens - 2006 - Variational bayes inference of spatial mixture models for segmentation.pdf:pdf},
isbn = {0278-0062 (Print)},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Woolrich2006},
number = {10},
pages = {1380--1391},
pmid = {17024841},
title = {{Variational bayes inference of spatial mixture models for segmentation}},
volume = {25},
year = {2006}
}
@article{Fe;eschow2018,
author = {Telschow, Fabian J E and Schwartzman, Armin},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/Random{\_}Field{\_}Theory/Schwartzman/Fabian's Work/2011 MULTIPLE TESTING OF LOCAL MAXIMA FOR DETECTION of peaks in 1D.pdf:pdf},
number = {September},
pages = {1--36},
title = {{On Simultaneous Confidence Statements and Inference for Functional Data Using the Gaussian Kinematic Formula}},
year = {2018}
}
@misc{Debreu1960,
author = {Harrell, Cleon and Debreu, Gerard},
booktitle = {Southern Economic Journal},
doi = {10.2307/1055180},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Harrell, Debreu - 1960 - Theory of Value An Axiomatic Analysis of Economic Equilibrium(2).pdf:pdf},
isbn = {0300015593},
issn = {00384038},
number = {2},
pages = {149},
pmid = {22167720},
title = {{Theory of Value: An Axiomatic Analysis of Economic Equilibrium}},
volume = {27},
year = {1960}
}
@book{little&rubin:2002,
author = {Little, R A and Rubin, D B},
edition = {2},
file = {:data/fireback/davenpor/davenpor/Books/Statistics and Linear Algebra/Data Analysis/Roderick J. A. Little, Donald B. Rubin-Statistical Analysis with Missing Data-Wiley-Interscience (2002).pdf:pdf},
publisher = {John Weily {\&} Sons. Inc.},
title = {{Statistical Analysis with Missing Data}},
year = {2002}
}
@article{Ker-Chau1999,
abstract = {"Regression Graphics: Ideas for Studying Regression Through Graphics" by R. Dennis Cook is reviewed.},
author = {Ker-Chau, Li},
doi = {10.2307/1271354},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ker-Chau - 1999 - Regression Graphics Ideas for Studying Regression Through Graphics(2).pdf:pdf},
isbn = {01621459},
issn = {00401706},
journal = {Journal of the American Statistical Association},
keywords = {Compu,Nonfiction,Regression analysis,Semiparametrics,Statistics},
mendeley-tags = {Semiparametrics},
number = {447},
pages = {980},
pmid = {724246},
title = {{Regression Graphics: Ideas for Studying Regression Through Graphics}},
volume = {94},
year = {1999}
}
@article{Geyer2005,
abstract = {The optimal hypothesis tests for the binomial distribution and some other discrete distributions are uniformly most powerful (UMP) one-tailed and UMP unbiased (UMPU) two-tailed randomized tests. Conventional confidence intervals are not dual to randomized tests and perform badly on discrete data at small and moderate sample sizes. We introduce a new confidence interval notion, called fuzzy confidence intervals, that is dual to and inherits the exactness and optimality of UMP and UMPU tests. We also introduce a new P-value notion, called fuzzy P-values or abstract randomized P-values, that also inherits the same exactness and optimality.},
author = {Geyer, Charles J and Meeden, Glen D},
doi = {10.1214/088342305000000340},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Multiple Testing/Bayesian/Fuzzy and Randomized Confidence.pdf:pdf},
isbn = {0883-4237},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {and phrases,confidence interval,formly most powerful unbiased,fuzzy set theory,hypothesis test,p -value,ran-,ump and umpu,uni-},
number = {4},
pages = {358--366},
title = {{Fuzzy and randomized confidence intervals and P-values}},
volume = {20},
year = {2005}
}
@article{Algeri2018,
abstract = {The identification of new rare signals in data, the detection of a sudden change in a trend, and the selection of competing models, are among the most challenging problems in statistical practice. These challenges can be tackled using a test of hypothesis where a nuisance parameter is present only under the alternative, and a computationally efficient solution can be obtained by the "Testing One Hypothesis Multiple times" (TOHM) method. In the one-dimensional setting, a fine discretization of the space of the non-identifiable parameter is specified, and a global p-value is obtained by approximating the distribution of the supremum of the resulting stochastic process. In this paper, we propose a computationally efficient inferential tool to perform TOHM in the multidimensional setting. Here, the approximations of interest typically involve the expected Euler Characteristics (EC) of the excursion set of the underlying random field. We introduce a simple algorithm to compute the EC in multiple dimensions and for arbitrary large significance levels. This leads to an highly generalizable computational tool to perform inference under non-standard regularity conditions.},
archivePrefix = {arXiv},
arxivId = {1803.03858},
author = {Algeri, Sara and van Dyk, David A.},
eprint = {1803.03858},
file = {:data/fireback/davenpor/davenpor/Referreeing/Testing One Hypothesis Multiple Times- The Multidimensional Case/One-Dimensional Case.pdf:pdf},
keywords = {bonferroni correction,bump hunting,ing,multiple hypothesis testing,non-identifiabily in hypothesis test-,non-nested models comparison},
pages = {1--30},
title = {{Testing One Hypothesis Multiple Times: The Multidimensional Case}},
year = {2018}
}
@article{Shao2013,
abstract = {Let X 1 , X 2 , . . . , be independent random variables with EX i = 0 and write Sn = n i=1 X i and V 2 n = n i=1 X 2 i . This paper provides an overview of current developments on the functional central limit the-orems (invariance principles), absolute and relative errors in the central limit theorems, moderate and large deviation theorems and saddle-point approximations for the self-normalized sum Sn/Vn. Other self-normalized limit theorems are also briefly discussed.},
author = {Shao, Qi-Man and Wang, Qiying},
doi = {10.1214/13-PS216},
file = {:homes/davenpor/global/PeakInference/Peak Papers/Self-normalized limit theorems$\backslash$:.pdf:pdf},
issn = {1549-5787},
journal = {Probability Surveys},
keywords = {absolute error,and phrases,central,convergence rate,invariance principle,limit theorem,rel-,self-normalized sum,student t statistic},
number = {0},
pages = {69--93},
title = {{Self-normalized limit theorems: A survey}},
volume = {10},
year = {2013}
}
@book{Efron1993,
author = {Efron, Bradley and Tibshirani, Robert J},
doi = {10.3366/drt.2013.0047},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Efron, Tibshirani - 1993 - An Introduction to the Bootstrap.pdf:pdf},
isbn = {9780412042317},
keywords = {Bootstrap},
mendeley-tags = {Bootstrap},
title = {{An Introduction to the Bootstrap}},
year = {1993}
}
@article{Teh,
author = {Teh, Yee Whye},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Teh - Unknown - A Bayesian Interpretation of Interpolated Kneser-Ney(2).pdf:pdf},
title = {{A Bayesian Interpretation of Interpolated Kneser-Ney}}
}
@article{McKeown1998,
abstract = {Current analytical techniques applied to functional magnetic resonance imaging (fMRI) data require a priori knowledge or specific assumptions about the time courses of processes contributing to the measured signals. Here we describe a new method for analyzing fMRI data based on the independent component analysis (ICA) algorithm of Bell and Sejnowski ([1995]: Neural Comput 7:1129–1159). We decomposed eight fMRI data sets from 4 normal subjects performing Stroop color-naming, the Brown and Peterson word/number task, and control tasks into spatially independent components. Each component consisted of voxel values at fixed three-dimensional locations (a component ‘‘map''), and a unique associated time course of activation. Given data from 144 time points collected during a 6-min trial, ICA extracted an equal number of spatially independent components. In all eight trials, ICA derived one and only one component with a time course closely matching the time course of 40-sec alternations between experimental and control tasks. The regions of maximum activity in these consistently task-related components generally overlapped active regions detected by standard correlational analysis, but included frontal regions not detected by correlation. Time courses of other ICA components were transiently task-related, quasiperiodic, or slowly varying. By utilizing higher-order statistics to enforce successively stricter criteria for spatial independence between component maps, both the ICA algorithm and a related fourth-order decomposition technique (Comon [1994]: Signal Processing 36:11–20) were superior to principal component analysis (PCA) in determining the spatial and temporal extent of task-related activation. For each subject, the time courses and active regions of the task-related ICA components were consistent across trials and were robust to the addition of simulated noise. Simulated movement artifact and simulated task-related activations added to actual fMRI data were clearly separated by the algorithm. ICA can be used to distinguish between nontask-related signal components, movements, and other artifacts, as well as consistently or transiently task-related fMRI activations, based on only weak assumptions about their spatial distributions and without a priori assumptions about their time courses. ICA appears to be a highly promising method for the analysis of fMRI data from normal and clinical populations, especially for uncovering unpredictable transient patterns of brain activity associated with performance of psychomotor tasks.},
author = {McKeown, M J and Makeig, S and Brown, G G and Jung, T and Kindermann, S S and Bell, A J and Sejnowski, Terrence J},
doi = {10.1002/(SICI)1097-0193(1998)6:3<160::AID-HBM5>3.0.CO;2-1},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/McKeown et al. - 1998 - Analysis of fMRI data by blind separation into independant spatial components.pdf:pdf},
isbn = {1065-9471 (Print)$\backslash$r1065-9471 (Linking)},
issn = {1065-9471},
journal = {Human Brain Mapping},
keywords = {functional magnetic resonance imaging,higher-order statistics,independent component analysis},
number = {June 1997},
pages = {160--188},
pmid = {9673671},
title = {{Analysis of fMRI data by blind separation into independant spatial components}},
volume = {6},
year = {1998}
}
@book{Murphy1991,
abstract = {Some of the most remarkable issues related to interharmonics observed from a probabilistic perspective are presented. Attention is firstly devoted to interharmonic frequency and amplitude variability. Starting from the basic mathematical and computational aspects of probabilistic harmonic models, the difficulties to include interharmonics are discussed with particular attention to the problem of the frequency resolution and of the computational burden. Then, simulation and measurement aspects are discussed, also showing some numerical and experimental results.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {{P. Murphy}, Kevin},
booktitle = {Machine Learning: A Probabilistic Perspective},
doi = {10.1007/SpringerReference_35834},
eprint = {0-387-31073-8},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/P. Murphy - 1991 - Machine Learning A Probabilistic Perspective(2).pdf:pdf},
isbn = {9780262018029},
issn = {0262018020},
pages = {73--78,216--244},
pmid = {20236947},
title = {{Machine Learning: A Probabilistic Perspective}},
year = {1991}
}
@article{Shao2010,
abstract = {We propose a new resampling procedure, the dependent wild bootstrap, for stationary time series. As a natural extension of the traditional wild bootstrap to time series setting, the dependent wild bootstrap offers a viable alternative to the existing block-based bootstrap methods, whose properties have been extensively studied over the last two decades. Unlike all of the block-based bootstrap methods, the dependent wild bootstrap can be easily extended to irregularly spaced time series with no implementational difficulty. Furthermore, it preserves the favorable bias and mean squared error property of the tapered block bootstrap, which is the state-of-the-art block-based method in terms of asymptotic accuracy of variance estimation and distribution approximation. The consistency of the dependent wild bootstrap in distribution approximation is established under the framework of the smooth function model. In addition, we obtain the bias and variance expansions of the dependent wild bootstrap variance estimator for irregularly spaced time series on a lattice. For irregularly spaced nonlattice time series, we prove the consistency of the dependent wild bootstrap for variance estimation and distribution approximation in the mean case. Simulation studies and an empirical data analysis illustrate the finite-sample performance of the dependent wild bootstrap. Some technical details and tables are included in the online supplemental material.},
author = {Shao, Xiaofeng},
doi = {10.1198/jasa.2009.tm08744},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shao - 2010 - The Dependent Wild Bootstrap(2).pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {block bootstrap,irregularly spaced time series,lag window estimator,tapering,variance estimation},
number = {489},
pages = {218--235},
title = {{The Dependent Wild Bootstrap}},
volume = {105},
year = {2010}
}
@article{Newton1994,
author = {Newton, Michael A and Raftery, Adrian E and Journal, Source and Statistical, Royal and Series, Society},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Bayesian Bootstrap/Approximate Bayesian Inference with the Weighted Likelihood Bootstrap.pdf:pdf},
keywords = {bayes factor,bayesian inference,dirichlet weights,monte carlo methods},
number = {1},
pages = {3--48},
title = {{Approximate Bayesian Inference with the Weighted Likelihood Bootstrap}},
volume = {56},
year = {1994}
}
@article{Davies1977,
author = {Davies, Robert B},
file = {:data/fireback/davenpor/davenpor/Referreeing/Testing One Hypothesis Multiple Times- The Multidimensional Case/MArk2/References/Davies 1977 Hypothesis Testing When a Nuisance Parameter is Present Only Under the Alternative1.pdf:pdf},
journal = {Biometrika},
number = {2},
pages = {33--247--254},
title = {{Trust Hypothesis Testing when a Nuisance Parameter is Present Only Under the Alternatives}},
volume = {64},
year = {1977}
}
@article{Vitale2000,
abstract = {Extensions and variants are given for the well-known comparison principle for Gaussian processes based on ordering by pairwise distance.},
archivePrefix = {arXiv},
arxivId = {math/0011093},
author = {Vitale, Richard A},
doi = {10.1090/S0002-9939-00-05367-3},
eprint = {0011093},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/EVT/Some Comparisons for Gaussian Processes.pdf:pdf},
issn = {0002-9939},
journal = {Convergence},
number = {1975},
pages = {4},
primaryClass = {math},
title = {{Some Comparisons for Gaussian Processes}},
volume = {128},
year = {2000}
}
@article{Knoblauch20182010,
author = {Knoblauch2018},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Changepoint Detection/JezzasFinalDraft.pdf:pdf},
title = {{Spatio-temporal Bayesian On-line Changepoint Detection with Model Selection}},
year = {2010}
}
@article{Adler1976,
author = {Adler, Robert J.},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Adler - 1976 - Excursions above a Fixed Level by n-Dimensional Random Fields.pdf:pdf},
journal = {Journal of Applied Probability},
number = {2},
pages = {276--289},
title = {{Excursions above a Fixed Level by n-Dimensional Random Fields}},
volume = {13},
year = {1976}
}
@article{Bonilla2008,
abstract = {In this paper we investigate multi-task learning in the context of Gaussian Processes (GP). We propose a model that learns a shared covariance function on input-dependent features and a ``free-form'' covariance matrix over tasks. This allows for good flexibility when modelling inter-task dependencies while avoiding the need for large amounts of data for training. We show that under the assumption of noise-free observations and a block design, predictions for a given task only depend on its target values and therefore a cancellation of inter-task transfer occurs. We evaluate the benefits of our model on two practical applications: a compiler performance prediction problem and an exam score prediction task. Additionally, we make use of GP approximations and properties of our model in order to provide scalability to large data sets.},
author = {Bonilla, Edwin and Chai, Kian Ming and Williams, Christopher},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bonilla, Chai, Williams - 2008 - Multi-task Gaussian Process Prediction(2).pdf:pdf},
isbn = {160560352X},
journal = {Advances in Neural Information Processing Systems},
keywords = {learning,statistics {\&} optimisation},
number = {October},
pages = {153--160},
title = {{Multi-task Gaussian Process Prediction}},
volume = {20},
year = {2008}
}
@article{Bien2011,
abstract = {Prototype methods seek a minimal subset of samples that can serve as a distillation or condensed view of a data set. As the size of modern data sets grows, being able to present a domain specialist with a short list of “representative” samples chosen from the data set is of increasing interpretative value. While much recent statistical research has been focused on producing sparse-in-the-variables methods, this paper aims at achieving sparsity in the samples.$\backslash$n$\backslash$nWe discuss a method for selecting prototypes in the classification setting (in which the samples fall into known discrete categories). Our method of focus is derived from three basic properties that we believe a good prototype set should satisfy. This intuition is translated into a set cover optimization problem, which we solve approximately using standard approaches. While prototype selection is usually viewed as purely a means toward building an efficient classifier, in this paper we emphasize the inherent value of having a set of prototypical elements. That said, by using the nearest-neighbor rule on the set of prototypes, we can of course discuss our method as a classifier as well.$\backslash$n$\backslash$nWe demonstrate the interpretative value of producing prototypes on the well-known USPS ZIP code digits data set and show that as a classifier it performs reasonably well. We apply the method to a proteomics data set in which the samples are strings and therefore not naturally embedded in a vector space. Our method is compatible with any dissimilarity measure, making it amenable to situations in which using a non-Euclidean metric is desirable or even necessary.},
archivePrefix = {arXiv},
arxivId = {arXiv:1202.5933v1},
author = {Bien, Jacob and Tibshirani, Robert J},
doi = {10.1214/11-AOAS495},
eprint = {arXiv:1202.5933v1},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bien, Tibshirani - 2011 - Prototype selection for interpretable classification(2).pdf:pdf},
isbn = {1932-6157},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {Classification,Integer program,Nearest neighbors,Prototypes,Set cover},
number = {4},
pages = {2403--2424},
title = {{Prototype selection for interpretable classification}},
volume = {5},
year = {2011}
}
@article{Ho1996,
author = {Ho, Hwai-Chung and Hsing, Tailen},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ho, Hsing - 1996 - On the Asymptotic Joint Distribution of the Sum and Maximum of Stationary Normal Random Variables(2).pdf:pdf},
keywords = {Normal Maxima},
mendeley-tags = {Normal Maxima},
number = {1},
pages = {138--145},
title = {{On the Asymptotic Joint Distribution of the Sum and Maximum of Stationary Normal Random Variables}},
volume = {33},
year = {1996}
}
@article{Degras2011,
abstract = {The goal of this paper is to present a method for construction of simultaneous Scheff´ e confidence bands for nonparametric prediction functions. The family of non- parametric functions studied here are of the polynomial-trigonometric series type, with estimation of the model parameters undertaken in the standard least-squares framework. After the method is set forth the performance of the procedure is stud- ied via Monte-Carlo simulations. iv},
archivePrefix = {arXiv},
arxivId = {arXiv:0908.1980v3},
author = {Degras, David a},
doi = {10.5705/ss.2009.207},
eprint = {arXiv:0908.1980v3},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/IMULTANEOUS CONFIDENCE BANDS FOR.pdf:pdf},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {and phrases,functional asymptotic normality,functional data,goodness-,nonparametric regression,of-fit test,simultaneous confidence bands},
pages = {1735--1765},
title = {{Simultaneous Confidence Bands for Nonparametric Regression With Functional Data}},
volume = {21},
year = {2011}
}
@article{Bengio2012,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
archivePrefix = {arXiv},
arxivId = {1206.5538},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1145/1756006.1756025},
eprint = {1206.5538},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Machine Learning/Neural Nets/Representation Learning$\backslash$: A Review and New
Perspectives:},
isbn = {1206.5538},
issn = {15324435},
number = {1993},
pages = {1--30},
pmid = {23787338},
title = {{Representation Learning: A Review and New Perspectives}},
url = {http://arxiv.org/abs/1206.5538},
year = {2012}
}
@article{James1993,
author = {James, Gareth M. and Silverman, Bernard W.},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/James, Silverman - 1993 - Functional Adaptive Model Estimation(2).pdf:pdf},
keywords = {Functional predictor,Functional principal components,Generalized,Generalized linear models,Projection pursuit regression,additive models},
pages = {1--20},
title = {{Functional Adaptive Model Estimation}},
year = {1993}
}
@article{Simon2013,
	title={On estimating many means, selection bias, and the bootstrap},
	author={Simon, Noah and Simon, Richard},
	journal={arXiv preprint arXiv:1311.3709},
	year={2013}
}
@article{Major1978,
abstract = {The paper deals with the invariance principle for sums of independent identically distributed random variables. First it compares the different possibilities of posing the problem. The sharpest results of this theory are presented with a sketch of their proofs. At the end of the paper some unsolved problems are given. ?? 1978.},
author = {Major, P{\'{e}}ter},
doi = {10.1016/0047-259X(78)90029-5},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/On the invariance principle for sums of independent identically distributed random variables.pdf:pdf},
issn = {10957243},
journal = {Journal of Multivariate Analysis},
keywords = {Invariance principle,weak convergence},
number = {4},
pages = {487--517},
title = {{On the invariance principle for sums of independent identically distributed random variables}},
volume = {8},
year = {1978}
}
@article{Efron1986,
author = {Efron, Bradley and Tibshirani, Robert J},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Classical Papers/Bootstrap Methodss.pdf:pdf},
title = {{Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy}}
}
@article{Webb2006,
abstract = {Publisher Summary This chapter discusses reliability coefficients as developed in the framework of the classical test theory (CTT) and describes the way the conception and estimation of reliability has been broadened in generalizability theory (G). It also discusses the foundations of CTT and focuses on the traditional methods of estimating reliability. It reviews generalizability theory, its applications, and recent theoretical contributions. In addition to computational procedures, G theory provides a conceptual framework and notational system for characterizing alternative intended universes of generalization, distinguishing fixed versus random facets, and designing efficient measurement procedures. G theory has also clarified the use and interpretation of coefficients and estimated standard errors from CTT. This is of value because, despite the greater sophistication of G theory and the important applications, CTT remains the dominant tool for estimating reliability. However, reliability is just one consideration in the responsible use and interpretation of tests, and not the most important consideration. CN - 0000},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Webb, Noreen M. and Shavelson, Richard J. and Haertel, Edward H.},
doi = {10.1016/S0169-7161(06)26004-8},
eprint = {arXiv:1011.1669v3},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Webb, Shavelson, Haertel - 2006 - Reliability Coefficients and Generalizability Theory.pdf:pdf},
isbn = {9780444521033},
issn = {01697161},
journal = {Handbook of Statistics},
pages = {81--124},
pmid = {25246403},
title = {{Reliability Coefficients and Generalizability Theory}},
volume = {26},
year = {2006}
}
@article{Majumdat2014,
abstract = {Extreme value statistics (EVS) concerns the study of the statistics of the maximum or the minimum of a set of random variables. This is an important problem for any time-series and has applications in climate, finance, sports, all the way to physics of disordered systems where one is interested in the statistics of the ground state energy. While the EVS of uncorrelated variables are well understood, little is known for strongly correlated random variables. Only recently this subject has gained much importance both in statistical physics and in probability theory. In this note, we will first review the classical EVS for uncorrelated variables and discuss few examples of correlated variables where analytical progress can be made.},
archivePrefix = {arXiv},
arxivId = {1406.6768},
author = {Majumdar, Satya N and Pal, Arnab},
eprint = {1406.6768},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/EVT/Extreme value statistics of correlated random variables.pdf:pdf},
journal = {Arxiv},
pages = {1--14},
title = {{Extreme value statistics of correlated random variables}},
year = {2014}
}
@article{Taylor2005,
author = {Taylor, Jonathan},
doi = {10.1016/0167-6636(93)90027-O},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Taylor - 2005 - 2 Random Fields(2).pdf:pdf},
isbn = {9780819487018},
journal = {Mechanics of Materials},
keywords = {Chapter 2,RFT},
mendeley-tags = {Chapter 2,RFT},
pages = {55--64},
title = {{2 Random Fields}},
volume = {16},
year = {2005}
}
@book{Tong1990,
author = {Tong, Y. L.},
file = {:data/fireback/davenpor/davenpor/Books/Statistics and Linear Algebra/Y.L. Tong The Multivariate Normal Distribution.djvu:djvu},
title = {{The Multivariate Normal Distribution}},
year = {1990}
}
@article{Tan2014,
	title={Selection bias correction and effect size estimation under dependence},
	author={Tan, Kean Ming and Simon, Noah and Witten, Daniela},
	journal={arXiv preprint arXiv:1405.4251},
	year={2014}
}
@article{Cao2012,
author = {Cao, J. and Worsley, K. J.},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao, Worsley - 2012 - The Detection of Local Shape Changes via the Geometry of Hotelling ' s T2 Fields.pdf:pdf},
number = {3},
pages = {925--942},
title = {{The Detection of Local Shape Changes via the Geometry of Hotelling ' s T2 Fields}},
volume = {27},
year = {2012}
}
@article{Westfall1985,
author = {Westfall, Peter H.},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Westfall - 1985 - Simultaneous Small-Sample Multivariate Bernoulli Confidence Intervals.pdf:pdf},
journal = {Biometrics},
number = {4},
pages = {1001--1013},
title = {{Simultaneous Small-Sample Multivariate Bernoulli Confidence Intervals}},
volume = {41},
year = {1985}
}
@book{Efron2010,
author = {Efron, Bradely},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Efron - 2010 - Large-Scale Inference Empirical Bayes Methods for Estimation, Testing and Prediction(2).pdf:pdf},
keywords = {empirical bayes},
mendeley-tags = {empirical bayes},
title = {{Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing and Prediction}},
year = {2010}
}
@article{Abrahamsen1997,
abstract = {This review presents some properties of Gaussian random field models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Abrahamsen, P.},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abrahamsen - 1997 - A review of Gaussian random fields and correlation functions(2).pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Proceedings Wollongong},
pages = {70},
pmid = {25246403},
title = {{A review of Gaussian random fields and correlation functions}},
volume = {1},
year = {1997}
}
@article{Lyddon2017,
abstract = {In this paper, we revisit the weighted likelihood bootstrap and show that it is well-motivated for Bayesian inference under misspecified models. We extend the underlying idea to a wider family of inferential problems. This allows us to calibrate an analogue of the likelihood function in situations where little is known about the data-generating mechanism. We demonstrate our method on a number of examples.},
archivePrefix = {arXiv},
arxivId = {1709.07616},
author = {Lyddon, Simon and Holmes, Chris and Walker, Stephen},
eprint = {1709.07616},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bayesian/Generalized Bayesian Updating and the.pdf:pdf},
pages = {1--17},
title = {{Generalized Bayesian Updating and the Loss-Likelihood Bootstrap}},
url = {http://arxiv.org/abs/1709.07616},
volume = {0},
year = {2017}
}
@article{Adler1982,
author = {Wilson, Richard and Adler, Robert J.},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wilson, Adler - 1982 - The Structure of Gaussian Fields near a Level Crossing.pdf:pdf},
journal = {Advances in Applied Probability},
number = {3},
pages = {543--565},
title = {{The Structure of Gaussian Fields near a Level Crossing}},
volume = {14},
year = {1982}
}
@article{Tobergte2013,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Tobergte, David R. and Curtis, Shirley},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tobergte, Curtis - 2013 - Lectures on Algebraic Statistics(2).pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Lectures on Algebraic Statistics}},
volume = {53},
year = {2013}
}
@article{Hallin2010,
author = {Hallin, Marc},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hallin - 2010 - Semi-parametric efficiency(2).pdf:pdf},
keywords = {Semiparametrics},
mendeley-tags = {Semiparametrics},
number = {1},
pages = {137--165},
title = {{Semi-parametric efficiency}},
volume = {9},
year = {2010}
}
@article{Camano-Garcia2006,
author = {Camano-Garcia, Gabriel},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Camano-Garcia - 2006 - Statistics on Stiefel Manifolds(2).pdf:pdf},
journal = {PhD Thesis},
keywords = {Statistics},
title = {{Statistics on Stiefel Manifolds}},
year = {2006}
}
@article{Sampson2016,
author = {Sampson, Paul D and Guttorp, Peter and Journal, Source and Statistical, American and Mar, No and Sampson, Paul D and Guttorp, Peter},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sampson et al. - 2018 - Nonparametric Estimation of Nonstationary Spatial Covariance Structure Nonparametric Estimation of Nonstationary.pdf:pdf},
keywords = {and possibly time,biorthogonal grids,dependent process,dispersion,kriging,multidimensional scaling,of estimated location,procedures for ex-,the development of nonparametric,the spatial covariances of,thin-plate spline,to unobserved sites,trapolating,variogram},
number = {417},
pages = {108--119},
title = {{Nonparametric Estimation of Nonstationary Spatial Covariance Structure Nonparametric Estimation of Nonstationary Spatial Covariance Structure}},
volume = {87},
year = {2018}
}
@article{Efron2011,
	title={Tweedie’s formula and selection bias},
	author={Efron, Bradley},
	journal={Journal of the American Statistical Association},
	volume={106},
	number={496},
	pages={1602--1614},
	year={2011},
	publisher={Taylor \& Francis}
}
@article{Hamidieh2009,
archivePrefix = {arXiv},
arxivId = {arXiv:1005.4358v1},
author = {Hamidieh, Kamal and Stoev, Stilian and Michailidis, George},
doi = {10.1198/jcgs.2009.08065},
eprint = {arXiv:1005.4358v1},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/EVT/Extremal Index - Stationary Dependent Extreme Value Theory/On the Estimation of the Extremal Index Based on.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Asymptotic normality,Bootstrap,Heavy tails,Perm},
number = {3},
pages = {731--755},
title = {{On the estimation of the extremal index based on scaling and resampling}},
volume = {18},
year = {2009}
}
@article{Nosko1984,
author = {Nosko, V. P.},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nosko - Unknown - The Local Strcture of a Homogeneous Gaussian Random Field in a Neighborhood of High Level Points.pdf:pdf},
number = {4},
pages = {767--782},
title = {{The Local Strcture of a Homogeneous Gaussian Random Field in a Neighborhood of High Level Points}},
volume = {XXX}
}
@article{Gelman2012,
abstract = {Applied researchers often find themselves making statistical inferences in settings that would seem to require multiple comparisons adjustments. We challenge the Type I error paradigm that underlies these corrections. Moreover we posit that the problem of multiple comparisons can disappear entirely when viewed from a hierarchical Bayesian perspective. We propose building multilevel models in the settings where multiple comparisons arise. Multilevel models perform partial pooling (shifting estimates toward each other), whereas classical procedures typically keep the centers of intervals stationary, adjusting for multiple comparisons by making the intervals wider (or, equivalently, adjusting the {\$}p{\$}-values corresponding to intervals of fixed width). Thus, multilevel models address the multiple comparisons problem and also yield more efficient estimates, especially in settings with low group-level variation, which is where multiple comparisons are a particular concern.},
archivePrefix = {arXiv},
arxivId = {0907.2478},
author = {Gelman, Andrew and Hill, Jennifer and Yajima, Masanao},
doi = {10.1080/19345747.2011.618213},
eprint = {0907.2478},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Multiple Testing/Bayesian/Why We (Usually) Don't Have to Worry.pdf:pdf},
isbn = {1934-5747},
issn = {1934-5747},
journal = {Journal of Research on Educational Effectiveness},
keywords = {bayesian inference,hierarchical modeling,multiple comparisons,statis-,type s error},
number = {2},
pages = {189--211},
title = {{Why We (Usually) Don't Have to Worry About Multiple Comparisons}},
volume = {5},
year = {2012}
}
@article{Hall1991,
author = {Hall, Peter and Wilson, Susan R.},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hall, Wilson - 1991 - Two Guidelines for Bootstrap Hypothesis Testing.pdf:pdf},
journal = {Biometrics},
number = {2},
pages = {757--762},
title = {{Two Guidelines for Bootstrap Hypothesis Testing}},
volume = {47},
year = {1991}
}
@misc{Monestiez1991,
author = {Monestiez, P and Switzer, P},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/Random{\_}Field{\_}Theory/Other/Non-Stationarity/1991 Semiparametric estimation ofnonstationary spatial covariance models by multidimensional scaling.pdf:pdf},
title = {{Semiparametric estimation ofnonstationary spatial covariance models by multidimensional scaling}}
}
@article{Taylor2006,
abstract = {In this paper we consider probabilistic analogues of some classical integral geometric formulae: Weyl–Steiner tube formulae and the Chern–Federer kinematic fundamental formula. The probabilistic building blocks are smooth, real-valued random fields built up from i.i.d. copies of centered, unit-variance smooth Gaussian fields on a manifold M. Specifically, we consider random fields of the form fp=F(y1(p),{\ldots},yk(p)) for F∈C2(ℝk;ℝ) and (y1,{\ldots},yk) a vector of C2 i.i.d. centered, unit-variance Gaussian fields. The analogue of the Weyl–Steiner formula for such Gaussian-related fields involves a power series expansion for the Gaussian, rather than Lebesgue, volume of tubes: that is, power series expansions related to the marginal distribution of the field f. The formal expansions of the Gaussian volume of a tube are of independent geometric interest. As in the classical Weyl–Steiner formulae, the coefficients in these expansions show up in a kinematic formula for the expected Euler characteristic, $\chi$, of the excursion sets M∩f−1[u,+∞)=M∩y−1(F−1[u,+∞)) of the field f. The motivation for studying the expected Euler characteristic comes from the well-known approximation .},
archivePrefix = {arXiv},
arxivId = {arXiv:math/0602545v1},
author = {Taylor, Jonathan},
doi = {10.1214/009117905000000594},
eprint = {0602545v1},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Taylor - 2006 - A Gaussian kinematic formula(2).pdf:pdf},
issn = {00911798},
journal = {Annals of Probability},
keywords = {Euler characteristic,Excursions,Gaussian processes,Manifolds,Random fields,Riemannian geometry},
number = {1},
pages = {122--158},
primaryClass = {arXiv:math},
title = {{A Gaussian kinematic formula}},
volume = {34},
year = {2006}
}
@article{Chen2004,
author = {Chen, L and Saho, Q},
doi = {10.1214/009117904000000450},
file = {:homes/davenpor/global/TomsMiniProject/Toms{\_}Relevant{\_}Papers/Random{\_}Field{\_}Theory/Other/Lattice/NORMAL APPROXIMATION UNDER LOCAL DEPENDENCE.pdf:pdf},
keywords = {and phrases,concentration,esseen bound,inequality,local dependence,nonuniform berry,normal approximation,random field,s method,stein,uniform berry},
number = {3},
pages = {1985--2028},
title = {{Normal approximation under local dependence b}},
volume = {32},
year = {2004}
}
@article{Robertson2015,
author = {Robertson, David and Prevost, Toby and Bowden, Jack and Robertson, David},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Robertson et al. - 2015 - Correcting for selection bias in two-stage trials with multiple correlated outcomes.pdf:pdf},
number = {May},
pages = {1--36},
title = {{Correcting for selection bias in two-stage trials with multiple correlated outcomes}},
year = {2015}
}
@article{Taylor2016,
abstract = {We derive an exact p-value for testing a global null hypothesis in a general adaptive regression problem. The general approach uses the Kac-Rice formula, as described in (Adler {\&} Taylor 2007). The resulting formula is exact in finite samples, requiring only Gaussianity of the errors. We apply the formula to the lasso, group lasso, and principal components and matrix completion problems. In the case of the lasso, the new test relates closely to the recently proposed covariance test of Lockhart et al. (2013).},
archivePrefix = {arXiv},
arxivId = {1308.3020},
author = {Taylor, Jonathan and Loftus, Joshua R. and Tibshirani, Ryan J.},
doi = {10.1214/15-AOS1386},
eprint = {1308.3020},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Taylor, Loftus, Tibshirani - 2016 - Inference in adaptive regression via the Kac-Rice formula(2).pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Convex analysis,Gaussian processes,Regularized regression},
number = {2},
pages = {743--770},
title = {{Inference in adaptive regression via the Kac-Rice formula}},
volume = {44},
year = {2016}
}
@article{Algeri2016,
abstract = {Searches for unknown physics and decisions between competing astrophysical models to explain data both rely on statistical hypothesis testing. The usual approach in searches for new physical phenomena is based on the statistical Likelihood Ratio Test (LRT) and its asymptotic properties. In the common situation, when neither of the two models under comparison is a special case of the other i.e., when the hypotheses are non-nested, this test is not applicable. In astrophysics, this problem occurs when two models that reside in different parameter spaces are to be compared. An important example is the recently reported excess emission in astrophysical {\$}\backslashgamma{\$}-rays and the question whether its origin is known astrophysics or dark matter. We develop and study a new, simple, generally applicable, frequentist method and validate its statistical properties using a suite of simulations studies. We exemplify it on realistic simulated data of the Fermi-LAT {\$}\backslashgamma{\$}-ray satellite, where non-nested hypotheses testing appears in the search for particle dark matter.},
archivePrefix = {arXiv},
arxivId = {1509.01010},
author = {Algeri, Sara and Conrad, Jan and van Dyk, David A.},
doi = {10.1093/mnrasl/slw025},
eprint = {1509.01010},
file = {:data/fireback/davenpor/davenpor/Referreeing/Testing One Hypothesis Multiple Times- The Multidimensional Case/A method for comparing non-nested models with application to astrophysical searches for new physics.pdf:pdf},
issn = {17453933},
journal = {Monthly Notices of the Royal Astronomical Society: Letters},
keywords = {Astroparticle physics,Darkmatter,Methods: data analysis,Methods: statistical},
number = {1},
pages = {L84--L88},
title = {{A method for comparing non-nested models with application to astrophysical searches for new physics}},
volume = {458},
year = {2016}
}
@article{Dumanjug2010,
abstract = {Two bootstrap procedures are introduced into the hybrid of the$\backslash$nbackfitting algorithm and the Cochrane-Orcutt procedure in the$\backslash$nestimation of a spatial-temporal model. The use of time blocks of$\backslash$nconsecutive observations in resampling steps proved to be optimal in$\backslash$nterms of stability and efficiency of estimates. Between iterations,$\backslash$nthere were minimal changes in the empirical distributions of the$\backslash$nparameter estimates associated with the covariate and temporal effects$\backslash$nindicating convergence of the algorithm. Crop yield data are used to$\backslash$nillustrate the proposed methods. The simulation study indicated that$\backslash$nprediction error from the fitted model (estimated from either Method 1$\backslash$nor Method 2) is very low. Also, the prediction error is relatively$\backslash$nrobust to the number of spatial units and the number of time points.},
author = {Dumanjug, C. F. and Barrios, E. B. and Lansangan, J. R.G.},
doi = {10.1080/00949650902785209},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Spatial Bootstrap/Bootstrap procedures in a spatial temporal model.pdf:pdf},
issn = {0094-9655},
journal = {Journal of Statistical Computation and Simulation},
number = {7},
pages = {809--822},
title = {{Bootstrap procedures in a spatial-temporal model}},
volume = {80},
year = {2010}
}
@article{Chernozhukov2015,
abstract = {Slepian and Sudakov-Fernique type inequalities, which compare expectations of maxima of Gaussian random vectors under certain restrictions on the covariance matrices, play an important role in probability theory, especially in empirical process and extreme value theories. Here we give explicit comparisons of expectations of smooth functions and distribution functions of maxima of Gaussian random vectors without any restriction on the covariance matrices. We also establish an anti-concentration inequality for the maximum of a Gaussian random vector, which derives a useful upper bound on the L$\backslash$'{\{}e{\}}vy concentration function for the Gaussian maximum. The bound is dimension-free and applies to vectors with arbitrary covariance matrices. This anti-concentration inequality plays a crucial role in establishing bounds on the Kolmogorov distance between maxima of Gaussian random vectors. These results have immediate applications in mathematical statistics. As an example of application, we establish a conditional multiplier central limit theorem for maxima of sums of independent random vectors where the dimension of the vectors is possibly much larger than the sample size.},
archivePrefix = {arXiv},
arxivId = {1301.4807},
author = {Chernozhukov, Victor and Chetverikov, Denis and Kato, Kengo},
doi = {10.1007/s00440-014-0565-9},
eprint = {1301.4807},
file = {:data/fireback/davenpor/davenpor/Papers by Subject/Bootstrap/Chernozukov/COMPARISON AND ANTI-CONCENTRATION BOUNDS FOR MAXIMA OF GAUSSIAN RANDOM VECTORS.pdf:pdf},
issn = {01788051},
journal = {Probability Theory and Related Fields},
keywords = {Anti-concentration,Conditional multiplier central limit theorem,L{\'{e}}vy concentration function,Maximum of Gaussian random vector,Slepian inequality},
number = {1-2},
pages = {47--70},
title = {{Comparison and anti-concentration bounds for maxima of Gaussian random vectors}},
volume = {162},
year = {2015}
}
@book{Epifani2004,
abstract = {Bayesian nonparametrics works – theoretically, computationally. The theory provides highly flexible models whose complexity grows appropriately with the amount of data. Computational issues, though challenging, are no longer intractable. All that is needed is an entry point: this intelligent book is the perfect guide to what can seem a forbidding landscape. Tutorial},
author = {Epifani, Ilenia},
booktitle = {Journal of the American Statistical Association},
doi = {10.1198/jasa.2004.s346},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Epifani - 2004 - Bayesian Nonparametrics Bayesian Nonparametrics . J. K. Ghosh and R. V. Ramamoorthi . New York Springer , 2003 . ISB(2).pdf:pdf},
isbn = {0387955372},
issn = {0162-1459},
number = {467},
pages = {898--899},
title = {{Bayesian Nonparametrics Bayesian Nonparametrics . J. K. Ghosh and R. V. Ramamoorthi . New York : Springer , 2003 . ISBN 0-387-95537-2 . xii   305 pp. {\$}79.95 (H).}},
volume = {99},
year = {2004}
}
@article{Brown1981,
author = {Brown, Charles C. and Fears, Thomas R.},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brown, Fears - 1981 - Exact Significance Levels for Multiple Binomial Testing with Application to Carcinogenicity Screens.pdf:pdf},
number = {4},
pages = {763--774},
title = {{Exact Significance Levels for Multiple Binomial Testing with Application to Carcinogenicity Screens}},
volume = {37},
year = {1981}
}
@book{Fuller1996,
author = {Fuller, Wayne A},
file = {:data/fireback/davenpor/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fuller - 1996 - Introduction to Statistical Time Series.pdf:pdf},
isbn = {0471552399},
title = {{Introduction to Statistical Time Series}},
year = {1996}
}
